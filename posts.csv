title,content,tokens
The Quad Cortex Desktop Editor is Finally Announced,"I first wrote about derails of the long awaited desktop editor for the Quad Cortex back in February 2022. At that point, it had been revealed a team had been working on the editor for months. Here we are fourteen months later and we have confirmation of a release.back in February 2022In their April 2023 update, they revealed they’ll be showing off a beta of Cortex Control at NAMM. On top of that, the QC’s will also be running CorOS 2.1.0 beta as well to support the editor.April 2023 updateNow, we know from community efforts like OpenCortex that the Quad Cortex has been capable of VNC like remote access for a whole now. The QC essentially runs a server. Presumably the desktop editor will work in a similar way to open source efforts. It’ll be interesting if this opens up the possibility of reverse engineering the desktop app and allowing the community to create their own clients.The remaining question on everyone’s lips is still: when is the Quad Cortex going to get plugin support? A question still don’t have an answer to. ",259
A Review of The Arrowmax SES Ultra Plus Screwdriver,"Only a few readers of this blog might know I am a serial Kickstarter backer. Fortunately, I’ve backed tens of projects over the last few years and have yet to back something that didn’t eventually deliver. One of the most recent projects I backed was the SES Ultra Screwdriver. Considering there isn’t much info about it yet, I wanted to share my first impressions and anything you should know about this.I opted for the non-motion control model. I just wanted a simple adjustable screwdriver, so I got the SES Ultra Plus.Packaging and first impressionsBefore we get into the features and other bits and pieces, we’ll briefly discuss my first impressions. With Kickstarter projects, you never know if the project pitch will be the same as the final product. Before I even opened the screwdriver, it was nicely boxed in a sturdy white box with exterior branded black packaging. Subtle and not too over the top. My hunch was this would be a quality product before even using it.Inside the box is a long sturdy zip-up case that resembles a Nintendo Switch travel case, except a little wider. Inside that case is a rugged aluminium case which is impressively built. It feels sturdy, like you could drop it, and it wouldn’t break (I wasn’t game enough to find out). Pressing on the bottom of the case, the charger holding the screwdriver and bits slide out. Strong magnets hold the bits in.Enough about the packaging, but the packaging is essential. The first time someone experiences your product can make the customer feel like they bought something high-quality or cheap.Features and specificationsIf you’re just a Kickstarter backer excitingly awaiting your SES Ultra screwdriver to arrive, you already know about these features and can skip to the next section. The MAX model has Bluetooth and Smart Motion Control (not listed), but the other specs are the same for both models.
4 kgf.cm torque force



70 S2 steel bits



Five torque modes



500mAh Lithium-ion battery



OLED display



LED illuminated tip that lights up what you’re screwing



Aluminium outer case with interior plastic USB-C charging case. The underside of the aluminium case has a magnetised pad for holding bits. The case also has an in-built magnetiser to keep your bits magnetised



It will automatically switch to manual mode allowing you to use it manually if the electric mode can’t provide enough torque.
4 kgf.cm torque force70 S2 steel bitsFive torque modes500mAh Lithium-ion batteryOLED displayLED illuminated tip that lights up what you’re screwingAluminium outer case with interior plastic USB-C charging case. The underside of the aluminium case has a magnetised pad for holding bits. The case also has an in-built magnetiser to keep your bits magnetisedIt will automatically switch to manual mode allowing you to use it manually if the electric mode can’t provide enough torque.Putting it to useFirstly, the SES feels premium in your hands. The material’s shape, feel, and square-like edges give you an excellent grip on this. It’s slightly weighted but very light and is comparable to a regular magnetised screwdriver. The little details make this screwdriver feel nice besides the engineer of the screwdriver itself.The way the screwdriver snaps into the case. It’s surprisingly satisfying. The magnets feel high-quality; you can feel the screwdriver snap into place and hear it. Like an expensive car door being closed, it just sounds pleasant.Another excellent detail is how easy it is to get the screwdriver out of the tray and the bits. I’ve owned plenty of screwdriver sets and handheld screwdrivers with bits. They’re often clicked into place in moulded plastic and are sometimes a pain to put in and get out. The SES case holds the bits into place using solid magnets, which are solid but easy to get out. I appreciate that Arrowmax had the foresight to leave space around the bits to get your fingers in there to unseat them easily.And then you have the OLED screen. As you’re holding down the directional buttons, it displays the direction of the screw on the screen, which animates, and it’s a nice touch. The screen is nice and bright (like a good OLED should be). The LEDS at the top where the bits attach are also nice when working in a small space (like a phone or computer tower).I don’t need a screwdriver that often, but I play guitar and service my guitars. I, fortunately, had a guitar that needed some machine heads replaced, and this is the first thing I used my SES Ultra Screwdriver on. I set the screwdriver torque to the number 3 setting, chose my needed bit and went to work. It didn’t break a sweat; the number 3 setting was more than enough. I could have used setting 2.Something worth noting is that heads are screwed into the wooden headstock of the guitar and aren’t very loose because they’ve been there since factory assembly. The screwdriver showed no signs of strain.Using the screwdriver was a lot of fun; it made me look around the house for other things to take apart. I used the lower torque one setting to tighten my glasses (there are some great small bits for glasses). Then I took apart the remote to my ceiling fan (also torque setting one) and, using torque setting five, tried it out on some door hinge screws for the door that leads into my study.The torque five setting is noticeably powerful. Unless you’re attempting to drill into masonry or something where a drill would be better suited, I honestly can’t see the SES Ultra struggling to achieve most screwdriver-based tasks. It won’t help you assemble flatpack furniture, but if you’re repairing gadgets or need a screwdriver for a quick task, this is what you want. Plus, it’s a lot of fun to use.Who would have thought a battery-operated screwdriver could be so much fun?A few shotsIf you’re wanting a few shots of what it looks like, see below for a few basic photos. I didn’t photograph the packaging, just the screwdriver and the case. The photos don’t do it justice; the lighting was terrible. But you can see, it’s very much a real product.ConclusionOverall, the SES Ultra is way better than I was expecting it to be. If you’re using the SES Ultra for what it is intended for (small electronics and other things), it does the job impressively. I can’t speak for the MAX version with the motion control feature and Bluetooth, but given how impressive the Ultra is, I am tempted to buy another one. And that should tell you all you need to know about this screwdriver: it’s worth buying again.If you’re interested in getting one yourself, they’re currently being sold on Indiegogo here.here",1647
Keeping a GitHub Fork Updated,"As developers, we often find ourselves forking open-source projects on GitHub to contribute or to modify for our specific needs. While creating a fork is a simple process, keeping it up-to-date with the original repository can sometimes be confusing. In this blog post, we will explore the steps required to keep your GitHub fork updated with the latest changes from the upstream repository.Step 1: Configure the Upstream RepositoryTo keep your fork updated, you must first configure the upstream repository – the original project from which you created your fork. This ensures you can easily fetch the latest changes from the upstream repository.
Navigate to your forked repository on GitHub.



Open a terminal and change to the local directory containing your fork’s cloned repository.



Run the following command to list the current configured remote repositories: git remote -v



If you don’t see an upstream repository listed, add one using the following command (replacing ‘upstream-url’ with the original repository’s clone URL): git remote add upstream upstream-url
Navigate to your forked repository on GitHub.Open a terminal and change to the local directory containing your fork’s cloned repository.Run the following command to list the current configured remote repositories: git remote -vgit remote -vIf you don’t see an upstream repository listed, add one using the following command (replacing ‘upstream-url’ with the original repository’s clone URL): git remote add upstream upstream-urlgit remote add upstream upstream-urlStep 2: Fetch Upstream ChangesNow that you have the upstream repository configured, you can fetch the latest changes from it. To do this, run the following command: git fetch upstreamgit fetch upstreamThis command fetches the latest changes from the upstream repository without merging them into your local branch. You can now see the changes by running: git log upstream/maingit log upstream/mainReplace ‘main’ with the appropriate branch name if the upstream repository uses a different default branch.Step 3: Rebase Your Local BranchFirst, ensure you’re on the correct branch:git checkout mainReplace ‘main’ with the appropriate branch name if needed.Next, rebase your local branch with the upstream changes:git rebase upstream/mainThis command will apply your commits on top of the latest changes from the upstream repository, resulting in a linear commit history.If you encounter any conflicts during the rebase, Git will pause the process and ask you to resolve them. Edit the conflicting files, save your changes, and then stage the resolved files with the following:git add path/to/conflicting/fileContinue the rebase process with:git rebase --continueRepeat the conflict resolution process until all conflicts have been resolved and the rebase is complete.Step 4: Push Changes to Your ForkAfter rebasing, you must force-push the changes to your fork on GitHub, as the commit history has been modified. Use the following command to do this:git push -f origin mainReplace ‘main’ with the appropriate branch name if needed. Your fork is now up-to-date with the latest changes from the upstream repository.ConclusionRebasing is an excellent alternative to merging when keeping a GitHub fork updated. It creates a cleaner, linear commit history that can be easier to understand and manage. However, be cautious when using git push -f, as it can overwrite remote changes if not used correctly. Ensure you push to the correct branch and remote repository to avoid potential issues.git push -f",884
ChatGPT: A Glorified Sentence Constructor or a Potential Threat? Should Governments Regulate AI?,"In recent years, we’ve seen an unprecedented rise in the development and adoption of artificial intelligence (AI) tools, such as OpenAI’s ChatGPT. As AI becomes increasingly integrated into our daily lives, it’s essential to ask whether governments should step in and regulate these technologies. While some argue that ChatGPT is a glorified sentence constructor and poses no real threat, others believe that regulation is necessary to prevent misuse and ensure ethical practices. In this article, we’ll explore both perspectives and attempt to determine whether AI regulation is needed.I talk about ChatGPT in this article, but the argument applies to all emerging AI tools that work similarly to GPT.Why Regulation Seems Silly: The Sentence Constructor ArgumentAt its core, ChatGPT is a language model designed to process and generate human-like text. Some argue that regulating such tools is unnecessary since they’re nothing more than advanced sentence constructors. Here are a few reasons to support this view:
Limited Sentience: Unlike humans, ChatGPT doesn’t possess consciousness, emotions, or the ability to think critically. It relies on pre-trained data to generate responses, lacking genuine understanding and intent.



User Control: The output of AI tools like ChatGPT is primarily determined by the user’s input. The responsibility lies with the user rather than the technology itself.



Precedent for Self-Regulation: Historically, emerging technologies have often adapted and evolved through self-regulation. Tech companies and AI developers may be better equipped to address ethical concerns and implement best practices without government interference.
Limited Sentience: Unlike humans, ChatGPT doesn’t possess consciousness, emotions, or the ability to think critically. It relies on pre-trained data to generate responses, lacking genuine understanding and intent.Limited Sentience:User Control: The output of AI tools like ChatGPT is primarily determined by the user’s input. The responsibility lies with the user rather than the technology itself.User Control:Precedent for Self-Regulation: Historically, emerging technologies have often adapted and evolved through self-regulation. Tech companies and AI developers may be better equipped to address ethical concerns and implement best practices without government interference.Precedent for Self-Regulation:Why Regulation Might Be Necessary: The Potential Threat ArgumentOn the other hand, there are valid concerns about the potential negative impacts of AI tools like ChatGPT, which could justify government regulation. These concerns include the following:
Misuse and Malicious Intent: While AI tools may not have their own intentions, they can be misused by users with malicious goals, such as spreading disinformation, hate speech, or engaging in cybercrime. ChatGPT has been used to create malware before or find exploits in software.



Bias and Discrimination: AI models like ChatGPT are trained on vast datasets containing human-generated content, which can inadvertently introduce biases into the generated responses. Regulation may be necessary to ensure AI tools are transparent and designed to reduce potential bias.



Ethical and Privacy Concerns: AI tools like consent and data privacy can raise ethical questions. Regulation may be required to establish ethical guidelines and protect user privacy. This is why ChatGPT is currently blocked in Italy over privacy concerns.
Misuse and Malicious Intent: While AI tools may not have their own intentions, they can be misused by users with malicious goals, such as spreading disinformation, hate speech, or engaging in cybercrime. ChatGPT has been used to create malware before or find exploits in software.Misuse and Malicious IntentBias and Discrimination: AI models like ChatGPT are trained on vast datasets containing human-generated content, which can inadvertently introduce biases into the generated responses. Regulation may be necessary to ensure AI tools are transparent and designed to reduce potential bias.Bias and DiscriminationEthical and Privacy Concerns: AI tools like consent and data privacy can raise ethical questions. Regulation may be required to establish ethical guidelines and protect user privacy. This is why ChatGPT is currently blocked in Italy over privacy concerns.Ethical and Privacy ConcernsThe Middle Ground: Striking a BalanceAs with many emerging technologies, there’s no one-size-fits-all answer to whether AI tools like ChatGPT should be regulated. It’s essential to strike a balance that acknowledges the potential risks while not stifling innovation.A possible approach involves creating regulatory frameworks focusing on specific AI aspects, such as data privacy, transparency, and accountability. This approach would allow governments to address valid concerns without hindering the development and growth of AI technologies.One of the problems is this is all emerging. AI is a relatively new thing. It’s like the emergence of the internet or social media all over again. If we do implement a regulatory framework, how is it enforced, and are governments even equipped to regulate such things while ensuring the essence of a free market remains?ConclusionThe debate surrounding AI regulation is complex and multifaceted. While AI tools like ChatGPT can be seen as glorified sentence constructors, it’s essential to recognize their potential risks and ensure responsible use. Striking a balance between innovation and regulation is key to fostering a thriving AI ecosystem that benefits society. Ultimately, the decision to regulate AI tools should be informed by ongoing dialogue, research, and a thorough understanding of the technology’s potential impacts.",1434
Is Notion Hype or The Real Deal?,"I’ve heard about Notion for years, and admittedly, I wrote it off as a fancy writing application until recently when I used it at work and got exposed to everything it can do. I’m also averse to overhyped things (it wasn’t until Breaking Bad was in its third season I succumbed to the hype and watched the show).NotionAs a developer, I have a lot on my plate. Between the consulting and freelancing work I do, there are numerous things that I have to manage daily. That’s why I’m always looking for productivity tools that can help me streamline my work and keep me on track.I have tested countless apps in the past that promised to enhance productivity and workflow, but most of them required a significant change in the way I worked or thought. Adapting to new methodologies and systems was quite difficult, so I reverted to my tried and tested approach of using Trello, which has always been a reliable tool.However, with the advent of Notion, a multifunctional app that claims to be capable of performing a plethora of amazing tasks, I can’t help but wonder if it is truly a game-changing app or just another hyped-up application. Can it deliver on its promises and revolutionise the way we work?Existing apps that Notion can replaceAt a glance, Notion can replace the following popular apps (and even more with its integrations support)
Trello



Evernote



Google Docs



Google Sheets



Asana



Airtable
TrelloEvernoteGoogle DocsGoogle SheetsAsanaAirtableNotion’s ability to integrate with other apps and services, such as Slack and Google Drive, adds to its appeal as a productivity tool. Its customisability and flexibility make it a powerful tool for managing tasks, notes, and projects all in one place.I don’t know about you, but it’s a red flag to me when a restaurant has too many items on the menu and an app with too many features and promises.AI WritingOne of my favourite features in Notion is the AI writing functionality. This feature allows you to leverage the ChatGPT-like functionality to generate original context, rewrite existing content and more. The AI functionality is incredibly versatile and can be used for various tasks. For example, you can brainstorm ideas for your next project, generate content for your website or social media channels, or even proofread your work.NotionI have used the AI functionality to write and proofread parts of this post you are reading. I was thoroughly impressed with the results, as the AI-generated high-quality content quickly and efficiently. While I am not sure if Notion is using GPT behind the scenes, it is clear that their AI functionality is top-notch and can help you save time and effort on a variety of tasks.It is, sadly, an additional upgrade of $10 per month to get unlimited functionality, but I can tell you that it is well worth it.For BloggingIf you’ve visited my blog before, you know I’ve been actively blogging for over 13 years. Writing a blog post can take hours, sometimes weeks, of effort before you hit the publish button.My approach to drafting blog posts is to create a draft post in WordPress and then slowly work on it. However, this method can result in tens or hundreds of draft posts, making it easy to lose sight of older content.With Notion, I now draft my blog posts and leverage AI features to expand dot points into fully-fledged sections. I then build upon those foundational sections to create complete posts. I’ve been able to come up with content ideas faster but still, stay true to my core as a blogger and not lose my tone of voice.For Project ManagementSo, with Notion, you can create KanBan boards and other cool project management structures that help you keep track of all your project tasks and collateral (like marketing and tech specifications).NotionNotion’s Kanban board feature is especially useful for managing projects, allowing you to visualise your workflow and track task progress intuitively. With Notion, you can collaborate with team members in real time, assign tasks, and leave comments on specific items. While it may take some time to learn all of the app’s features, once you do, you’ll find that Notion is a powerful and flexible tool that can help you work smarter, not harder.For WikisNotion provides a great platform for organising and sharing knowledge within your team for wikis. You can create a knowledge base containing articles, guides, and other resources your team can access and contribute to. Notion’s customisability means you can organise your wiki in a most useful way to your team, making it easy to find and share information.Furthermore, you can set up tags and access to restrict who can access and edit parts of your wiki. It saves the hassle of using something else, like MediaWiki, with nothing to install.For Technical Specification DocumentsNotion provides an excellent platform for technical documentation that allows you to create and share technical specification documents with your team. You can create tables, diagrams, and other visual aids to help illustrate your ideas and specifications. If you are scoping a technical project most likely comprised of multiple components, Notion allows you to divide those parts up and then collaboratively work with others to build it.For Managing Your Personal LifeFor managing your personal life, Notion provides a great platform for organising your personal tasks, notes, and projects. You can create to-do lists, track habits, and journal your day. Its customisability and flexibility make it useful for keeping your life organised and productive. In our household, we use it to manage daily and weekly chores.For StudentsWhether you’re learning a new programming language or managing your degree. Notion is an excellent tool for students, providing a platform for organising class notes, tracking assignments and deadlines, and managing study plans. You can create information databases, such as vocabulary lists or research topics, and share them with classmates or study groups. Additionally, Notion’s customisability allows you to tailor your study experience to your needs and learning style. My wife is currently studying, and while it took a bit to set up, she uses it to keep track of her semesters, take notes and more.ConclusionWhile Notion may have been hyped up, it seems to be the real deal regarding productivity and project management. Its versatility and extensive features make it a valuable tool for individuals and teams.NotionIf you’re looking for a productivity tool that can do it all, Notion may be the answer. With its customizability and flexibility, Notion can replace multiple apps and services, from Trello to Google Docs. Its AI writing functionality is also a game-changer, allowing you to generate high-quality content quickly and efficiently.Maybe you were on the fence like I was. But I can tell you that you don’t have to be a project manager or writer to get value out of Notion. You can check it out by signing up for free here.signing up for free here",1750
A Review of ChatGPT GPT-4,"Well, the rumours were true. GPT-4 has been announced, and it’s just as impressive as we had hoped. We’ve heard of big things for GPT-4 for months, so does the latest and greatest version of OpenAI’s hyped model live up to the hype?For comparison, here is ChatGPT using GPT-3.5:And here is ChatGPT using GPT-4:The first thing you notice is reasoning is 5/5. Speed is 2/5, and conciseness is 4/5. The one thing you see before using GPT-4 is how it’s over half as fast as GPT-3.5. This is most likely due to the increased parameters the model deals with as it processes your inputs.First ImpressionsThere is currently a cap which OpenAI says is a dynamic and temporary cap because it hasn’t been publicly released yet. Because I have ChatGPT Plus, I can access the GPT-4 model early. When writing this, my cap was 100 messages every 4 hours.The first thing I noticed was GPT-4 is quite slow. I am used to the fast responses I get with GPT-3.5, but GPT-4 is very slow. The words seem to type slower. However, this might be an artificial speed constraint until publicly released. Or, it could be system load as the GPT-4 model might not have the required resources yet. I’ve seen GPT-3.5 respond to this slowly on the free version, so I imagine it will be faster.But the second thing I noticed is the responses are a lot larger. In GPT-3.5, long responses weren’t uncommon to cut off part of the way through. You could type “continue”, and it would complete, but it was painful if you were generating code. GPT-4 can now handle up to 25,000 words of text.In terms of tokens, GPT-4 has a context length of 8,192 tokens. Pretty much double the size of GPT-3.5, which is 4,096 tokens. More crazily, OpenAI also has a version of GPT-4, which supports 32,768 tokens. This isn’t available in ChatGPT right now, and I wonder if it will only be reserved for the API and Playground, not ChatGPT.Quality of responsesDespite being a new model, the dataset is still the same 2021 cut-off data. As such, GPT-4 isn’t bringing any new information. Where things differ between GPT-3.5 and GPT-4 is how it interprets that data and returns it to you based on your inputs.A simple test of GPT-4 capabilities is to take existing prompts and then re-run them through the new model in ChatGPT. Unbeknownst to some, I use ChatGPT to write satire. While the GPT-3.5 model did a fantastic job producing something, it produced robotic-sounding articles that didn’t meet the brief most of the time. It required some additional prompting and shaping to be something usable.One problem I would always encounter is it loved to start off with, “In a shocking turn of events”, which gave it away that ChatGPT was being used. It would find another way to say something similar even when I told it not to. And it loved to say “In conclusion” at the end.With GPT-4, I can tell you the end result is a night and day difference. GPT-3.5 wasn’t terrible, but the results aren’t fantastic. If I were a copywriter, GPT-3.5 would have been scary and GPT-4 terrifying. GPT-4 has reached a point where it can produce good and not only good but long results requiring minimal editing.OpenAI also says that images can be used as input, and you can ask it to describe things in the image or boost a prompt without typing. Say, for example, take a drawing and some notes of a website, then produce the code for you.ConclusionThe GPT-4 model is a significant leap in OpenAI’s models. It’s arguably the best model they have right now, which might be partly thanks to Microsoft’s involvement. The BingGPT integration was clearly using GPT-4 as the results were quite good, and now ChatGPT has the same level.If ChatGPT 3.5 was seen as an assistant, ChatGPT 4 could be seen as a junior-level employee. It doesn’t get everything right, but it produces significantly better results, especially in writing. That should terrify authors, journalists and anyone that makes a living out of writing.",984
"Elon Musk Wants to Build a ChatGPT Competitor, Does He Have the Attention Span to Make It Happen?","Elon Musk, who needs no introduction, is at it again. This time, he has set his sights on the world of AI language models with his new venture, a competitor to ChatGPT. But before we delve into the nitty-gritty of this latest development, let’s talk about Elon Musk himself and the reputation he has garnered over the years.Many people know him as the visionary entrepreneur who co-founded PayPal, launched the electric car company Tesla, and is leading the charge on space exploration with SpaceX. But behind the scenes, there are those who question his character and motives. Some have accused him of being a conman with a short attention span, easily losing interest in things once the initial excitement wears off.One need only look at his recent acquisition of Twitter for $44 billion to see an example of this behaviour. Musk made headlines last year when he purchased the social media giant, only for Twitter to have been subjected to a series of troubling moves that equate to serious red flags, many decisions made without thinking them through and causing some negative fallout. It’s not hard to see why some may view this move as impulsive and lacking foresight.So, what does all of this have to do with his new AI language model venture to try and compete with ChatGPT? Well, it raises the question of whether Musk’s involvement in this space is driven by genuine interest and commitment or if it’s just another shiny object that caught his eye. After all, this isn’t the first time he’s dabbled in AI, having previously been involved with OpenAI, a research institute focused on artificial intelligence, a company he now criticises.But regardless of his motives, Musk’s involvement in the AI language model space is significant. ChatGPT, OpenAI’s flagship product, has been hailed as a breakthrough in natural language processing, capable of generating human-like text with startling accuracy. More competition is always good, especially in AI, and we are currently seeing an arms race amongst some of the bigger companies. For OpenAI to have a monopoly would be a bad thing. So, if Elon wants to level the playing field, it’s a good thing (if he can stay focused long enough to launch it).Microsoft came out swinging with BingGPT, based on ChatGPT and leveraging what many believe to be close to GPT-4. Google has Bard (although it sounds like a garbage product at the moment). Elon has the connections to build a ChatGPT rival, but like his other ventures, will he lose interest when something new and shiny catches his eye?",635
Experimenting With Brewing a Non-alcoholic IPA,"As some of you know, I am an avid homebrewer. And, I love my IPA’s and Pale Ale style beers. Sadly, they’re often high in ABV (alcoholic content), and as I get older, I want to appreciate what I drink and not have to worry about the hangover the next day if I have too many.That is where my interest in non-alcoholic beers came from. Not wanting to compromise on taste and mouthfeel, I set out to see if brewing a low-alcohol beer (<0.5%) without compromise was possible.And that is where I learned about speciality yeasts specifically for brewing non-alcoholic beers. Specifically, the Fermentis SafBrew LA-01. What better way to experiment than an extract recipe? Because who wants to spend a day doing an all-grain brew only for it to fail?Fermentis SafBrew LA-01So, I messed around in Brewersfriend to create a non-alcoholic IPA recipe at 0.5% ABV called Lightly Hopped Session IPA. By the way, if you’re a brewer and not already using it, I highly recommend Brewers Friend, a comprehensive recipe and brew app I’ve been using for years.Lightly Hopped Session IPABrewers FriendThe speciality yeast gives you quite a bit of grace regarding the ABV, but you must watch the grains you use and how much you use them. 225g for each grain for a 23L fermenter was the sweet spot for this recipe, but you can experiment with different quantities and grains.In a future post, I will let you know how it turned out.",352
Microsoft Modern Wireless Headset Review,"Being a remote worker, a good tech setup is essential. A comfortable chair, nice desk, well-positioned monitor and a keyboard and mouse you love. Then there are the other parts that people don’t think about as much: webcam and microphone.Before buying the Microsoft Modern Wireless Headset, I was using an AT2020+ microphone on a boom arm, which I use for streaming and other purposes. Then I got the Blue Yeti X, a great microphone, my primary one. It’s a great microphone, but I have to adjust the audio levels with it through my interface routinely.Microsoft Modern Wireless HeadsetHowever, I realised I sit often and am terrible when taking breaks or leaving my computer. They do say sitting is the new smoking.Sometimes I don’t need to sit at the computer on a call. Unless I am going over some code with someone, if it’s a meeting where we discuss work, I could be standing and walking around (something a wired headset or microphone doesn’t let you do). My monitor is still visible, but the ability to pace around my study would benefit my developer body.And that realisation is what made me think of a wireless headset.My initial search yielded some quite expensive headsets. I was shocked when I saw some for $300+, then even more shocked when the prices kept going up past $1k. I am sure the expensive wireless headsets are great, but I just wanted a comfortable one that won’t bankrupt me and have decent sound quality.And that’s when I came across the Microsoft Modern Wireless Headset.Microsoft Modern Wireless HeadsetIt ticked all of the boxes:It ticked all of the boxes:
Good reviews



Affordable



Built for audio calls



It’s a Microsoft product, so it would be easier to replace it if it’s faulty.
Good reviewsAffordableBuilt for audio callsIt’s a Microsoft product, so it would be easier to replace it if it’s faulty.Like everything I buy, even the cheap things, I research extensively. The Modern Wireless Headset seems to be a highly-rated headset for the price. While it is marketed as a Microsoft Teams-compatible headset, it will work with any application. Furthermore, you can even use it with your phone if you like. Some say they use it for music, but I am a bit of an audiophile and don’t think I could bring myself to do that.For the price, I was expecting the headset to leave my head or ears sore, but Microsoft seems to have surprisingly built an affordable headset that doesn’t comprise comfort. I unknowingly leave the headset on even when not on a call or listening to music. The battery life is also surprisingly good. I am not sure of the claim of 50 hours as I am in the habit of charging my devices at the end of the day.And just when you thought things couldn’t get any better, I tested this headset on Ubuntu Linux, and it worked out of the box. No drivers were needed, as Ubuntu recognised it and allowed me to use it.Are there better headsets out there? Undoubtedly. But you can’t go wrong if you’re like me and just wanted a headset specifically for audio calls. The Microsoft Modern Wireless headset gets the job done without breaking the bank.Microsoft Modern Wireless headset",781
5G Is a Bit of a Disappointment So Far,"Australia is a technology backwater, so we are no strangers to being left behind when it comes to the latest in wireless standards and internet speeds. Specifically, 5G promises to be bigger and better than 4G and the other protocols that came before it. I don’t know about you and whether disappointment is a global phenomenon, but I have found 5G quite disappointing.With 5G, we’re talking about potential speeds of up to 10 Gbps, compared to 4G’s maximum theoretical download speed of 100 Mbps in the real world. That’s a significant improvement, but real-world speeds will vary based on several factors. The reality is nobody is getting anywhere near 10 Gbps outside of a lab.One of the main advantages of 5G is its low latency. Latency is the delay between when you request data and when it’s actually delivered. With 5G, latency is expected to be as low as one millisecond, significantly faster than 4G’s latency of around 30-50 milliseconds. But if you’re uploading photos to Instagram or watching cat videos on YouTube, you probably don’t notice the latency on 4G anyway.Another issue with 5G is its coverage. 5G relies on high-frequency millimetre waves, which have a shorter range and can’t penetrate obstacles like buildings and trees as easily as 4G’s lower-frequency waves. This means you must be close to a 5G transmitter to get a good connection, and obstacles can affect the signal. In contrast, 4G uses lower-frequency waves that can penetrate obstacles better, making it more reliable in some cases.The reality is the only time I have noticed things being faster while on 5G is when I am in the CBD of a major city or, strangely enough, inside a large shopping centre. It seems major shopping centres mostly have good 5G in Australia (probably so the telco stores can sell you a 5G device). However, don’t be surprised once you leave the CBD’s tiny permitter, walk between skyscrapers or walk through the shopping centre doors to your car to see your phone go back to 4G or worse.There are several reasons for the lacklustre performance of 5G. First, 5G requires more infrastructure than 4G, so telcos must invest in more antennas and base stations to provide widespread coverage. This can be costly and time-consuming, and it’s one of the reasons why 5G coverage is currently limited to certain areas.Another reason is that the technology is still relatively new, and some teething issues still need to be ironed out. For example, 5G technology is still evolving, and different countries use different frequencies and network technologies, which can cause compatibility issues.5G is not the game-changer it was hyped up to be, at least not yet. While 5G promises faster speeds and lower latency than 4G, its performance can be affected by several factors, including distance from the transmitter, obstacles, and infrastructure. In contrast, 4G is a reliable and fast option with widespread coverage and better signal penetration in areas with obstacles.I still encounter numerous dead spots in Australia where I can’t even get 4G. I am not talking about remote regional places without water or electricity. I am talking about suburbs 10-20 kilometres from the CBD. So, even if 5G is improved in the next few years, there will still be parts where you can’t even get 4G.As much as I would love faster speeds (because faster is always better), I haven’t felt that 4G is not enough. For most uses on your phone or tablet, 4G is plenty fast. You’re not downloading torrents or hosting a web server on your phone, and you’re also not playing competitive games like Dota 2 that require minuscule latency either.I see 5G playing a significant part in future IOT, virtual and mixed reality applications. We will probably see connected cars that can talk to each other (similar to protocols planes use to notify other planes of their presence.) and other cool things. But, for now, 4G is still the king in my eyes.",981
BingGPT: Microsoft Trips at the Finish Line,"I was really rooting for Microsoft with its ChatGPT integration into the Bing search engine. You might have seen the hype, including the hilarious controversy around Bing’s ChatGPT threatening journalists and being easily provoked.After a few weeks of closed access and insurmountable hype, Microsoft has opened the floodgates to many more people, and Bing’s ChatGPT integration is a dismal disappointment.Perhaps the passive-aggressive and threatening nature of Microsoft’s ChatGPT integration forced their hand. Still, after trying it for a while, it’s clear it isn’t a rival to the original ChatGPT anymore. Despite having access to up-to-date information, it has been dumbed down as it’s obvious Microsoft has cut both legs off to get it under control.The BingGPT is very cautious now. If you ask it harmless questions, it gets to the point where it will cut you off and say it can’t talk to you anymore.Take this interaction I had with BingGPT via the Bing Android app. I asked it what it thought about me. It commented about how I am a developer and write technical articles for the customer service industry (which is incorrect). When I tried to correct Bing, it said it preferred not to continue the conversation and stopped responding. All subsequent attempts to get it to talk were met with silence.That’s not to say Bing with ChatGPT is entirely terrible. For news, for example, it’s helpful to ask questions if you have heard something happening but were unsure or wanted further context. The Jake Paul vs Tyson Fury fight is a good example. I wanted to know if Bing knew Paul lost, and it did.There was also an alleged script leak for the fight, which hasn’t been proven true as Bing explained it said Paul was meant to win by knockout in round eight.I think the early reviews and talk of Bing’s ChatGPT integration made it seem like Bing would be offering an untethered version of ChatGPT. It makes sense that they’re not, as it would compete with ChatGPT, right?Still, all the talk we got of this intelligent internet-connected algorithmic AI has been tapered somewhat, undoubtedly driven by the fact Microsoft’s version has been routinely described as unhinged and out of control. Allegedly GPT-4 has been delayed, possibly by the Microsoft launch of BingGPT, citing ethical reasons.out of controlMaybe we will see Microsoft turn the dial back up to 11, but I think Microsoft has lost a profound opportunity to go toe-to-toe with Google here and take a chunk of their search dominance. BingGPT for the newcomers is anything but impressive. After seeing all the stories about it fighting with people and even proclaiming to have linked someone to a murder, we got a smart assistant.Although, to be fair, I still think BingGPT is better than anything Google currently has. It’s a step above standard search, and being able to do comparison searches and ask questions (especially for product searches) is excellent.Don’t get me wrong, BingGPT is still awesome and a step forward for better search, but it’s not the Google killer I or others thought it would be.",769
Building an Affordable Dedicated Linux Machine Using Intel NUC,"I have been dual-booting Ubuntu Linux on my main desktop PC for development. Docker on macOS and Windows with WSL suffers from severe I/O performance issues for “reasons”. Docker is infuriating to use outside of Linux, so I started looking for alternative builds.My primary 3900x gaming PC feels a little sacrilegious to use as a dedicated Linux machine to run some virtual machines and a Webpack server. I am not solving cryptographic problems here, so I just primarily need storage and ram; a decent CPU helps.At first, I did consider getting a laptop. However, it’s the same problem. Most decent laptops are expensive and have limited amounts of RAM and storage. Not all laptops these days have replaceable storage or RAM either (thanks, Apple). Interestingly, most mini PCs I found were basically laptops in different cases; the architecture down to the ram and storage slots are the same.After weighing up the options, I chose the Intel NUC 12 Pro. It’s a barebones mini PC kit from Intel that supports not just Windows but also numerous Linux distributions, including Ubuntu. Perfect. I considered older versions like the NUC 11, but the 12, one of the newest, ticked the right boxes at the right price.Intel NUC 12 ProBecause it’s a barebones kit, you don’t get any storage or memory. This little kit can support up to 64 GB of ram. So, that’s the first thing I purchased. I went for the maximum of 64 GB of DDR4 ram. I chose Crucial 32 GB DDR ram, two of them.Crucial 32 GB DDR ramFinally, I chose the Silicon Power P34A60 2 TB PCIe NVMe storage for storage. You will also want to buy a clover leaf cable, as the box doesn’t have a power lead. The Intel NUC C5 cable will do the job of powering your newfound tiny PC technological beast.Silicon Power P34A60 2 TB PCIe NVMeIntel NUC C5 cableIf you have ever installed memory before, installing the ram and NVMe storage is dead simple. You don’t need to be an expert in computer assembly to install them. They slot right in.As you’re aware, Linux is designed to be performant on low-spec hardware, so Ubuntu on the NUC 12 will be noticeably performant, provided you’re not attempting to play graphically intensive games or do anything that requires massive amounts of power. Another deciding factor on the NUC 12 Pro was that Linux is officially supported.Impressively, the NUC 12 Pro has 2 x HDMI 2.1 ports and 2 DP 1.4a via type C connectors. Then you have two USB 3.2 ports on the front. It has more connectors than you would think for such a small box. Then you have support for the Wi-Fi 6e protocol, an ethernet port, and integrated Bluetooth.Furthermore, you can power multiple screens off this tiny little box (depending on the resolution). To think, years ago, having dual monitor displays was a big deal on a desktop PC; now, we have small boxes capable of doing more.I am discussing the hardware a lot here, but it’s important. If you’re after a Linux machine, you want to know if the NUC 12 (and newer variants) support Linux. A resounding yes. Furthermore, you can affix the NUC because it’s so light to the back of a monitor and create your own makeshift all-in-one iMac. You will want an Intel NUC-compatible VIVO to mount like this one to do that.ntel NUC-compatible VIVO to mount like this oneIn another post, I will detail getting my development environment setup:
Installing drivers 



Installing software like Visual Studio Code



Installing and configuring Docker



Differences between Windows 11 and Ubuntu Linux



What works/what doesn’t
Installing drivers Installing software like Visual Studio CodeInstalling and configuring DockerDifferences between Windows 11 and Ubuntu LinuxWhat works/what doesn’tOverall, the NUC is what you’re after if you want a powerful Linux machine, especially as a secondary development machine to complement your gaming PC. I don’t doubt the older NUC variants would also hold their own running a flavour of Linux. I just wanted to buy something that wouldn’t have driver support revoked after buying it.",1005
Sony PSVR 2 Review,"The PlayStation VR 2 is Sony’s second attempt at a virtual reality headset. The PlayStation 4 had a PSVR headset, but it was marred by screen door, performance issues, a cacophony of cables and dildo-like move controllers with a convoluted tracking process. The PSVR 1 had some great titles but never felt like an adequately supported device.My first foray into the PSVR 2 as someone experienced with virtual reality headsets was enjoyable. My point of reference is my HP Reverb G2 headset which is similar in specs to the PSVR 2 but is frustrating to use as I have encountered numerous problems with Steam VR and Windows Mixed Reality software. The PSVR 2 is comfortable, and getting lost for hours is easy.What’s in the box?Admittedly, not much comes in the box. In line with modern products, the packaging is pretty minimal. You get a headset, two VR 2 Sense controllers, and a USB-A to USB-C cable (the same as the one you already have to charge your PS5 controller). Some pieces of paper and everything has a protective covering over them.The setup process is also exceptional. You are guided through the setup process that guides you through configuring eye tracking, tethering the controllers to your PS5 and setting up your room play area. It’s one of the nicest VR onboarding experiences I have had.I won’t bore you with the technical specifics. Sony has an impressive FAQ here, which tells you everything about the PSVR 2, from its resolution to haptic features.FAQ hereThe flagship title for the PlayStation VR 2, Horizon: Call of The Mountain, is what most will play first, and I highly recommend getting it. If you didn’t get the bundle, then the game is a cool AUD 110, but it’s a flagship title that uses everything that PSVR 2 has to offer, and it’s as impressive as the trailers made it seem, I would say it looks better when you experience it for yourself.This game is an excellent showcase of not only the graphical prowess of the 4K displays in the PSVR 2 but also highlights the capabilities of the VR 2 Sense controllers. You get haptic feedback depending on what you’re doing. In the opening scene, when you’re in the boat, you can look over the edge and scoop your hands in the water. You feel the haptic feedback, and it is incredibly immersive.When you are climbing, you feel the resistance, and after a while, I found my fingers were getting sore like I had been climbing for real. The Legendary Climbs side-quest will make you feel like you’ve been out for a climb.And Horizon: Call of The Mountain is where you learn how the VR 2 Sense controllers are an engineering feat and a step forward for VR. While the Sense controllers might look similar to other headsets, they’re some of the best I’ve tried. The controllers are very comfortable to wear, to the point where I was so immersed that I forgot I was holding controllers. The controllers on other headsets I’ve tried have always felt unnatural after extended use.Another underrated feature of the PSVR 2 headset is eye tracking. Thanks to an embedded camera in the headset, it can track your eye movement. Other VR headsets with this feature are double the price (and more). So, for a consumer VR headset at this price point, to have eye tracking is impressive. And, trust me, eye tracking isn’t a gimmick. It works incredibly well, and you get used to using your eyes to navigate menus.The headset is also impressively engineered, from the little details to the bigger things. On the face part of the headset, there is a rubber seal that is incredibly effective at blocking outside light; the included headset affixes to the headset band, and the length of the cables are just right, so they’re not too loose and dangly.Wearing the headset is also very comfortable. Sony engineers put a lot of thought into the design, from the weight of the headset to the distribution of the weight. The adjustment mechanisms, from tightening the band to adjusting the face part, are easy to configure to your face and head. The PSVR 2 is the first VR headset I’ve tried where I felt I could play for hours without experiencing aches or pains.I would also like to point out that I wear glasses, and the PSVR 2 headset is the first that feels like it took glass wearers into account. My glasses weren’t pressed into the bridge of my nose or face. The headset accommodated them, and no light leaks. Thank you, Sony.I have concerns over the inability to detach the cable from the headset (a fixed cable) and how resilient it would be if it were pulled too much. While some will decry the need for a cable, it’s one long USB-C cable, and it’s a small price to pay for leveraging the power of the PS5 console. But, for now, I am not overly concerned.Speaking of immersion, one thing I found wasn’t overly helpful was the haptic feedback of the headset itself. Maybe this can be tweaked in future updates to be more immersive, but I found the headset haptic feedback was more annoying than immersive, and I turned it off.After playing Horizon: Call of The Mountain for a few hours, I concluded it was an excellent introductory title to PSVR 2 but not a title I would find myself revisiting a lot. I have found other titles to have more replay value, like Tetris Effect: Connected, and I’ve fallen back in love with No Man’s Sky again with the PSVR 2 headset.The image quality on the headset is clear, and the foveated rendering in Gran Turismo is seamless. Comparatively, graphics quality-wise, I wouldn’t say the PSVR 2 is on the same level as my Reverb G2 headset, but it’s close, and you really have to nitpick to see perceivable differences in the field of view and the textures.Overall, the PlayStation VR 2 headset is fantastic and promising for the price point. Many have raised the headset’s cost as a downside, but you’ll pay a lot more to get something with the same feature set as the PSVR 2. For this to succeed, Sony must commit to it and get big titles to support it, like Spiderman or Grand Theft Auto (which I would love to play in VR).",1504
How to Interview Front-End Developers Without Coding Puzzles or Algorithms?,"When hiring front-end developers, there are many ways to evaluate candidates’ skills and abilities. However, some assessment methods can be exclusionary, while others may not accurately reflect the type of work the candidate will do. In this article, we’ll explore some efficient ways to test front-end developer hires without relying on coding puzzles and algorithms and how to be mindful of inclusivity.I come from a self-taught background, a time when self-taught invited increased scrutiny because being self-taught in the early to mid-00s wasn’t as common as it is now. We have numerous online courses, boot camps and other resources. When I learned to code, these thick phonebook-like books that came with one or more CD-ROM discs with software and code examples were how many learned.Pair programmingPair programming is an excellent way to assess a front-end developer’s skills. This involves the candidate and an interviewer working together on a coding problem or feature. The interviewer can observe the candidate’s thought process, coding style, and communication skills while providing real-time guidance and feedback. This method provides a more accurate representation of the type of work the candidate will do. It also allows the interviewer to understand the candidate’s problem-solving and collaboration skills. As long as the paired programming is relevant to the position and not made up coding puzzles, it’s quite effective.With the rise of AI tools that can help candidates fake answers to algorithm questions and take-home projects, pair programming can accurately tell you a candidate’s skill level.Code reviewCode review is an excellent method for assessing a candidate’s ability to work collaboratively with others and can help you identify candidates who can deliver high-quality work. Another way to evaluate a front-end developer’s skills is to ask them to review an existing codebase and provide feedback or suggestions. This can help you gauge their understanding of coding best practices, attention to detail, and ability to identify and fix issues in code.Now, something to be aware of here depends on the complexity and structure of your codebase; it might be difficult for a candidate to provide feedback without the context or prior domain knowledge of how things piece together.Discussion-based assessmentA discussion-based assessment involves asking the candidate about their approach to specific front-end development tasks, their understanding of various front-end technologies, or their experience with specific tools or frameworks. This method can help you evaluate a candidate’s broader understanding of front-end development and ability to communicate their ideas effectively. Additionally, a discussion-based assessment can be done remotely, which can be a more inclusive evaluation method.You can tell much about a person based on how they speak about a subject. It’s a red flag if you discuss with someone and they can’t provide you with at least one in-depth opinion on something related to their field of expertise, especially if you let them choose what to talk about.Project-based assessmentProject-based assessments involve providing the candidate with a real-world project that aligns with the work they would be doing if hired. While project-based assessments can present challenges and barriers for some candidates, they can also effectively evaluate front-end developers. This can give you a better sense of their ability to practically apply their front-end development skills. However, it’s essential to be mindful of the potential barriers to this assessment method, such as candidates not having the time or resources to complete the project.Inclusivity and considerationWhen evaluating front-end developer candidates, it’s essential to consider the impact of your assessment methods on different candidates. Some assessment methods, such as requiring candidates to spend their free time working on take-home projects, may be exclusionary. Therefore, it’s essential to be mindful of the potential barriers and to make accommodations for candidates with other responsibilities outside of work.When designing your hiring process, it’s essential to have a clear set of evaluation criteria and a consistent process for assessing candidates. This can help ensure that you’re evaluating candidates fairly and consistently. It’s also essential to be open to alternative assessment methods that can be more inclusive and effective, such as pair programming or discussion-based assessments.ConclusionThere are many ways to evaluate front-end developer candidates that don’t rely on coding puzzles and algorithms. By using various assessment methods and being mindful of how they impact different candidates, you can create a more inclusive hiring process that allows you to evaluate candidates effectively while supporting diversity and inclusivity in your workplace.The upside to all of these ways of hiring is they are remote-friendly and more realistic. Unless you’re hiring the candidate to write algorithms all day, maybe don’t ask those questions, or you risk losing quality talent. Not all developers are equal. Some are academically minded, and others are more practical.",1307
Writing a GitHub Actions File To Test Against Multiple Node.js Versions,"If you’re distributing a package, say a plugin, you may want to test it against multiple Node versions (especially if you’re using Jest for tests). In my situation, I have a popular plugin for Aurelia called Aurelia Google Maps. It’s for Aurelia 1, but many people use it, so I wanted to test it against the LTS of Node and the latest version.name: Node.js Jest Tests

on: [push, pull_request]

jobs:
  build-and-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'yarn'

    - name: Install dependencies
      run: yarn install

    - name: Run Jest tests
      run: yarn test

    strategy:
      matrix:
        node-version: [18.x, 19.x]
name: Node.js Jest Tests

on: [push, pull_request]

jobs:
  build-and-test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'yarn'

    - name: Install dependencies
      run: yarn install

    - name: Run Jest tests
      run: yarn test

    strategy:
      matrix:
        node-version: [18.x, 19.x]
You will want to save this file in your .github/workflows directory at the root of your project. You can save it as whatever you want: run-tests.yml.github/workflowsrun-tests.ymlAs you can see in the on part, the tests will run on every push and pull request. It’s also worth going in and ensuring you have some rules to ensure only pull requests that pass the tests meet the criteria for merging too.on",430
Why You Should Be Using globalThis Instead of Window In Your Javascript Code,"I hate to be “that guy” that publishes a blog post and says, “Stop using X” and “Why you should be using X instead”, but after a recent situation in some code I wrote ages ago and updated, I felt it was worthy writing a blog post about why you should use globalThis instead. That’s not to say if you’re currently using window that you’re wrong (because globalThis aliases window in a browser context). However, by using globalThis, you can save yourself a lot of trouble, especially in unit tests.If you’re not familiar with globalThis, it’s a new global object that was introduced in ECMAScript 2020. It provides a way to access the global object in any environment, whether a web browser, a Node.js server or a web worker. The main benefit of using globalThis instead of window or global is that it makes your code more consistent and future-proof.Here are a few examples of how globalThis works in different contexts:In a browser contextIn a web browser context, globalThis is essentially an alias for the window object. Here’s an example:console.log(globalThis === window); // true
console.log(globalThis.location.href); // same as window.location.hrefconsole.log(globalThis === window); // true
console.log(globalThis.location.href); // same as window.location.hrefBy using globalThis instead of window in your code, you can ensure that it will work in any web browser environment.In a Node.js contextIn a Node.js context, globalThis is an alias for the global object. Here’s an example:console.log(globalThis === global); // true
console.log(globalThis.setTimeout === global.setTimeout); // trueconsole.log(globalThis === global); // true
console.log(globalThis.setTimeout === global.setTimeout); // trueBy using globalThis instead of global in your code, you can ensure that it will work in any Node.js environment.In a web worker contextIn a web worker context, globalThis is the global object for the worker. Here’s an example:console.log(globalThis === self); // true
console.log(globalThis.postMessage === self.postMessage); // trueconsole.log(globalThis === self); // true
console.log(globalThis.postMessage === self.postMessage); // trueBy using globalThis instead of self in your code, you can ensure that it will work in any web worker environment.In unit testsOne of the benefits of using globalThis in your code is that it can make unit testing easier, especially if you need to mock global objects like window or global.For example, let’s say you have a function that depends on the window object:function showMessage(message) {
  window.alert(message);
}function showMessage(message) {
  window.alert(message);
}To test this function, you might need to mock the window object. However, mocking the window object can be tricky, especially if you want your tests to work in different environments. If you’re using Jest, the default test environment is Node where Window doesn’t exist, but global does.By using globalThis instead of window, you can make it easier to mock the window object in your tests:function showMessage(message) {
    globalThis.alert(message);
}function showMessage(message) {
    globalThis.alert(message);
}Now, when you’re writing tests for this function, you can mock the globalThis object instead of the window object:it('shows message', () => {
    const originalAlert = globalThis.alert;
    globalThis.alert = jest.fn();
    showMessage('Hello, world!');
    expect(globalThis.alert).toHaveBeenCalledWith('Hello, world!');
    globalThis.alert = originalAlert;
});it('shows message', () => {
    const originalAlert = globalThis.alert;
    globalThis.alert = jest.fn();
    showMessage('Hello, world!');
    expect(globalThis.alert).toHaveBeenCalledWith('Hello, world!');
    globalThis.alert = originalAlert;
});By using globalThis instead of window, you can write code that’s easier to test and works across different environments.In conclusion, by using globalThis in your code, you can make it more consistent, future-proof, and easier to test. Whether you’re working in a web browser, a Node.js server, or a web worker",1018
How To Configure Jest 28+ To Work With HTML Imports,"If you have an application that has HTML imports like this import template from './my-component.html and then attempt to test this code in Jest 28+ (or previous versions, for that matter), you will get a syntax error similar to this one:import template from './my-component.htmlSyntaxError: Unexpected token ‘<‘ HTMLFortunately, there is an easy fix. Firstly, you need to install the package jest-html-loader. npm install jest-html-loader -D. Then you need to configure your Jest configuration as follows.npm install jest-html-loader -D  ""transform"": {
    ""^.+\\.tsx?$"": ""ts-jest"",
    ""^.+\\.html?$"": ""jest-html-loader""
  },  ""transform"": {
    ""^.+\\.tsx?$"": ""ts-jest"",
    ""^.+\\.html?$"": ""jest-html-loader""
  },That is all you need to do. Your HTML imports will work and not throw errors during your test process.",205
I Still Sometimes Don’t Know What I Am Doing as a Senior Developer,"It’s been well over a decade since I started my journey as a front-end developer. I’ve worked on numerous projects, built and maintained websites, and developed applications. I have numerous open-source projects and am on the Aurelia Javascript framework core team. Over the years, I’ve gained experience and learned a lot of things, but there are still times when I feel like I don’t know what I’m doing.I know I’m not alone in this. As developers, we face a lot of challenges, big and small. From dealing with complex algorithms to fixing simple bugs, obstacles always exist. But despite my experience, I still get caught up in stupid bugs, struggle to install and configure packages and spend hours stuck on things that ultimately have simple solutions.I could not tell you how many hours I spent; sometimes, days stuck on a problem only to look at what I committed, a few lines of code.It can be frustrating, to say the least. Sometimes I feel like an imposter, wondering if I’m even cut out for this profession. But then I remind myself this is just part of the learning process. No matter how experienced you are, there will always be new challenges and new things to learn. It ebbs and flows that many experienced developers grapple with.One of the things that I’ve learned over the years is the importance of asking for help. Sometimes, all it takes is a fresh set of eyes to spot the solution to a problem that has been bugging me for hours. There’s no shame in not knowing everything, and it’s important to remember that there is always someone out there who knows more than you do.It’s easy to fall into the trap as an experienced developer that you are supposed to have all the answers, and even easier to find yourself stubbornly persisting on a problem someone else might be able to help you solve in a matter of minutes.I’ve also learned that taking breaks and stepping away from a problem is essential when you get stuck. When you’re staring at the same lines of code for hours, it’s easy to miss something obvious. Sometimes, a fresh perspective is all you need to find the solution, maybe a cup of coffee and a walk to get the blood flowing back to your neglected developer limbs.Despite the challenges and the moments of self-doubt, I still love being a developer. I love the feeling of finally solving a problem that had been bugging me for hours. I love the satisfaction of building something from scratch and seeing it come to life. And I love the fact that there is always something new to learn.So if you’re a developer who sometimes feels like they don’t know what they’re doing, know you’re not alone. We all have moments of self-doubt and frustration. But remember that every challenge is an opportunity to learn and grow, and there is always help available if needed. Being a senior developer isn’t about knowing everything or even being the fastest. It’s about knowing your limitations and how to work around them.",736
Where Did All The TypeScript Haters Go?,"TypeScript is a statically-typed superset of JavaScript introduced by Microsoft in 2012. It’s a language that some developers love to hate, but over the years, TypeScript has won over many sceptics, becoming an essential part of many modern JavaScript projects.You now have developers that once hated TypeScript liking it. Perhaps one of the most known developer advocates to get their TypeScript stance wrong was Eric Elliott, who published an article where he discusses a TypeScript tax. To Eric’s credit, he doesn’t say not to use TypeScript but attempts to argue against its use.once hated TypeScript liking itTypeScript taxOne of the most significant benefits of TypeScript is static type checking. By enforcing types at compile time, TypeScript can catch errors that might have gone unnoticed until runtime. This can save a lot of headaches and debugging time. It also makes code more maintainable and understandable since the types serve as documentation for the code.Another benefit of TypeScript is that it offers better IDE integration. With proper configuration, TypeScript enables editors to provide more accurate code completion and documentation, making writing and understanding code easier. This is especially useful in larger codebases with many dependencies.But it wasn’t always sunshine and rainbows for TypeScript. When it was first introduced, many developers were sceptical. They saw it as an unnecessary layer of complexity that only added more work to their already busy schedules. And while some of these developers are still around, many have since come to the idea that TypeScript has a lot of value.So, what changed? For starters, Microsoft has been committed to continuously improving TypeScript. Each new release addresses issues adds new features, and refines the language. This has gone a long way in winning over sceptics concerned about using a language they felt was still in its infancy.But there’s more to it than that. TypeScript has also evolved and become more accessible to developers over time. It’s easier to set up, with straightforward installation and configuration. As more people have started using TypeScript, the community has grown and become more supportive. There are plenty of resources and helpful developers out there who are willing to lend a hand.Furthermore, TypeScript has significantly increased in popularity with high-profile projects such as Angular, Vue, and Aurelia. These projects have adopted TypeScript, demonstrating its power in real-world applications. Seeing how these projects have benefited from TypeScript has encouraged many developers to try it.Of course, not everyone loves TypeScript, and that’s perfectly okay. However, for those who are still hesitant to try it out, it’s worth noting that TypeScript offers many benefits and is a tool that can make developers’ lives easier. With its static type checking, better IDE integration, and accessibility, TypeScript has earned its place in the modern JavaScript ecosystem.The TypeScript haters either started using TypeScript or ran out of valid arguments and had nothing left to say.",778
"The Mass Lay Offs Aren’t an Apocalyptic Sign of Recession, They Are a Much Needed Correction","The widespread tech layoffs over the past few months, and in January and February 2023 alone, have been causing concern for some. Is it a sign of a possible recession and economic avalanche that will see the unemployment rate skyrocket in different countries?I am not an economist, so this is just more observational. But, I don’t believe the widespread tech exodus we are currently witnessing is symptomatic of an economic storm battering down on the tech sector. It’s a correction.That’s not to say that the economy in many countries isn’t going down the toilet and, inflation despite peaking in many places still being stubbornly high. But, we are not quite at the point where high interest rates and constrained economic circumstances are causing businesses to start laying people off en masse.During the pandemic, when everyone was forced inside, and everyone could work remotely, tech companies went on a hiring spree. Apps like Zoom saw unprecedented growth. They needed to hire more people to tackle the scale. The same thing happened with Netflix and other companies. All of the big ones hired a lot. Google, in particular, has been on a hiring spree forever.However, people didn’t want to be stuck inside once the pandemic restrictions were lifted. The shift to virtual changed to physical. Somehow companies like Meta and Zoom thought pandemic hermit living would continue, and so would their profits.It’s a classic case of over-capitalisation. Companies expanded their workforce too quickly and too much. Mark Zuckerberg even admitted so much himself last year that he got it wrong.Despite big tech laying people off, smaller, better-run companies are still hiring. The ones that didn’t overextend themselves, naively believing the good times of the pandemic and stupidly high valuations would continue to soar.It’s one of the reasons that I closed my LinkedIn. My feed was doom and gloom; it was beginning to affect me, making me fear what 2023 would hold for me. It doesn’t help when you have a lot of people from these big tech companies as connections or followers talking about their last day or worse.The reality is we aren’t at the point where layoffs are happening because of recession fears. That’s not to say it’s not already happening, but it’s not the reason for the widespread layoffs we are currently seeing. Just some unfortunate fat-cutting from bloated big tech companies that have always operated under the illusion they are too big to fail.",618
The Forced Return to the Office Has Resulted in Decreased Productivity and Increased Cost of Living Pressure,"Just when you thought the return to office movement was gaining momentum, driven by high-profile companies and out-of-touch boomer CEOs that struggle to adapt to the new paradigm of flexible working, it appears there might be a few bumps in the road.Some data has come out on productivity from the U.S. Bureau of Labor Statistics since some workers have been forced back into the office, most notably showing that productivity has decreased. The irony of this is not lost; considering the justification for pushing workers back into the office has been driven by claims of in-person collaboration increasing productivity, the data seems to suggest otherwise.It also doesn’t help that many companies I have heard of mandating a return to the office have done so rather aggressively. The “return to the office or be fired” approach that some have taken, most notably Elon Musk is a staunch proponent of, could only ever result in decreased productivity. Then you have Amazon announcing a forced return to the office on May 1, 2023. forced return to the office on May 1, 2023If you have ever worked in an office before, you would know it’s full of distractions. I wish I had a non-biased way to quantify my productivity, but my productivity has been very high over the years I have worked remotely. I think part of that is I get sick less. I have kids in school, and since working remotely full-time, I’ve noticed I get sick maybe once or twice a year.Don’t just take my word for it, they have quantified some of this stuff since the 2020 pandemic, and people aren’t just slacking off playing games and watching Netflix while working remotely.quantified some of this stuff since the 2020 pandemicWhen the Integrated Benefits Institute surveyed workers in October 2022, it found employees who work remotely or in a hybrid environment indicated that they are more productive (21.8%), more satisfied (20.7%), and more highly engaged (50.8%). Those are significant numbers.surveyed workers in October 2022And in another October 2022 survey, Slack found that workers forced to return to the office were spending up to four hours on video calls daily. On average, workers were spending two hours in meetings a day. spending up to four hours on video calls dailyPeople are working remotely in offices. Isn’t it ironic during the pandemic, we were all doing video calls remotely, only for some to be forced back into the office and still work like they’re working remotely? It makes no sense.There are arguments that working remotely devoids employees of human connection. Maybe that’s true; maybe it’s not. In my industry, even working in an office, I had headphones on, and human interaction was reserved for pointless meetings, lunch and coffee trips. Since when do companies care about human connection?I have always described my in-office experiences to people like this, “I worked remotely in an office” detached from my surroundings, having to be a part of meetings where people were phoning in any way. I’m very fortunate that my job can be done remotely. I acknowledge that not every job can.Have you noticed that the most outspoken against remote work are primarily in their fifties, white and rich? But, remote work proponents are currently fighting a war against Elon Musk and Marc Andreessen, coincidentally also a billionaire. Andreessen claims that remote work is not a good life for young workers.not a good life for young workersCost of living pandemicThe COVID-19 pandemic might be over, but we are entering a new pandemic with no vaccine—the cost of living. Over the last few months, you might have noticed that a supermarket trip has been more costly than usual.While inflation is the primary contributing cause of the increased cost of living, despite inflation starting to fall in most places (U.K., USA and Australia especially), the cost of living crisis will continue throughout 2023 and beyond.will continue throughout 2023 and beyondThe price of eggs in the USA was up 150% from the year prior, prompting some U.S. lawmakers to demand an answer from egg companies as they reap record profits. Not everyone eats eggs, but it’s one of many components of the food chain that feeds into the price of other things and adds to the overall strain on household budgets.U.S. lawmakers to demand an answerOn top of that, companies forcing remote workers back into the office add to those costs—transportation being a big one. If you drive, that’s fuel, tolls, possibly paying for parking, the wear/tear on your vehicle and maintenance costs. Public transport can still add up.And then you have the social pressure of office lunches (the price of eating out has dramatically risen) and trips to get coffee. When I worked in an office, people often ate out and got coffee, and if you didn’t partake, you missed out on office relationship building and risked being seen as an outlier. Great ways to bond, but at what cost?Give people the choiceRemote work isn’t some new thing invented during the pandemic. A lot of companies have been successfully been working remotely for years. There are 100% remote work companies like 37 Signals that have been doing it very successfully, profitable and have high retention rates and satisfaction levels.But here is the thing. Not everyone wants to work remotely, and that’s okay. If you’re applying for a 100% remote company and you hate remote work, then that’s on you. But, if you’re working for a company with an office, it shouldn’t be either all, especially if you offered remote work during the pandemic and now trying to take it away.There is no reason why companies can’t offer both. Give those that want an office a physical space. Give those that want remote the ability to stay and work remotely. People should be free to work the way they want too. Ultimately, all that matters is people get their work done in a way that works for them.",1473
The Twitter Blue 2fa Fiasco,"Since taking the helm of Twitter, Chief Twit and manical entrepreneur Elon Musk has ruffled some feathers. From losing advertisers to claims he asked engineers to boost his popularity on the platform, it has been a wild ride.The latest wild ride is Twitter has announced two factor authentication using text messages will be a Twitter Blue only feature. There is this image circulating and people are upset.Is Twitter disabling text message two-factor authentication a security threat? Well, it is if you don’t configure something else in its place. Install Authy and spend the 2 minutes configuring it. Problem solved.Now, here is the thing. Twitter isn’t monetising all forms of 2fa, just text messages. You can still use an authenticator app like Authy or Google Authenticator. That’s what everyone should be using anyway. Twitter are doing users using this insecure form of security a solid here.Text messaging is very insecure. Over the years, there have been many high-profile attacks because of sim swapping especially. Jack Dorsey (the ex-CEO of Twitter) famously fell victim to a sim-swapping attack that saw hackers gain access to his Twitter account.famously fell victim to a sim-swapping attackNot many people probably realise this, but text messages are highly-insecure forms of communication. They are sent plaintext over cellular networks, and it is possible to intercept them using easily available hardware and software online.While it hasn’t been said out loud, the reason Twitter appears to be doing this is for cost-related reasons. It costs money to send text messages and based on the intensity of the backlash, it appears a lot of people used this form of 2fa (which I find quite worrying).So, the irony of this situation is that Twitter is doing non-paid subscribers a favour here by not allowing them to use one of the most insecure forms of 2fa around. Are text messages convenient? Absolutely. But, is it any less steps opening up an authentication app to get a code? No. Instead of a text message, it’s an authenticator app. Am I missing something here?",520
A Review of ChatGPT Plus,"After a lengthy free period of constant downtime and unreliability, ChatGPT has opened up its paid Plus plan to more people and wanting to see what the difference was between Plus and free, I signed up.For some, USD 20 might be too much to reconcile in this current economic climate. If you use ChatGPT as part of your daily tasks like some do, then $20 might be pretty valuable. If you’re a hobbyist user or just curious, $20 might be a cost you’ll have to weigh against your coffee budget.So, what do you get in ChatGPT for your twenty dollars?So, what do you get in ChatGPT for your twenty dollars?
Increased reliability (responses don’t stop part way through because the servers are overloaded)



Faster responses 



Higher limits (I seemingly hit the hourly rate a lot easier on the free version) 
Increased reliability (responses don’t stop part way through because the servers are overloaded)Faster responses Higher limits (I seemingly hit the hourly rate a lot easier on the free version) Here is the thing about ChatGPT Plus, it just feels better. I know it’s hardly a scientific answer, but everything feels faster, and the responses even feel more detailed and accurate (although that could be a side effect of the increased response speed). The reliability is better. Say goodbye to the constant downtime and responses cutting out part way through.The increased speed, because of the default ChatGPT Plus exclusive turbo option (which is just called default now), gives you 2.5x faster responses.And one thing with the free version of ChatGPT, I found myself hitting the free hourly limits more than you would expect. If you only need minimal use from it, then free might be fine (if the servers are not melting). However, I have not encountered any hourly limitations with the Plus-paid version.One thing that ChatGPT does, regardless of whether you’re a paid or non-paid subscriber, it has a response limit. Inevitably for lengthy responses, you’ll notice it finishes but doesn’t complete its response. I have a blog post about this here.hereFor some, Plus will be a hard sell. Although, if you’re using it to be more efficient in your job (I know many content writers are using it) and if it helps you save $20 in time quantified calculations, the subscription has paid for itself.I would imagine over time, as ChatGPT gets better, Plus subscribers will have first dibs on new features and improvements before paid users will.",611
Creating a Secure Password Generator Using TypeScript,"Password generation is something you’re hopefully using a password manager for these days. However, you might not be aware that modern browsers support some great crypto features.In this quick little tutorial, we will use the Crypto API to create a strong password generator. The code is remarkably simple, and you can adapt this to generate unique values for games and other purposes besides passwords./**
 * Generates a random password of the specified length.
 *
 * @param length The length of the password to generate.
 * @returns A Promise that resolves to a string containing the generated password.
 * @throws An error if the length argument is less than 1.
 */
const generatePassword = async (length: number): Promise<string> => {
  if (length < 1) {
    throw new Error('Length must be greater than 0');
  }

  // Create a new Uint8Array with the specified length.
  const buffer = new Uint8Array(length);

  // Get the browser's crypto object for generating random numbers.
  const crypto = window.crypto || (window as any).msCrypto; // For compatibility with IE11.

  // Generate random values and store them in the buffer.
  const array = await crypto.getRandomValues(buffer);

  // Initialize an empty string to hold the generated password.
  let password = '';

  // Define the characters that can be used in the password.
  const characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';

  // Iterate over the array of random values and add characters to the password.
  for (let i = 0; i < length; i++) {
    // Use the modulus operator to get a random index in the characters string
    // and add the corresponding character to the password.
    password += characters.charAt(array[i] % characters.length);
  }

  // Return the generated password.
  return password;
};/**
 * Generates a random password of the specified length.
 *
 * @param length The length of the password to generate.
 * @returns A Promise that resolves to a string containing the generated password.
 * @throws An error if the length argument is less than 1.
 */
const generatePassword = async (length: number): Promise<string> => {
  if (length < 1) {
    throw new Error('Length must be greater than 0');
  }

  // Create a new Uint8Array with the specified length.
  const buffer = new Uint8Array(length);

  // Get the browser's crypto object for generating random numbers.
  const crypto = window.crypto || (window as any).msCrypto; // For compatibility with IE11.

  // Generate random values and store them in the buffer.
  const array = await crypto.getRandomValues(buffer);

  // Initialize an empty string to hold the generated password.
  let password = '';

  // Define the characters that can be used in the password.
  const characters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789';

  // Iterate over the array of random values and add characters to the password.
  for (let i = 0; i < length; i++) {
    // Use the modulus operator to get a random index in the characters string
    // and add the corresponding character to the password.
    password += characters.charAt(array[i] % characters.length);
  }

  // Return the generated password.
  return password;
};As you can see, the getRandomValues method does all of the heavy lifting here for generating our password. We also define what characters are allowed in our password, allowing us to remove ambiguous characters if we wish.TestingWe will write some simple Jest unit tests to ensure our code works. The basics, such as password length and uniqueness.describe('generatePassword', () => {
  test('should return a string of the specified length', async () => {
    const password = await generatePassword(10);
    expect(typeof password).toBe('string');
    expect(password.length).toBe(10);
  });

  test('should throw an error if the length argument is less than 1', async () => {
    await expect(generatePassword(0)).rejects.toThrow();
    await expect(generatePassword(-1)).rejects.toThrow();
  });

  test('should generate different passwords for different calls', async () => {
    const password1 = await generatePassword(10);
    const password2 = await generatePassword(10);
    expect(password1).not.toBe(password2);
  });

  test('should only contain characters from the defined set', async () => {
    const password = await generatePassword(10);
    expect(password).toMatch(/^[A-Za-z0-9]+$/);
  });
});describe('generatePassword', () => {
  test('should return a string of the specified length', async () => {
    const password = await generatePassword(10);
    expect(typeof password).toBe('string');
    expect(password.length).toBe(10);
  });

  test('should throw an error if the length argument is less than 1', async () => {
    await expect(generatePassword(0)).rejects.toThrow();
    await expect(generatePassword(-1)).rejects.toThrow();
  });

  test('should generate different passwords for different calls', async () => {
    const password1 = await generatePassword(10);
    const password2 = await generatePassword(10);
    expect(password1).not.toBe(password2);
  });

  test('should only contain characters from the defined set', async () => {
    const password = await generatePassword(10);
    expect(password).toMatch(/^[A-Za-z0-9]+$/);
  });
});",1323
It’s Done When It’s Done,"There are more methodologies than you can shake a stick at. All promise to streamline your workflow and deliver quality software, many of which leverage the same old approach to work: estimation and timelines.I am not saying that timelines need to be replaced entirely. Because they are a necessary evil. However, in my experience, most companies putting deadlines on features and projects use arbitrary figures, not for a valid reason.What is a valid reason for a deadline?What is a valid reason for a deadline?
Complying with a regulatory or legal requirement



An event where you’re launching a new feature or product



A serious bug in your application that is breaking things for users (allowing you to communicate an approximate timeframe for when it will be fixed)



And a few other things
Complying with a regulatory or legal requirementAn event where you’re launching a new feature or productA serious bug in your application that is breaking things for users (allowing you to communicate an approximate timeframe for when it will be fixed)And a few other thingsThe truth is most companies aren’t using deadlines in this way. They’re using deadlines to push developers to fit their work inside a timed box. When you place artificial constraints on something, a few things happen.When you place artificial constraints on something, a few things happen.
Quality suffers as your developers rush to complete the work



Work/life balance can be impacted if your developers are forced to work additional unpaid hours to get things done



The work is most likely not being tested properly



Your developers are more likely to burn out



Your culture is going to be negatively impacted



Developers talk, and word ultimately gets out that your company is poorly managed and stopping short of offering way above market price, you’re going to be low on the list of companies people want to work for
Quality suffers as your developers rush to complete the workWork/life balance can be impacted if your developers are forced to work additional unpaid hours to get things doneThe work is most likely not being tested properlyYour developers are more likely to burn outYour culture is going to be negatively impactedDevelopers talk, and word ultimately gets out that your company is poorly managed and stopping short of offering way above market price, you’re going to be low on the list of companies people want to work forThis is where the idea of “shipping when it’s done” comes into play. Instead of focusing on arbitrary deadlines, the emphasis should be on ensuring the work is high quality and meets the customer’s needs.By allowing developers to work at their own pace and focusing on the outcome, you give them the freedom to use their expertise and creativity to deliver something great. This approach is better for the developers’ work/life balance and mental health and encourages collaboration and innovation.The benefits of shipping when it’s done include:The benefits of shipping when it’s done include:
Higher quality work, as developers have the time they need to test and refine the product thoroughly



Greater job satisfaction, as developers can take pride in their work and feel supported by their organization



Increased productivity, as developers are not constrained by an artificial timeline and can focus on delivering something truly valuable to the customer



Reduced technical debt, as developers have the time they need to ensure that the code is clean and maintainable



Better communication with customers, as they are kept informed of progress and can offer feedback along the way
Higher quality work, as developers have the time they need to test and refine the product thoroughlyGreater job satisfaction, as developers can take pride in their work and feel supported by their organizationIncreased productivity, as developers are not constrained by an artificial timeline and can focus on delivering something truly valuable to the customerReduced technical debt, as developers have the time they need to ensure that the code is clean and maintainableBetter communication with customers, as they are kept informed of progress and can offer feedback along the wayDelivering more minor features often is a critical aspect of the “ship when it’s done” approach. This allows companies to focus on delivering value to the customer as quickly as possible rather than waiting until a larger project is complete. It also allows flexibility to adjust priorities based on customer feedback or changing market conditions.By breaking down projects into smaller features or user stories, teams can focus on delivering working software more frequently. This approach helps identify potential problems earlier in the development cycle, saving time and resources in the long run.The benefits of continuous delivery include:The benefits of continuous delivery include:
Greater agility: By delivering smaller features more frequently, companies can respond more quickly to customer needs and market changes.



More accurate feedback: Frequent releases allow for more accurate feedback from users, which can help teams make more informed decisions about future development.



Reduced risk: Smaller releases are less risky than larger, all-or-nothing releases, as they allow for quick course correction if something goes wrong.



Increased motivation: Developers are more likely to be motivated by seeing the results of their work sooner rather than later. This is crucial for new hires especially.



Better team collaboration: Frequent releases encourage team collaboration and communication, which can lead to more creative problem-solving and better products.
Greater agility: By delivering smaller features more frequently, companies can respond more quickly to customer needs and market changes.More accurate feedback: Frequent releases allow for more accurate feedback from users, which can help teams make more informed decisions about future development.Reduced risk: Smaller releases are less risky than larger, all-or-nothing releases, as they allow for quick course correction if something goes wrong.Increased motivation: Developers are more likely to be motivated by seeing the results of their work sooner rather than later. This is crucial for new hires especially.Better team collaboration: Frequent releases encourage team collaboration and communication, which can lead to more creative problem-solving and better products.In conclusion, focusing on delivering smaller features often is a crucial component of the “ship when it’s done” approach. By prioritizing outcomes over time, companies can deliver value to their customers more frequently while fostering greater agility, reducing risk, and improving collaboration within their development teams.",1699
Samsung Galaxy S23 Ultra review,"Samsung is making yearly phone upgrades obsolete. I used my beloved Galaxy Note 10+ Plus until its dying breath. Even when the charging port failed and could only be charged wireless, I persisted until one day; it refused to charge.I have been using the Samsung Galaxy S21 Ultra for the last couple of years. It’s no Note, but it’s one of the best phones I have ever owned. Even now, the hardware specifications of the S21 hold up against newer devices.The pandemic dampened innovation as components became more challenging for companies to obtain. Combined with rampant inflation and increased cost of living, people aren’t buying new phones yearly as they used to. Made even more evident by the fact that it’s now commonplace for Australian telcos to offer 36-month plans.There isn’t much reason to upgrade to the latest smartphone like there used to be.If you’re upgrading from the Galaxy S22, the S23 might not have much to offer you besides camera improvements and better hardware specs. But, if you’re coming from the S21 or older, the Galaxy S23 comes as close to a modern Galaxy Note as possible.Since the S21, Samsung has trimmed down what comes in the box. Minimal packaging, a USB-C cable and a metal pin for opening the sim card tray. There are some pieces of paper for the warranty and stuff too. Like other manufacturers, you don’t get a charger anymore, but you probably already have one.The S23 Ultra is the new Samsung Galaxy Note.The S Pen now ships with the S23 Ultra with a dedicated spot on the phone to hold it. For those like myself that use it, it’s incredible for taking notes and interacting with your device like a boss. Once you get used to the S Pen, you can interact with your phone in ways your fingers could only dream of. It was one of the best things about the Note series.Essentially, the Galaxy S23 Ultra feels like a Samsung device from the last few models. The interface has had some subtle changes, and Samsung has reduced the preinstalled junk, but there is nothing distinctive from a UI perspective on the S23 that makes it stand out.Even though the phone isn’t the same as the S22, dimension-wise, they’re essentially the same in almost every other way. The phone is less rounded on the edges and feels more square, which makes more of a difference than you would expect when you hold it. That is one of the criticisms of my S21 Ultra, the rounded corners made it uncomfortable to hold one-handed for long periods.SpecsWhere things begin to differ from my S21 Ultra is the speed. Specs-wise, it’s a no-brainer that the S23 Ultra is bigger and better on every level. Finally, Samsung is shipping just the Snapdragon chipset to everyone instead of Exynos and Snapdragon. You don’t have to be a hardware geek to know that Snapdragon outperformed Exynos consistently in most benchmarks on previous Samsung Galaxy phones.Things feel snappier. The battery drains slower, tabbing between browser tabs in Chrome feels faster, and even auto-completing with 1Password into fields when using my favourite password manager feels faster. Maybe it’s just new phone energy, but I suspect it’s because there is more ram and a faster chipset. Things are happening more quickly.The Camera = wowThe Camera is noticeably better than my S21 Ultra in both photo and video. And let’s be honest, the essential feature of a smartphone isn’t how fast it can run a graphically intensive game; it’s the camera. It’s the thing you use your phone for the most, probably even more than making calls.It takes better nighttime shots that blur less, has better clarity, and has a monster 200mp camera. Although, you won’t need 200-megapixel photos unless you’re taking photos for print billboards or something. An understated feature they have added is multi-timed photos. Instead of setting a timer to take a single photo, you can take multiple timed photos and set the interval between each photo.Arguably, the best camera feature is the new Expert RAW feature, and it’s not even shipped with the phone.If/when you get the S23 Ultra, you will want to download the Expert RAW app. This gives you a powerful new RAW mode that is more configurable than ever. An astrophotography mode is a new feature that can take those long exposure shots of the sky and stars for 10 minutes. You’ll want a tripod, as the lowest exposure is still 4 minutes.And then you have the video capabilities. With Super Steady, Samsung has finally made the feature worthy of the title. You can take videos with rapid movement, and they’re pretty smooth, with the white balance no longer flickering like a Christmas tree. Finally, 8k is usable on a smartphone, with the S23 Ultra offering 8k video at 30fps, which makes a substantial difference compared to the S22 Ultra and other 8k-capable phones.The space zoom feature, which promises 100x zoom, is still a gimmick. The photo quality of 100x photos is not usable at all. A cool trick to show your friends, but you wouldn’t post those photos to social media or use them for anything.The autofocus is perhaps the most significant improvement to the Camera many won’t talk about. Look, the S21 wasn’t terrible, but it had autofocus issues. I read how the S22 also had similar issues with autofocus. To the point where it became a thing synonymous with Samsung cameras. In the S23 Ultra, they stepped things up; autofocus is finally fixed, from what I can see. The autofocus tracking feature works well, up to 4k 60fps video.Don’t just take my word for it. See the YouTube video that made me decide on the S23 Ultra:

The score given to the S23 Ultra is 7.5 vs the iPhone 14 Pro Max at 4.5. The reviewer in this video highlights how Samsung has seemingly caught up to Apple, which has had the better smartphone camera for years.ConclusionThe cost of the S23 Ultra is nothing to sneeze at. The total price upfront is a hefty chunk of change. Although, if you shop around, you can get a good discount on the S23 if you go on a plan. I managed to get the 1 TB model for the price of the 512 GB version through Optus here in Australia when I pre-ordered.S23 UltraIf you’re coming from the S22, I won’t rush to upgrade unless you want to. However, I upgraded from the S21 Ultra, and the difference in camera quality and overall improvement of the phone that addresses many outstanding Samsung-esque issues made it worthwhile.",1588
ChatGPT Stopping Part Way Through a Response? Here Is How You Fix It,"The ChatGPT tool is possibly part of your everyday workflow. I’ve been using it for research purposes and as a writing assistant, where it thrives at. However, one of the issues with ChatGPT is that it has a limitation on its response length.You write in a prompt, and ChatGPT starts generating its response. All of a sudden, it stops. I hope OpenAI one day increases the response limit because, besides the fact, there is a way to fix it, it’s still frustrating.The solution is simple: when it stops, write “continue” (without the double quotes), and it will continue to complete its response.Now, this isn’t a permanent fix. You will have to do this every time it happens. And if you think paying for ChatGPT Plus will increase the response limit, it doesn’t. The same issue exists for both the free and paid versions of ChatGPT.",208
Happy AI Valentine’s Day,"Valentine’s Day is upon us and this year that heartfelt card you get from your significant other this year might be written using AI. Specifically, ChatGPT which has taken the internet by storm.In fact, I asked ChatGPT to write a really heartfelt card message and this is what it produced.My dearest [Wife's Name],

On this Valentine's Day, I want to take a moment to express how grateful I am to have you in my life. You are my rock, my support, and my best friend. You have brought so much joy and happiness into my life and I am forever grateful for your love.

I remember the moment I first laid eyes on you and I knew that you were the one for me. Every day since then, my love for you has only grown stronger. You make me want to be a better man and I am so proud to call you my wife.

You are my everything, my sunshine on a cloudy day, and the beat in my heart. I am so grateful for all the amazing memories we have shared together and I can't wait to make many more in the future.

I love you now and always. Happy Valentine's Day, my love.

Forever and always,
[Your Name]Excuse me for a moment while I dry the tears from my eyes. How heartfelt and sincere.Admittedly, ChatGPT is an impressive tool, but it doesn’t always get things right. From a writing perspective, it is incredible. I know people out there are using it for everything, but as a writing assistant ChatGPT is impressive. The results it produces are a bit robotic, but with some prompting, it can be made to produce outstanding results.I am not adverse to AI, but using it to write a heartfelt message for your partner is peak laziness. I understand we all live busy lives, but what kind of life are we living if we can’t stop for ten minutes and think of some nice things to say about those we love without asking an AI tool to do it for us?On the same hand, Valentine’s Day is a terrible day in itself. One of the most infamous “Hallmark holidays” societal pressure from social media means that these days designed to sell cards, chocolate and flowers will continue. You don’t want your partner seeing others being spoiled and you didn’t get them anything, do you?If we need a specially marked day in the calendar to tell our partners we love them and buy them flowers, where is the romance? If anything, it means more to surprise someone with chocolate and roses on days that aren’t Valentine’s Day, right?Celebrate love on days other than Valentine’s Day, it will mean so much more.",616
A real threat has emerged to challenge Google and it might be Microsoft,"The term “googling” is synonymous with searching for things online. For years, Google has enjoyed a monopoly on search, advertising, and other facets of internet life. As Yahoo! fades into the ether and Microsoft’s Bing exists but isn’t used much (who says “binging” or “bing it”?), we are starting to see competition heat up in the search space are decades of Google dominance.The threat to Google is ironically coming from Microsoft.In 2019, Microsoft invested $1b into OpenAI and another $10b into OpenAI in January 2023. That’s a serious chunk of change to invest in an economic climate of high-interest rates, layoffs in tech and other uncertainty.When OpenAI debuted ChatGPT in late 2022, it was a surprising hit—taking just two months to reach 100 million active users. The kind of fear that other companies took years to achieve (TikTok being the exception at nine months).Suddenly, the success and hype around ChatGPT were starting to look good for Microsoft’s initial investment. But things didn’t end there.And then Microsoft threw down the gauntlets again when it announced it was integrating ChatGPT into its Bing search engine. Not only was the integrated version of ChatGPT more advanced when Microsoft debuted it, but it could also recommend search results and act as an assistant.Google was left with an egg on its face. They subsequently announced their own search-assisted AI called Bard. But, when it came time to demo it, it turns out the information in the demo wasn’t accurate and so came tumbling down the share price for Google.It appears that Google is being challenged in its own kingdom by Microsoft of all companies. AI is shaping up to disrupt traditional industries as we know them.",428
Why Don’t You Hear Much About GraphQL These Days?,"You might have noticed that you don’t hear about GraphQL as much as you used to. Some people might have you believe that developers have lost interest in this technology, but that couldn’t be further from the truth. This blog post will explore why GraphQL isn’t as talked about as it used to be and why it’s still a relevant and valuable technology for developers.Maturity and stabilityGraphQL has come a long way since its introduction a few years ago. As more and more companies adopt GraphQL, the technology has become more mature and stable. The growing number of real-world applications built with GraphQL has resulted in fewer bugs and compatibility issues, making GraphQL a reliable and predictable technology for developers. With its growing popularity, GraphQL has become a well-established technology widely accepted and used in the industry, reducing the need for developers to talk about and evangelize GraphQL.However, that’s not to say that GraphQL has stopped evolving. The technology continues to evolve and improve, and developers are finding new and innovative ways to use GraphQL to solve complex problems. With its growing popularity and continued evolution, GraphQL is set to remain a valuable and relevant technology for years to come.It’s no longer new and shinyWhen GraphQL was first introduced, it was a new and exciting technology that developers were eager to talk about and share with others. The novelty and excitement surrounding GraphQL were palpable, and developers were eager to explore the potential of this new technology. However, as GraphQL has become more widely adopted and its use has become more common, the novelty and excitement surrounding GraphQL have diminished.This is a natural progression for any new technology. As new things like GraphQL become more established and widely used, the need to talk about it decreases, and the focus shifts to practical usage and problem-solving. Despite this shift, GraphQL remains a valuable and relevant technology, and developers continue to find new and innovative ways to use GraphQL to solve real-world problems.As you can see in the 2022 State of The API report, despite REST being the dominant approach to building APIs, GraphQL is still growing. Growing to 28% usage, up from 24% the year prior in surveyed responded.GraphQL is still growingFurthermore, the 2022 State of GraphQL report shows that GraphQL is maturing, with many respondents reporting three to five years of experience.2022 State of GraphQL report showsFocus on practical usageGraphQL is now being used in production by many companies, and the focus has shifted from talking about GraphQL to using it to build applications. Developers are now more focused on using GraphQL to solve real-world problems rather than discussing its potential. This shift in focus has contributed to a decrease in the amount of discussion and chatter about GraphQL, but it doesn’t mean that developers have lost interest in the technology.In fact, as GraphQL becomes more widely used and accepted, developers are becoming increasingly familiar with its capabilities and limitations. This increased familiarity has allowed developers to use GraphQL more creatively and innovatively, leading to new and exciting technology applications. With its growing popularity and continued evolution, GraphQL is set to remain a valuable and relevant technology for years to come.PayPal was an early adopter of GraphQL. Similarly, GitHub also embraced GraphQL quite early. Both companies are still using GraphQL in production and many others, including; Soundcloud, Netflix and plenty of other known companies.SoundcloudNetflixJust because you don’t hear GraphQL dominate the buzz cycle anymore doesn’t mean it’s dead. If anything, the dying down of the hype is a good thing.",949
Sentiment Analysis Using TypeScript Without Dependencies,"Sentiment analysis is usually a task that requires a specialised dataset and machine-learning techniques to implement properly. However, I thought it might be a nice exercise to try and implement sentiment analysis with TypeScript without training models.// Define a type for the sentiment result
type Sentiment = 'positive' | 'neutral' | 'negative';

// Class to perform sentiment analysis on a given text
class SentimentAnalysis {
  // Arrays of positive and negative words to use in the analysis
  private positiveWords = [""love"", ""like"", ""great"", ""good"", ""happy"", ""awesome""];
  private negativeWords = [""hate"", ""dislike"", ""bad"", ""angry"", ""sad"", ""terrible""];

  // Method to perform sentiment analysis on a given text
  public getSentiment(text: string): { sentiment: Sentiment, positiveWords: number, negativeWords: number, neutralWords: number } {
    // Convert the text to lowercase to make the analysis case-insensitive
    const lowerText = text.toLowerCase();

    // Sum up the number of times each positive word appears in the text
    let positiveScore = this.positiveWords.reduce((acc, word) => {
      // Use a regular expression to match the word in the text
      return acc + (lowerText.match(new RegExp(word, 'g')) || []).length;
    }, 0);

    // Sum up the number of times each negative word appears in the text
    let negativeScore = this.negativeWords.reduce((acc, word) => {
      // Use a regular expression to match the word in the text
      return acc + (lowerText.match(new RegExp(word, 'g')) || []).length;
    }, 0);

    // Calculate the number of neutral words by subtracting the positive and negative words from the total number of words
    let neutralScore = lowerText.split(' ').length - positiveScore - negativeScore;

    // Compare the number of positive and negative words and return the sentiment result
    if (positiveScore > negativeScore) {
      return { sentiment: ""positive"", positiveWords: positiveScore, negativeWords: negativeScore, neutralWords: neutralScore };
    } else if (positiveScore < negativeScore) {
      return { sentiment: ""negative"", positiveWords: positiveScore, negativeWords: negativeScore, neutralWords: neutralScore };
    } else {
      return { sentiment: ""neutral"", positiveWords: positiveScore, negativeWords: negativeScore, neutralWords: neutralScore };
    }
  }
}

// Create an instance of the SentimentAnalysis class
const sentimentAnalysis = new SentimentAnalysis();

// Analyze some sample text
const result = sentimentAnalysis.getSentiment(""I love this code and think it is great!"");

// Log the result
console.log(result);// Define a type for the sentiment result
type Sentiment = 'positive' | 'neutral' | 'negative';

// Class to perform sentiment analysis on a given text
class SentimentAnalysis {
  // Arrays of positive and negative words to use in the analysis
  private positiveWords = [""love"", ""like"", ""great"", ""good"", ""happy"", ""awesome""];
  private negativeWords = [""hate"", ""dislike"", ""bad"", ""angry"", ""sad"", ""terrible""];

  // Method to perform sentiment analysis on a given text
  public getSentiment(text: string): { sentiment: Sentiment, positiveWords: number, negativeWords: number, neutralWords: number } {
    // Convert the text to lowercase to make the analysis case-insensitive
    const lowerText = text.toLowerCase();

    // Sum up the number of times each positive word appears in the text
    let positiveScore = this.positiveWords.reduce((acc, word) => {
      // Use a regular expression to match the word in the text
      return acc + (lowerText.match(new RegExp(word, 'g')) || []).length;
    }, 0);

    // Sum up the number of times each negative word appears in the text
    let negativeScore = this.negativeWords.reduce((acc, word) => {
      // Use a regular expression to match the word in the text
      return acc + (lowerText.match(new RegExp(word, 'g')) || []).length;
    }, 0);

    // Calculate the number of neutral words by subtracting the positive and negative words from the total number of words
    let neutralScore = lowerText.split(' ').length - positiveScore - negativeScore;

    // Compare the number of positive and negative words and return the sentiment result
    if (positiveScore > negativeScore) {
      return { sentiment: ""positive"", positiveWords: positiveScore, negativeWords: negativeScore, neutralWords: neutralScore };
    } else if (positiveScore < negativeScore) {
      return { sentiment: ""negative"", positiveWords: positiveScore, negativeWords: negativeScore, neutralWords: neutralScore };
    } else {
      return { sentiment: ""neutral"", positiveWords: positiveScore, negativeWords: negativeScore, neutralWords: neutralScore };
    }
  }
}

// Create an instance of the SentimentAnalysis class
const sentimentAnalysis = new SentimentAnalysis();

// Analyze some sample text
const result = sentimentAnalysis.getSentiment(""I love this code and think it is great!"");

// Log the result
console.log(result);This code uses a class called SentimentAnalysis to perform sentiment analysis on a given text. The class has two arrays of positive and negative words and a method called getSentiment that performs the analysis. SentimentAnalysisgetSentimentThe method first converts the text to lowercase and then uses two calls to reduce, to sum up, the number of times each positive and negative word appears in the text. It then calculates the number of neutral words by subtracting the positive and negative words from the total number of words. Finally, it compares the number of positive and negative words and returns a result that includes the sentiment and the number of positive, negative, and neutral words found.While this code isn’t scientific because it doesn’t perform a deep analysis beyond breaking up a string into words, nor does it account for all positive, negative and neutral words, it’s a cool little implementation you could use for basic purposes.Testing our codeBecause testing is very important and provides a great way to understand code, we will write some Jest unit tests for our sentiment analysis. It’s a basic implementation, but writing tests is a good habit to get into.import SentimentAnalysis from ""./sentiment-analysis"";

describe(""SentimentAnalysis"", () => {
  let sentimentAnalysis: SentimentAnalysis;

  beforeEach(() => {
    sentimentAnalysis = new SentimentAnalysis();
  });

  test(""should return positive sentiment for positive text"", () => {
    const result = sentimentAnalysis.getSentiment(""I love this code and think it is great!"");
    expect(result).toEqual({ sentiment: ""positive"", positiveWords: 2, negativeWords: 0, neutralWords: 7 });
  });

  test(""should return negative sentiment for negative text"", () => {
    const result = sentimentAnalysis.getSentiment(""I hate this code and think it is terrible!"");
    expect(result).toEqual({ sentiment: ""negative"", positiveWords: 0, negativeWords: 2, neutralWords: 7 });
  });

  test(""should return neutral sentiment for neutral text"", () => {
    const result = sentimentAnalysis.getSentiment(""This code is just okay."");
    expect(result).toEqual({ sentiment: ""neutral"", positiveWords: 0, negativeWords: 0, neutralWords: 3 });
  });
});import SentimentAnalysis from ""./sentiment-analysis"";

describe(""SentimentAnalysis"", () => {
  let sentimentAnalysis: SentimentAnalysis;

  beforeEach(() => {
    sentimentAnalysis = new SentimentAnalysis();
  });

  test(""should return positive sentiment for positive text"", () => {
    const result = sentimentAnalysis.getSentiment(""I love this code and think it is great!"");
    expect(result).toEqual({ sentiment: ""positive"", positiveWords: 2, negativeWords: 0, neutralWords: 7 });
  });

  test(""should return negative sentiment for negative text"", () => {
    const result = sentimentAnalysis.getSentiment(""I hate this code and think it is terrible!"");
    expect(result).toEqual({ sentiment: ""negative"", positiveWords: 0, negativeWords: 2, neutralWords: 7 });
  });

  test(""should return neutral sentiment for neutral text"", () => {
    const result = sentimentAnalysis.getSentiment(""This code is just okay."");
    expect(result).toEqual({ sentiment: ""neutral"", positiveWords: 0, negativeWords: 0, neutralWords: 3 });
  });
});Our test cases are basic, but we check that we return the three different types of sentiment from our code. We aren’t handling invalid values or other edge cases you might have in a proper bunch of tests.",2106
Handling Errors with the Fetch API,"The Fetch API is a modern and efficient way to retrieve resources from a server. It is an interface that provides a unified way to fetch resources from different sources. Fetch makes sending HTTP requests, including GET and POST, easy and handles responses asynchronously.However, handling errors properly is important when working with the Fetch API. Errors can occur for various reasons, such as a network error, a server-side error, or an invalid URL. Failing to handle these errors can result in unexpected behaviour and break your application.This article will look at several ways to handle errors using the Fetch API with Async/Await.1. Using Try/Catch BlocksTry-catch blocks are one of the most straightforward ways to handle errors when using Fetch. You can wrap the fetch call in a try block and catch any errors that occur in the catch block.async function fetchData(url) {
  try {
    const response = await fetch(url);
    const data = await response.json();
    // Use the data as needed
  } catch (error) {
    console.error(error);
  }
}async function fetchData(url) {
  try {
    const response = await fetch(url);
    const data = await response.json();
    // Use the data as needed
  } catch (error) {
    console.error(error);
  }
}This approach is easy to implement and works well for basic error handling. However, it only works for errors thrown by the fetch function itself. If the response from the server indicates an error, such as a 404 Not Found or 500 Internal Server Error, this approach won’t catch those errors.2. Checking the response statusYou can check the response status in the returned Promise to handle server-side errors. The fetch function returns a Promise that resolves to a Response object. This Response object has a status property that indicates the HTTP status code of the response.async function fetchData(url) {
  try {
    const response = await fetch(url);
    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }
    const data = await response.json();
    // Use the data as needed
  } catch (error) {
    console.error(error);
  }
}async function fetchData(url) {
  try {
    const response = await fetch(url);
    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }
    const data = await response.json();
    // Use the data as needed
  } catch (error) {
    console.error(error);
  }
}In this example, we use the response.ok property, which returns a Boolean indicating whether the HTTP status code is in the 200-299 range (i.e., a success status). If the response is not okay, we throw a new error that includes the HTTP status code.response.ok3. Handling specific error status codesSometimes, you may need to handle specific HTTP error status codes in a specific way. For example, you may want to show a different error message for a 404 Not Found error than for a 500 Internal Server Error. Sometimes, you may want to check for an authorized error response and handle it differently.async function fetchData(url) {
  const response = await fetch(url);
  if (response.status === 404) {
    throw new Error('Page not found');
  } else if (response.status === 500) {
    throw new Error('Server error');
  } else if (!response.ok) {
    throw new Error(`HTTP error! status: ${response.status}`);
  }
  const data = await response.json();
  return data;
}async function fetchData(url) {
  const response = await fetch(url);
  if (response.status === 404) {
    throw new Error('Page not found');
  } else if (response.status === 500) {
    throw new Error('Server error');
  } else if (!response.ok) {
    throw new Error(`HTTP error! status: ${response.status}`);
  }
  const data = await response.json();
  return data;
}This example uses a series of if statements to handle specific HTTP status codes. We throw different errors for each HTTP status code and catch them in the calling code if needed.4. CombineWe can take all the above error-handling techniques and create a function that makes a Fetch request and uses checks alongside a try/catch to handle errors.async function fetchData(url) {
  try {
    const response = await fetch(url);
    if (response.status === 404) {
      throw new Error('Page not found');
    } else if (response.status === 500) {
      throw new Error('Server error');
    } else if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }
    const data = await response.json();
    return data;
  } catch (error) {
    console.error(error);
  }
}async function fetchData(url) {
  try {
    const response = await fetch(url);
    if (response.status === 404) {
      throw new Error('Page not found');
    } else if (response.status === 500) {
      throw new Error('Server error');
    } else if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }
    const data = await response.json();
    return data;
  } catch (error) {
    console.error(error);
  }
}This function uses a try-catch block to handle errors thrown by the fetch function. It also checks the response status to handle server-side errors and specific HTTP error status codes. If an error occurs, it logs the error to the console.You can use this function or modify it to fit your specific needs. For example, you could add additional error-handling logic or display an error message to the user instead of logging the error to the console. Or, use ranges instead of specific status code checks. But, as you can see, error handling with Fetch is a breeze. It’s a lot nicer than the days of using XMLHttpRequest.",1404
The PlayStation VR 2 (PSVR 2) Headset Needs PC Modders to Survive,"I tend to get excited about virtual reality and have wanted to see it succeed for over a decade. While VR has undoubtedly grown, it’s not mainstream due to the barrier to entry. Most notably, requiring beefy PC setups or locked down (like the original PSVR headset). Adding to my growing collection of VR headsets, I preordered the PSVR 2 headset for the PlayStation 5.There has been work done on untethered VR headsets, failed beginnings with Samsung creating phone VR headsets before Meta struck gold with the Meta Quest. Not only does it support using it by itself, but it can also be connected to a computer. Then you have the heavy, expensive hitters like the HTC Vive Pro 2 for powerful computer VR experiences.This is why everyone was shocked when Sony announced PSVR 2 and its incredible specs:
2000 x 2040 per eye panel resolution



OLED panels



Supporting 90Hz and 120Hz refresh rates



Four embedded cameras for headset and controller tracking



Embedded IR trackers for each eye



A vast array of sensors, including proximity sensors



Built-in microphone



All are driven by a single USB Type-C cable
2000 x 2040 per eye panel resolutionOLED panelsSupporting 90Hz and 120Hz refresh ratesFour embedded cameras for headset and controller trackingEmbedded IR trackers for each eyeA vast array of sensors, including proximity sensorsBuilt-in microphoneAll are driven by a single USB Type-C cableNow, for a virtual reality headset, these are high-end specs. It puts the PSVR 2 headset right at the top of the list of headsets (spec-wise). All of the reviews so far seem to be highly favourable of the headset, especially with Sony, including haptic feedback and other additions to make the VR experience more immersive.However, the cost the PSVR 2 headset is expensive. It costs more than the console itself to buy. A luxury that not a lot of people have right now with increasing interest rate increases, spiralling cost of living and uncertainty as the world edges closer to recession.This is why the PSVR 2 will need the help of PC modders to survive. While I have no doubt those that can afford this headset will love it (myself included when it arrives), the PSVR 2 headset has the advantage of being close to the specs of the coveted HTC Vive Pro 2 headset (which is over double the cost of the PSVR 2 headset).Support will inevitably come if Sony inevitably caves in and provides an official way to use the PlayStation VR 2 headset on a PC or modders find a way. The fact a single USB-C cable is all you need provides hope that the path to PC support won’t be as difficult as you might think. Better still, imagine if Sony had the foresight to monetise support and provided a paid add-on you could use to use the PSVR 2 headset on PC. It would mean they profit from PC support, which doesn’t diminish its exclusivity on the PS5.",713
Hogwarts Legacy: PlayStation 5 Review,"Harry Potter fans have waited for an open-world Harry Potter game for almost two decades, and it’s crazy to think that we may have finally got what we have been asking for. After a lengthy wait and fear of delays, it’s finally here.So, the question is: does Hogwarts Legacy live up to the expectations, or do we have another No Man’s Sky and Cyberpunk 2077 situation on our hands?From the beginning of the game, you’re thrown into the action. I won’t spoil the gameplay and storyline. The game does a good job of getting you to move around. At the start, you explore on foot, and then you get to use a broomstick. The game does a good job guiding you through Hogwarts, introducing you to the teachers, your classmates, and the things you need.It’s Beautiful.From a graphics perspective, the game looks better than the official trailers and clips gave it credit for. Hogwarts Legacy looks incredible, and despite the fact it’s not a massive open world, it’s big enough it feels exciting, and there is plenty to explore. I didn’t test to see what the game looked like with performance mode turned on. I chose to prioritise graphics over performance. And despite that, the game performs well, even during intense battles.It’s rare for a licenced IP like this to be so well executed. After being burned by other titles promising the world over the years and not delivering, it’s a shock to see a game so well done right out of the gate. There are weird little glitches and things with NPCs, but I have not encountered anything game-breaking like I did when Cyberpunk 2077 launched.The depth of this game is deep and immersive. Not only does it do the lore of the Harry Potter universe justice, but it’s filled with secrets, puzzles, side questions and magic that keep you wanting to play the game.Impressive CombatThe combat system is way better than I expected. You start off with basic spells and then learn more advanced ones as you progress. You can duel to practice, even against other groups. Not only can you perform spells, but you can also combo (which becomes quite important as you progress through the game). Once again, the fighting is way better than any official video clips released prior.You can’t get by just blocking and fighting back when you start to go up in the difficulty levels. A degree of skill is required to win some of the fights you find yourself in, requiring you to choose the right spells and combos.And complementing the combat is a great use of the PlayStation 5 Duelsense controller. The tactile feedback you get, the vibrations and feeling you get during battle, is surprisingly helpful.Character CreationDuring the initial character creation, I found all the characters had a same-samey vibe. It’s hard to explain. But, no matter how hard you try to create a unique character, it’ll end up looking like a character you’ll always see.Where this, fortunately, is fixed is the endless clothing options. You can go wild changing your outfits and style in the game. As you progress, you can get new items and give your character much-needed personalisation that addresses this.But let’s be honest: who buys this game for the customisation options anyway? You want to fly around the wizarding world of Hogwarts and its surroundings. It’s a tiny insignificant downside.Great sound designThe voice acting was mostly fantastic. There did appear to be times when some of the characters you meet sounded like they were hired from Fiverr, but for the most part, the game is consistent in its delivery of character voicing from the main character to most of the NPCs.Surprisingly, the thing I was shocked about the most was the music. Instead of just unoriginally copying music from the Harry Potter films, the game offers a unique take on Harry Potter’s mysterious and magical music you would associate with the films but offers a unique spin on it that makes the music feel like its own thing.You will also notice the variations in the music during different scenes and battles, like the music was curated for a film and not a game. This is the kind of detail many games overlook and it makes a much bigger difference to the overall atmosphere of the game than you would think.There is no Quidditch.There is no Quidditch.Before the game was released, we knew there would be no Quidditch. And in the game, there is an explanation for why there is no Quidditch. Oddly enough, despite the reasoning, there is still no Quidditch arena, which seems odd.Many fans hope we inevitably get a Quidditch DLC of some kind, which would make this game even more perfect. And I have hope that this will happen, perhaps with changes to the storyline that allows Quidditch to return to the game universe. We already have the flying mechanics; let’s get it in there.ConclusionIf you were on the fence, get down. Hogwarts Legacy has all but cemented it will be a serious game-of-the-year contender, and it will take some serious competition for it to be dethroned. It’s immersive, mysterious, exciting and a solid IP title.Given the game’s success and how well-received it is from critics and those already playing it, Avalance and Warner Brothers would be crazy not to pump out a few DLC packs for this game. They have a solid foundation here; they can expand with many new things (hopefully Quidditch).This title was worthy of a pre-order (which I did and got early) and a day-one purchase. Don’t wait for it to go on sale. This game exceeds my expectations and most likely will exceed yours too.",1376
Is Asking Developers How to Write FizzBuzz Outdated?,"I have never understood why FizzBuzz was deemed a means of screening developers. The idea is for multiples of 3; you print Fizz. For multiples of 5, you print Buzz, and for multiples of 3 and 5 (15), you print FizzBuzz.While this kind of prescreening question might have worked 15 years ago when information wasn’t as accessible as it is now (smartphones, smartwatches. etc.), it seems strange that some companies still ask developers how to write FizzBuzz.The modulus operator returns the remainder of a division operation between two numbers. Ironically, it’s the kind of operator you don’t see many front-end developers using anyway. Even the most junior developers can easily Google before an interview to know that you use the modulus operator.Here is a basic FizzBuzz implementation in JavascriptHere is a basic FizzBuzz implementation in Javascriptfor (let i = 1; i <= 100; i++) {
  if (i % 15 === 0) {
    console.log(""FizzBuzz"");
  } else if (i % 3 === 0) {
    console.log(""Fizz"");
  } else if (i % 5 === 0) {
    console.log(""Buzz"");
  } else {
    console.log(i);
  }
}for (let i = 1; i <= 100; i++) {
  if (i % 15 === 0) {
    console.log(""FizzBuzz"");
  } else if (i % 3 === 0) {
    console.log(""Fizz"");
  } else if (i % 5 === 0) {
    console.log(""Buzz"");
  } else {
    console.log(i);
  }
}In my experience, non-technical skills are more important than technical skills, and few companies focus on non-technical skills. Generally, I am against technical hiring trivia questions. They tell you nothing about the developer’s skill level if they’re an excellent culture fit, if they have a good attitude.In the case of the aged old FizzBuzz, companies may ask for variations on FizzBuzz beyond the standard; 3, 5 and 15 divisions, but most likely not. If your company is using FizzBuzz screening questioning, it might be time to reevaluate if it’s effective.",468
Will ChatGPT Replace Programmers?,"ChatGPT is an AI-powered conversational API by Open AI that generates a lot of hype and fear amongst consumers, professionals and experts alike. Depending on who you talk to, ChatGPT will either revolutionise how we work, or it’s coming for our jobs and will taint academia.You might have seen some news stories, including the impressive feats of ChatGPT passing prestigious law and business exams. And while these stories are understandably concerning, AI is based on learning and is no different from people reading books and learning materials available online to do the same thing.One area of concern for some is ChatGPT and AI replacing programmers. Can ChatGPT write code? Yes. But it’s not as impressive as you may think. Is AI a threat to programmers? Should we be concerned our once coveted jobs are under threat?No, ChatGPT is not designed to replace programmers. Instead, it can assist them in the development process. ChatGPT can generate code snippets and suggest solutions based on patterns it has learned from data, but it lacks the creativity and critical thinking skills that human programmers possess. Ultimately, ChatGPT is a tool, and like any other tool, its effectiveness depends on its use.In my testing of ChatGPT, I found it could generate code for simplistic problem-solving questions quite well. For example, ask ChatGPT to generate a Javascript function that can generate random guid strings, and it will give you code that will probably work. But, as programmers, we don’t work on tiny problems.Here is what I asked ChatGPT:Write me a Javascript function that can generate valid guids without using dependenciesWrite me a Javascript function that can generate valid guids without using dependenciesAnd here is the code it created:function generateGuid() {
  function s4() {
    return Math.floor((1 + Math.random()) * 0x10000)
      .toString(16)
      .substring(1);
  }
  return (
    s4() +
    s4() +
    ""-"" +
    s4() +
    ""-"" +
    s4() +
    ""-"" +
    s4() +
    ""-"" +
    s4() +
    s4() +
    s4()
  );
}function generateGuid() {
  function s4() {
    return Math.floor((1 + Math.random()) * 0x10000)
      .toString(16)
      .substring(1);
  }
  return (
    s4() +
    s4() +
    ""-"" +
    s4() +
    ""-"" +
    s4() +
    ""-"" +
    s4() +
    ""-"" +
    s4() +
    s4() +
    s4()
  );
}In this situation, it generated code that works. You can take this and run it in the console inside developer tools, and it will generate a guid string in the format of xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx — impressive, but you could achieve the same thing using a library of a simple Google search.xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxxWhere ChatGPT seems to fail is more complex coding tasks. Sometimes it will comply and generate code, which will probably not be valid (unless you get lucky). Other times it will outright deny generating the code and occasionally provide bullet-point tips telling you how you might be able to do it.ChatGPT has proved itself in its ability to write boilerplate code. Sometimes you want a primary starting point. Tasks that are simple but time-consuming. If you can offload those to AI tools, you’re saving possibly hours. Like our function above, something an experienced programmer could do, but AI can do faster. More impressively, you can even ask it follow-up questions like, “Now, write me some unit tests for this code,” and it will.People see ChatGPT as this extraordinary genius AI that can do everything. Still, it becomes less impressive when you realise ChatGPT and all forms of current artificial intelligence are just more intelligent search engines. Think about it.ChatGPT and all known public AI are based on publicly scraping data. Isn’t this what search engines do? When you perform a search query, it queries that data and returns a result. Where ChatGPT differs from a search engine is its ability to work with context, allowing you to ask follow-up questions or, in some cases, correct it when it is wrong.And then we come full circle. What is it many programmers do when they get into trouble? They Google. Now, imagine if you had an AI companion; you could provide problematic code or error messages, and because it scraped the documentation, StackOverflow, blogs and other data, it could do the work you would usually do of finding a solution and prevent it to you. That’s where the true power of ChatGPT lies.And that’s the critical thing that programmers need to understand. While it’s second nature to know what to type into Google or ChatGPT, that’s a skill you learn as a programmer. You learn what to type in to get the result you need. That’s the difference between a junior, midweight and senior developer: your ability to condense a problem down into a Google search query.It takes skill to be a programmer. You need to know how to code and what prompts to write into Google, ChatGPT or other tools. A solution is only a sum of its parts. And while ChatGPT can handle some of those parts, many facets of programming are beyond the ability of ChatGPT.While some hysterically decry the end of programmers, lawyers, and other industries, AI will be another tool programmers can use to be better and more efficient at their jobs.If anyone should be concerned, it’s Google. ChatGPT and other AI tools are a real threat to Google and its search engine. It’s the reason Microsoft invested billions into Open AI and why they’re integrating ChatGPT into Bing. If people choose to use ChatGPT instead of Google, that’s one less set of eyeballs to show ads to (although most developers use an ad blocker anyways).",1400
Why I Quit Social Media (Almost) – A Journey to Better Mental Health,"Have you ever stopped to think about the impact social media has on your peace of mind? It was a realisation that came too late towards the end of 2022. The constant arguing, negativity, and drama on platforms like Facebook and LinkedIn took a toll on my mental health and well-being. I realised I needed to step back and break free from the negative echo chamber.You might not realise it, but even seemingly harmless platforms like LinkedIn can have a negative impact on your life. What started as a professional networking platform has fallen prey to the negativity that plagues other social media platforms like Facebook. The constant barrage of conflicts and drama was too much for me to handle.So, I decided to quit most of my social media accounts, except for one – Instagram. I use it to document my life through photos, and I don’t feel compelled to open it all the time as I did with Facebook and LinkedIn. It’s a nice break from the constant negativity that comes with other social media platforms.Quitting social media was about more than just avoiding drama and negativity. It was about regaining control over my time and energy. I was tired of feeling drawn into arguments and drama on Facebook and the negativity surrounding mass lay-offs and recruiter spam on LinkedIn. I was tired of feeling like I was wasting my time and energy on something that wasn’t productive or meaningful.So, I leapt and deleted those accounts. And you know what? It was one of the best decisions I’ve ever made. I have more time to focus on the things that are truly important to me, like my relationships and my passions. I’m less likely to be pulled into arguments and have more control over my time and energy.We have been led to believe that we need LinkedIn to market ourselves, to be in the market. I realised that my GitHub profile speaks for me as a developer much better than any LinkedIn profile ever could. Most of my job opportunities have come from either applying for them or people reaching out to me through my blog.LinkedIn is probably great if you’re in sales, a recruiter, or a company using it to market yourself. Still, it’s primarily low-value content from self-professed thought leaders and celebrities. I have never applied for a job through LinkedIn nor got a job from a recruiter spamming me an opportunity. Usually, recruiters send me jobs I am not qualified for anyway.If you’re feeling overwhelmed by social media and the constant negativity, I encourage you to consider taking a break. It’s okay to prioritise your peace of mind and break free from the negative echo chamber. You might be surprised by how much better you feel.",662
Start-Ups and Companies That Embrace Work From Anywhere Will Be More Likely to Survive the Coming Recession in 2023,"Well, it’s 2023, and many experts are predicting a recession on the horizon. And while no one knows how bad it will be or if many countries will avoid recession, one thing is sure: companies that can weather the storm will be the ones that can adapt quickly and efficiently.Despite this pending threat of economic meltdown, many companies persist with anti-WFH policies, offering ultimatums to employees: return to the office or quit.Isn’t it strange that companies would instead force their employees into an office even though inflation has substantially driven up the cost of living? Interest rate increases from central banks have eaten into the budget as mortgage and rent payments skyrocket. It’s not free to catch public transport or drive to work (especially if you have to pay for parking).Unless you’re willing to offer a company car or subsidised transport, whenever an employee is forced into the office to do a job they can do remotely, they’re losing money.And the ironic thing about all this is the benefits of remote work are not exclusive to employees. Companies reap the rewards of remote work too.When employees work remotely, companies don’t have to pay for expensive office space or other physical infrastructure. This can save a significant amount of money, especially if the recession hits hard and budgets are tight. The electricity cost has increased in some countries by over 100%. Offices are not immune to those costs.Productivity is an important metric to increase to pull an economy out of recession. Despite the unfounded lies that remote workers are slacking off and lazy, studies have shown that remote workers are often more productive than those who work in an office. This can help companies get more done with fewer resources, which is especially important during a recession.Of course, transitioning to 100% remote work is not without its challenges. Companies must invest in technology and training to ensure employees have the tools and resources to be productive. They also need to find ways to foster collaboration and communication among remote teams.But, the crazy thing about all of this is these tools have existed for years before WFH became the norm. This isn’t some new problem that has been sprung on employers. Some companies have been offering remote work for well over a decade. I know people who have been working remotely since the late nineties.We have Asana, Trello, Jira and a bunch of other apps that can make prioritising and streamlining the work pipeline that does not require being in the office. If you prescribe a methodology like Kanban that encourages developers to pick from a pool of work and limits overwhelming your employees, you would be surprised how effective it can be.And then, we have communication tools like Slack which has a plethora of bots and integrations that can make communication effective. More your daily standup meeting (if you have one) into a Slack channel, and people can post their text updates there. We have Zoom or Google for video calls. Notion for wikis, content organisation and sharing. GitHub and Bitbucket for code.Think about what your day is like in the office for a moment. You’re sitting at a desk working at a computer. Well, you can do that at home. Loud kitchen conversations, the sound of a coffee machine hissing, the sound of someone torturing milk for their coffee, the constant interruptions of someone coming over to your desk to ask a question they could have Slacked.It’s so bizarre that some people believe remote workers are slacking off, even though most modern offices are filled with distractions and perks designed to make you leave your desk and use (like beer kegs, coffee, free food, gaming consoles, breakout spaces). I have distractions at home too, but I am less tempted.And then you have the extended lunch breaks. I can’t tell you the number of times I would go to lunch with coworkers when I worked in an office and had a two-hour lunch break. Not on purpose, but because people would lose track of time. Some would get a beer at lunch (or two). And then you have the coffee runs when someone says they’re getting coffee, and some people feel the need to tag along.Remotely, most of my working days are spent eating lunch at my desk. I don’t say that because I want praise or an award. I feel more motivated, relaxed and less distracted at home. By forfeiting a lunch break, I’ll finish up early for the day (which I will inform my coworkers about). This coincides with my son getting home from school, and I go outside and kick a soccer ball with him before dinner.I consider myself a hard worker, but I can tell you, when I worked in an office, I was more distracted and wasted hours a week on non-work related things. Working remotely subconsciously makes you feel you must prove to yourself that you’re contributing. But, once you realise this and work just how you would in an office, you don’t have to prove anything. Let the work speak for itself.Nothing has been better for me for my mental health than remote work. Sadly, many people who experienced remote work for the first time did so during the pandemic. And let me tell you something: remote work during the pandemic, even for someone who loves it like me, was difficult.Our kids were at home, much younger than they are now, because schools closed down. We couldn’t go anywhere. Our lives consisted of work, staying home, and working. Sadly, my wife was disproportionately affected by this too.I wanted to point out that remote work during the pandemic is not indicative of properly implemented remote work. When you force people to stay at home, it’s never a good thing. All workers need is choice. I am not saying companies should shut down their offices completely, but by offering remote work, you can offset some of the costs you would incur.It’s simple math—fewer overheads = more money saved.",1475
How to Insert Gutenberg Blocks Using wp_insert_post,"It’s 2023, and we still have no simple way to insert Gutenberg blocks into WordPress using wp_insert_post. You’re out of luck if you want to pull content from an API and insert it dynamically with ease. There are methods like parse_blocks and render_blocks but they still require a lot of messing around to work with.parse_blocksrender_blocksThe way Gutenberg blocks work is strange. They’re not stored as an array or structured data in the database. They are stored in the HTML as HTML comments.For a YouTube embed, it might look like this:<!-- wp:core-embed/youtube {""url"":""https://www.youtube.com/watch?v=VIDEOID"",""type"":""video"",""providerNameSlug"":""youtube"",""className"":""wp-embed-aspect-16-9 wp-has-aspect-ratio""} -->
<figure class=""wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio""><div class=""wp-block-embed__wrapper"">

</div></figure>
<!-- /wp:core-embed/youtube --><!-- wp:core-embed/youtube {""url"":""https://www.youtube.com/watch?v=VIDEOID"",""type"":""video"",""providerNameSlug"":""youtube"",""className"":""wp-embed-aspect-16-9 wp-has-aspect-ratio""} -->
<figure class=""wp-block-embed-youtube wp-block-embed is-type-video is-provider-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio""><div class=""wp-block-embed__wrapper"">

</div></figure>
<!-- /wp:core-embed/youtube -->You can see how the blocks are handled by viewing the source of your page/post. I can understand why they did it this way for backward compatibility, but it means if you were previously inserting content into your posts, they wouldn’t be Gutenberg blocks by default.Which is why I ended up creating something that works with the Gutenberg HTML comments. It’s a rather simple function that takes the name, attributes and some content.function create_block( $block_name, $attributes = array(), $content = '' ) {
    $attributes_string = json_encode( $attributes );
    $block_content = '<!-- wp:' . $block_name . ' ' . $attributes_string . ' -->' . $content . '<!-- /wp:' . $block_name . ' -->';
    return $block_content;
}function create_block( $block_name, $attributes = array(), $content = '' ) {
    $attributes_string = json_encode( $attributes );
    $block_content = '<!-- wp:' . $block_name . ' ' . $attributes_string . ' -->' . $content . '<!-- /wp:' . $block_name . ' -->';
    return $block_content;
}Going one step further, I also created a wrapper function called create_blocks which allows you to pass in multiple Gutenberg blocks.create_blocksfunction create_blocks( $blocks = array() ) {
    $block_contents = '';
    foreach ( $blocks as $block ) {
        $block_contents .= create_block( $block['name'], $block['attributes'], $block['content'] );
    }
    return $block_contents;
}function create_blocks( $blocks = array() ) {
    $block_contents = '';
    foreach ( $blocks as $block ) {
        $block_contents .= create_block( $block['name'], $block['attributes'], $block['content'] );
    }
    return $block_contents;
}And here is how you use it.$blocks = array();
$blocks[] = array(
    'name' => 'paragraph',
    'attributes' => array( 'align' => 'center' ),
    'content' => '<p>Hello World!</p>',
);
$blocks[] = array(
    'name' => 'paragraph',
    'attributes' => array( 'align' => 'left' ),
    'content' => '<p>This is another paragraph.</p>',
);
$blocks[] = array(
    'name' => 'paragraph',
    'attributes' => array( 'align' => 'right' ),
    'content' => '<p>And this is yet another paragraph.</p>',
);
$post_content = create_blocks( $blocks );
$post_id = wp_insert_post( array(
    'post_title'  => 'My post title',
    'post_content' => $post_content,
    'post_status' => 'publish',
    'post_type'   => 'post',
) );
$blocks = array();
$blocks[] = array(
    'name' => 'paragraph',
    'attributes' => array( 'align' => 'center' ),
    'content' => '<p>Hello World!</p>',
);
$blocks[] = array(
    'name' => 'paragraph',
    'attributes' => array( 'align' => 'left' ),
    'content' => '<p>This is another paragraph.</p>',
);
$blocks[] = array(
    'name' => 'paragraph',
    'attributes' => array( 'align' => 'right' ),
    'content' => '<p>And this is yet another paragraph.</p>',
);
$post_content = create_blocks( $blocks );
$post_id = wp_insert_post( array(
    'post_title'  => 'My post title',
    'post_content' => $post_content,
    'post_status' => 'publish',
    'post_type'   => 'post',
) );
Is this the best solution you could use? Probably not. Did it work for me? Yes. This saved me the hassle of coming up with a clever solution when all I needed was something that got me out of a pickle.",1144
How to Create Files in a GitHub Action and Commit Them to Your Repository,"I recently worked with GitHub Actions, where I generated a JSON file and then needed to add it to the repository. I parsed some Markdown files and then dynamically created a manifest JSON file of them, committing it into the repo every time a push was made.I hit a permissions roadblock I embarrassingly spent over an hour solving, and I hope you don’t make the same mistake.name: Build Blog JSON

on:
  push:
    paths:
      - 'blog-posts/**/*.md'

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Get list of Markdown files
      run: |
        cd blog-posts
        files=($(ls *.md))
        json_array=()
        for file in ""${files[@]}""
        do
          date_string=$(grep -E '^date: ' ""$file"" | cut -d' ' -f2)
          # Use the date command to extract the year, month, and date
          year=$(date -d ""$date_string"" +%Y)
          month=$(date -d ""$date_string"" +%m)
          day=$(date -d ""$date_string"" +%d)
          json_array+=($(echo ""{\""file\"":\""$file\"",\""date\"":\""$date_string\"",\""year\"":\""$year\"",\""month\"":\""$month\"",\""day\"":\""$day\""}""))
        done
        echo ""[$(IFS=,; echo ""${json_array[*]}"" | jq -s -c 'sort_by(.date)')]"" > ../static/blog.json
        
    - name: Remove trailing comma
      run: |
        sed -i '$ s/,$//' static/blog.json
        
    - name: Commit changes
      run: |
        git config --global user.email ""no-reply@github.com""
        git config --global user.name ""GitHub Actions""
        git add static/blog.json
        git commit -m ""Update blog.json""
        git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}name: Build Blog JSON

on:
  push:
    paths:
      - 'blog-posts/**/*.md'

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Get list of Markdown files
      run: |
        cd blog-posts
        files=($(ls *.md))
        json_array=()
        for file in ""${files[@]}""
        do
          date_string=$(grep -E '^date: ' ""$file"" | cut -d' ' -f2)
          # Use the date command to extract the year, month, and date
          year=$(date -d ""$date_string"" +%Y)
          month=$(date -d ""$date_string"" +%m)
          day=$(date -d ""$date_string"" +%d)
          json_array+=($(echo ""{\""file\"":\""$file\"",\""date\"":\""$date_string\"",\""year\"":\""$year\"",\""month\"":\""$month\"",\""day\"":\""$day\""}""))
        done
        echo ""[$(IFS=,; echo ""${json_array[*]}"" | jq -s -c 'sort_by(.date)')]"" > ../static/blog.json
        
    - name: Remove trailing comma
      run: |
        sed -i '$ s/,$//' static/blog.json
        
    - name: Commit changes
      run: |
        git config --global user.email ""no-reply@github.com""
        git config --global user.name ""GitHub Actions""
        git add static/blog.json
        git commit -m ""Update blog.json""
        git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}Now, the important part of my action is the Commit changes action. You need to supply an email and name for the committer. In this instance, I just made up something generic. The first important line is setting the origin URL. We are referencing some variables GitHub creates for us automatically. Notably, GITHUB_TOKEN and repository.Commit changesMany of the blog posts and even the documentation alluded to the fact that this is all you have to do. And maybe I was referencing outdated information, but there is an additional step for permissions you need to do.Under your repository settings, go to “Actions” and then “General”, and scroll right to the bottom until you get to “Workflow permissions”. By default, the GITHUB_TOKEN that is automatically created only has read permission. To commit, you need Read and write permissions.Without this change, your GITHUB_TOKEN will not have permission and you will keep seeing a permission denied message. That’s all you need to do.",1051
What Would Get People Back Into the Office?,"In a hilarious read in the Australian Financial Review, a propaganda piece disguised as an article on remote work and office perks has been published titled WFH raises the bar for offices. WFH raises the bar for officesI meant to post this last year, so this has been sitting in my drafts. But I found it so comical and biased that I felt compelled to write about it.Mirvac, a large Australian property developer with a vested interest in getting people back into the office (because it also owns commercial real estate), has set up a trial space for clients and architectural firms. But, showing just how out of touch they are, a paragraph in the linked story reads.“There are pot plants and whiteboards on wheels. Desks that can roll around. Power points that hang from the ceiling. Telephone boxes that bring back memories of Dr Who. Foam blocks that can be stacked like Lego.”There are pot plants and whiteboards on wheels. Desks that can roll around. Power points that hang from the ceiling. Telephone boxes that bring back memories of Dr Who. Foam blocks that can be stacked like LegoIs this what employees want? Do they want pot plants and whiteboards on wheels? Offices have had these things since the eighties. When choosing an employer, whiteboard on wheels isn’t high on my wishlist of perks.In other articles disguised as independent think-pieces but really being funded by the commercial real estate lobby, allegedly, according to the AFR, some companies are trading up to try and lure people back into the office.trading upI am not convinced.While I have no desire to go back into the office, I did ask myself the question; what would it take for me to go back into the office?In my case, nothing.Unlike other up-and-coming workers who have never set foot into an office, I’ve experienced what it is like on the other side.I’ve worked in open-plan offices as well as more traditional room-based offices. I then worked hybrid for a few years before the pandemic forced the hand of employers, and everyone for a moment in time (those that could) worked remotely.I was fortunate to land a remote job before the pandemic. Not having to sit in peak hour traffic and speed up the date of my next car service was amazing. Being home for packages, getting to help my wife with my two young kids so she can shower or go to the shop by herself, even better.Remote work was an eye-opening experience. For some, it was a negative and positive one for many. For some people, the cat was out of the bag.Despite how well it worked during the pandemic, some companies are swinging the pendulum the other way. It seems strange to me as the threat of recession looms that companies don’t see the savings of office space and equipment as a good thing.",687
PHP Will Not Die,"PHP, the programming language that has been declared dead more times than a cat has lives, is still very much alive and kicking. Despite what some elitist developers may say, PHP is not going anywhere anytime soon.Despite all the naysayers constantly predicting its demise, PHP continues to chug along, powering some of the biggest websites on the internet.Let’s start by taking a look at the statistics. According to W3Techs, PHP is currently used by 78.9% of all websites with a known server-side programming language. That’s a pretty impressive number, considering that PHP has been around since 1995. To put it in perspective, that’s longer than some developers have been alive. And the reason why it has been around for so long is that it just keeps getting better.Then you have the big players. Facebook, one of the most visited websites in the world, uses PHP. Wikipedia, the go-to source for all knowledge, also uses PHP. And let’s not forget about WordPress, the content management system that powers over 40% of all websites on the internet. These websites aren’t small, insignificant players. They’re major players in their respective industries and rely on PHP.I know Facebook (or Meta) has instigated initiatives to increase the performance of PHP, including Hack, but that’s because they face scaling problems that most developers could only dream of.HackThe fact that these companies still use PHP but also contribute patches and updates has kept the language alive, even as trendier languages like Go or Rust have popped up.But it’s not just about the big players. PHP has been around for over two decades and has a huge user base. It’s not just a popular language with big corporations and enterprises. Small businesses, independent developers, and hobbyists all use PHP as well. And it’s not just because they’re used to it or because they don’t know any better. PHP has stood the test of time because it’s a solid, stable language that’s easy to learn and use.And let’s not forget about all the modern features that have been added to PHP in recent years. Things like static types, union types, and other new features have made PHP a much more powerful language. The composer package manager, tooling, and frameworks like Laravel and Symfony have made it even more accessible for developers.Some might argue that PHP is outdated and that there are newer, better languages. But the truth is that PHP is still relevant and is not going anywhere. Sure, there are other languages that are more popular or trendy right now, but PHP has a proven track record, and it’s not going anywhere.So, to all the PHP haters out there, I say this: PHP may not be the newest or the coolest kid on the block, but it’s still here, and it’s still kicking. And it’s not going anywhere anytime soon.",699
How to Create a Blockchain With TypeScript,"If you have been a reader of my blog for a while, you would know that I am an avid cryptocurrency enthusiast. I believe in the tech more so than the financial side. I find blockchains fascinating because, despite their perceived complexity, you can implement a blockchain in any programming language; Javascript included.I thought it would be fun to create a blockchain using TypeScript and then iteratively change the code to offer more flexibility, such as the ability to add metadata into the blocks and query the blocks themselves.Disclaimer: This is all proof of concept and if you’re planning on using this for actual financial purposes, I would consider making the code better. This is just a bit of fun.Disclaimer:Please also note that the crypto-js package is being used here to handle the sha256 hashes.crypto-jsis being used here to handle the sha256 hashes.import * as CryptoJS from ""crypto-js"";

class Block {
  public index: number;
  public timestamp: number;
  public data: string;
  public jsonData: any;
  public previousHash: string;
  public hash: string;

  constructor(index: number, data: string, jsonData: any, previousHash: string) {
    this.index = index;
    this.timestamp = Date.now();
    this.data = data;
    this.jsonData = jsonData;
    this.previousHash = previousHash;
    this.hash = this.calculateHash();
  }

  public calculateHash(): string {
    // Use SHA256 to generate a hash for the current block
    return CryptoJS.SHA256(this.index + this.timestamp + this.data + JSON.stringify(this.jsonData) + this.previousHash).toString();
  }
}

class Blockchain {
  public chain: Block[];

  constructor() {
    this.chain = [this.createGenesisBlock()];
  }

  public createGenesisBlock(): Block {
    // The first block in the blockchain is called the Genesis Block
    return new Block(0, ""Genesis Block"", ""0"");
  }

  public getLatestBlock(): Block {
    // Returns the last block in the chain
    return this.chain[this.chain.length - 1];
  }

  public addBlock(newBlock: Block): void {
    // Assign the hash of the previous block to the new block's previousHash property
    newBlock.previousHash = this.getLatestBlock().hash;
    // Calculate the new block's hash
    newBlock.hash = newBlock.calculateHash();
    // Add the new block to the chain
    this.chain.push(newBlock);
  }

  public isChainValid(): boolean {
    for (let i = 1; i < this.chain.length; i++) {
      const currentBlock = this.chain[i];
      const previousBlock = this.chain[i - 1];

      // Check if the current block's hash is still valid
      if (currentBlock.hash !== currentBlock.calculateHash()) {
        return false;
      }

      // Check if the previousHash of the current block is still valid
      if (currentBlock.previousHash !== previousBlock.hash) {
        return false;
      }
    }

    // If all checks pass, the chain is valid
    return true;
  }
}import * as CryptoJS from ""crypto-js"";

class Block {
  public index: number;
  public timestamp: number;
  public data: string;
  public jsonData: any;
  public previousHash: string;
  public hash: string;

  constructor(index: number, data: string, jsonData: any, previousHash: string) {
    this.index = index;
    this.timestamp = Date.now();
    this.data = data;
    this.jsonData = jsonData;
    this.previousHash = previousHash;
    this.hash = this.calculateHash();
  }

  public calculateHash(): string {
    // Use SHA256 to generate a hash for the current block
    return CryptoJS.SHA256(this.index + this.timestamp + this.data + JSON.stringify(this.jsonData) + this.previousHash).toString();
  }
}

class Blockchain {
  public chain: Block[];

  constructor() {
    this.chain = [this.createGenesisBlock()];
  }

  public createGenesisBlock(): Block {
    // The first block in the blockchain is called the Genesis Block
    return new Block(0, ""Genesis Block"", ""0"");
  }

  public getLatestBlock(): Block {
    // Returns the last block in the chain
    return this.chain[this.chain.length - 1];
  }

  public addBlock(newBlock: Block): void {
    // Assign the hash of the previous block to the new block's previousHash property
    newBlock.previousHash = this.getLatestBlock().hash;
    // Calculate the new block's hash
    newBlock.hash = newBlock.calculateHash();
    // Add the new block to the chain
    this.chain.push(newBlock);
  }

  public isChainValid(): boolean {
    for (let i = 1; i < this.chain.length; i++) {
      const currentBlock = this.chain[i];
      const previousBlock = this.chain[i - 1];

      // Check if the current block's hash is still valid
      if (currentBlock.hash !== currentBlock.calculateHash()) {
        return false;
      }

      // Check if the previousHash of the current block is still valid
      if (currentBlock.previousHash !== previousBlock.hash) {
        return false;
      }
    }

    // If all checks pass, the chain is valid
    return true;
  }
}We have a simplistic blockchain where the chain and blocks are separate, using classes to clean things up.And to use our newfound blockchain, here is how we would implement it:// Import crypto-js library
import * as CryptoJS from ""crypto-js"";

// Create a new blockchain
const myBlockchain = new Blockchain();

// Add some blocks to the blockchain
myBlockchain.addBlock(new Block(1, ""This is block 1"", {name: ""John"", age: 30}, myBlockchain.getLatestBlock().hash));
myBlockchain.addBlock(new Block(2, ""This is block 2"", {name: ""Jane"", age: 25}, myBlockchain.getLatestBlock().hash));
myBlockchain.addBlock(new Block(3, ""This is block 3"", {name: ""Bob"", age: 35}, myBlockchain.getLatestBlock().hash));

// Check if the blockchain is valid
console.log(myBlockchain.isChainValid()); // Output: true
// Import crypto-js library
import * as CryptoJS from ""crypto-js"";

// Create a new blockchain
const myBlockchain = new Blockchain();

// Add some blocks to the blockchain
myBlockchain.addBlock(new Block(1, ""This is block 1"", {name: ""John"", age: 30}, myBlockchain.getLatestBlock().hash));
myBlockchain.addBlock(new Block(2, ""This is block 2"", {name: ""Jane"", age: 25}, myBlockchain.getLatestBlock().hash));
myBlockchain.addBlock(new Block(3, ""This is block 3"", {name: ""Bob"", age: 35}, myBlockchain.getLatestBlock().hash));

// Check if the blockchain is valid
console.log(myBlockchain.isChainValid()); // Output: true
We have a functional blockchain now, but let’s make one more change to make it useful—the ability to get blocks by ID or metadata properties in the block. Inside the Blockchain class we create a method called getBlock which can query for blocks in our chain.BlockchaingetBlockimport * as CryptoJS from ""crypto-js"";

class Block {
    public index: number;
    public timestamp: number;
    public data: string;
    public jsonData: any;
    public previousHash: string;
    public hash: string;

    constructor(index: number, data: string, jsonData: any, previousHash: string) {
        this.index = index;
        this.timestamp = Date.now();
        this.data = data;
        this.jsonData = jsonData;
        this.previousHash = previousHash;
        this.hash = this.calculateHash();
    }

    public calculateHash(): string {
        // Use SHA256 to generate a hash for the current block
        return CryptoJS.SHA256(this.index + this.timestamp + this.data + JSON.stringify(this.jsonData) + this.previousHash).toString();
    }
}

class Blockchain {
    public chain: Block[];

    constructor() {
        this.chain = [this.createGenesisBlock()];
    }

    public createGenesisBlock(): Block {
        // The first block in the blockchain is called the Genesis Block
        return new Block(0, ""Genesis Block"", ""0"");
    }

    public getLatestBlock(): Block {
        // Returns the last block in the chain
        return this.chain[this.chain.length - 1];
    }

    public addBlock(newBlock: Block): void {
        // Assign the hash of the previous block to the new block's previousHash property
        newBlock.previousHash = this.getLatestBlock().hash;
        // Calculate the new block's hash
        newBlock.hash = newBlock.calculateHash();
        // Add the new block to the chain
        this.chain.push(newBlock);
    }

    public getBlock(searchTerm: string | number, by: 'id' | 'metadata' = 'id'): Block | undefined {
      if (by === 'id') {
        const block = this.chain.find((b) => b.index === searchTerm);
        return block;
      } else if (by === 'metadata') {
        // define a function to check if the jsonData of a block contains the searchTerm
        const checkMetadata = (jsonData: any) => {
          for (let key in jsonData) {
            if (jsonData[key] === searchTerm) {
              return true;
            }
          }
          return false;
        }
        
        // check if jsonData property exists before searching by metadata values
        const block = this.chain.find((b) => b.jsonData && checkMetadata(b.jsonData));
        
        return block;
      }
    }

    public isChainValid(): boolean {
        for (let i = 1; i < this.chain.length; i++) {
            const currentBlock = this.chain[i];
            const previousBlock = this.chain[i - 1];

            // Check if the current block's hash is still valid
            if (currentBlock.hash !== currentBlock.calculateHash()) {
                return false;
            }

            // Check if the previousHash of the current block is still valid
            if (currentBlock.previousHash !== previousBlock.hash) {
                return false;
            }
        }

        // If all checks pass, the chain is valid
        return true;
    }
}import * as CryptoJS from ""crypto-js"";

class Block {
    public index: number;
    public timestamp: number;
    public data: string;
    public jsonData: any;
    public previousHash: string;
    public hash: string;

    constructor(index: number, data: string, jsonData: any, previousHash: string) {
        this.index = index;
        this.timestamp = Date.now();
        this.data = data;
        this.jsonData = jsonData;
        this.previousHash = previousHash;
        this.hash = this.calculateHash();
    }

    public calculateHash(): string {
        // Use SHA256 to generate a hash for the current block
        return CryptoJS.SHA256(this.index + this.timestamp + this.data + JSON.stringify(this.jsonData) + this.previousHash).toString();
    }
}

class Blockchain {
    public chain: Block[];

    constructor() {
        this.chain = [this.createGenesisBlock()];
    }

    public createGenesisBlock(): Block {
        // The first block in the blockchain is called the Genesis Block
        return new Block(0, ""Genesis Block"", ""0"");
    }

    public getLatestBlock(): Block {
        // Returns the last block in the chain
        return this.chain[this.chain.length - 1];
    }

    public addBlock(newBlock: Block): void {
        // Assign the hash of the previous block to the new block's previousHash property
        newBlock.previousHash = this.getLatestBlock().hash;
        // Calculate the new block's hash
        newBlock.hash = newBlock.calculateHash();
        // Add the new block to the chain
        this.chain.push(newBlock);
    }

    public getBlock(searchTerm: string | number, by: 'id' | 'metadata' = 'id'): Block | undefined {
      if (by === 'id') {
        const block = this.chain.find((b) => b.index === searchTerm);
        return block;
      } else if (by === 'metadata') {
        // define a function to check if the jsonData of a block contains the searchTerm
        const checkMetadata = (jsonData: any) => {
          for (let key in jsonData) {
            if (jsonData[key] === searchTerm) {
              return true;
            }
          }
          return false;
        }
        
        // check if jsonData property exists before searching by metadata values
        const block = this.chain.find((b) => b.jsonData && checkMetadata(b.jsonData));
        
        return block;
      }
    }

    public isChainValid(): boolean {
        for (let i = 1; i < this.chain.length; i++) {
            const currentBlock = this.chain[i];
            const previousBlock = this.chain[i - 1];

            // Check if the current block's hash is still valid
            if (currentBlock.hash !== currentBlock.calculateHash()) {
                return false;
            }

            // Check if the previousHash of the current block is still valid
            if (currentBlock.previousHash !== previousBlock.hash) {
                return false;
            }
        }

        // If all checks pass, the chain is valid
        return true;
    }
}This method takes two parameters searchTerm and by, where searchTerm is the value you want to search for and by is either ‘id’ or ‘metadata’ to define what you want to search by. By default, it is set to ‘id’, so if you call the method without passing any second parameter, it will search by id.searchTermbyIf by is ‘id’, it uses the Array.prototype.find method to search the chain array for a block with a matching index property. If it finds a match, it returns the block. Otherwise, it returns undefined.byArray.prototype.findchainindexundefinedIf by is ‘metadata’, it uses the Array.prototype.find method to search the chain array for a block with metadata that matches the searchTerm . It uses a helper function checkMetadata that iterates over the jsonData of the block and checks if any of the values match the searchTerm. If it finds a match, it returns the block. Otherwise, it returns undefined.byArray.prototype.findchainundefinedYou can then use this method to search for blocks by ID or metadata values like this:console.log(myBlockchain.getBlock(1)); 
console.log(myBlockchain.getBlock(""John"", 'metadata'));console.log(myBlockchain.getBlock(1)); 
console.log(myBlockchain.getBlock(""John"", 'metadata'));ConclusionAs you can see, a blockchain is just a collection of objects with hashes. While this is a rather simplistic implementation of a blockchain, it shows that it’s not as complicated as you would think.",3514
Parsing Metadata Inside of Markdown Using JavaScript Without Any Dependencies,"In a recent project, I worked with Markdown files that contained metadata at the top for blog posts. I needed to parse the Markdown with JavaScript and make a set of key/value pairs.As you can see, the metadata is encapsulated in – – – with the metadata contained within.// Function to parse metadata from a markdown file
const parseMarkdownMetadata = markdown => {
  // Regular expression to match metadata at the beginning of the file
  const metadataRegex = /^---([\s\S]*?)---/;
  const metadataMatch = markdown.match(metadataRegex);

  // If there is no metadata, return an empty object
  if (!metadataMatch) {
    return {};
  }

  // Split the metadata into lines
  const metadataLines = metadataMatch[1].split(""\n"");

  // Use reduce to accumulate the metadata as an object
  const metadata = metadataLines.reduce((acc, line) => {
    // Split the line into key-value pairs
    const [key, value] = line.split("":"").map(part => part.trim());
    // If the line is not empty add the key-value pair to the metadata object
    if(key) acc[key] = value;
    return acc;
  }, {});

  // Return the metadata object
  return metadata;
};// Function to parse metadata from a markdown file
const parseMarkdownMetadata = markdown => {
  // Regular expression to match metadata at the beginning of the file
  const metadataRegex = /^---([\s\S]*?)---/;
  const metadataMatch = markdown.match(metadataRegex);

  // If there is no metadata, return an empty object
  if (!metadataMatch) {
    return {};
  }

  // Split the metadata into lines
  const metadataLines = metadataMatch[1].split(""\n"");

  // Use reduce to accumulate the metadata as an object
  const metadata = metadataLines.reduce((acc, line) => {
    // Split the line into key-value pairs
    const [key, value] = line.split("":"").map(part => part.trim());
    // If the line is not empty add the key-value pair to the metadata object
    if(key) acc[key] = value;
    return acc;
  }, {});

  // Return the metadata object
  return metadata;
};You can call this function and pass in the markdown file content. It will return an object with the metadata.const markdown = `---
title: My Blog Post
author: John Doe
date: 2021-01-01
---
# My Blog Post
This is the content of my blog post.`;
var metadata = parseMarkdownMetadata(markdown);
console.log(metadata);
// Output: { title: ""My Blog Post"", author: ""John Doe"", date: ""2021-01-01"" }const markdown = `---
title: My Blog Post
author: John Doe
date: 2021-01-01
---
# My Blog Post
This is the content of my blog post.`;
var metadata = parseMarkdownMetadata(markdown);
console.log(metadata);
// Output: { title: ""My Blog Post"", author: ""John Doe"", date: ""2021-01-01"" }The great thing about this solution is it requires no additional libraries. It’s all plain Javascript.",694
Programmatically Update the Status of a Post Using Gutenberg JS,"In a WordPress project I am building, I needed a way to programmatically update the post status of a post on the edit screen, specifically in the Gutenberg editor.Before arriving at the solution below, I first struggled to figure this out. I was trying to call the updatePost method with a post object and modified status, then calling savePost but what kept happening was the post would revert to draft status.It turns out that to update a post status (and other post-specific values), you need to call editPost first followed by savePost right after saving the changes.wp.data.dispatch('core/editor').editPost({
    status: 'publish'
});

wp.data.dispatch('core/editor').savePost();wp.data.dispatch('core/editor').editPost({
    status: 'publish'
});

wp.data.dispatch('core/editor').savePost();It seems so simple in hindsight, but it took me a while to work this out. If you are reading this, chances are you got stuck on this simple task too.",237
How to Hide Meta Boxes in WordPress Gutenberg,"When WordPress introduced the Gutenberg editor, it was a mess, to say the least. Everything was turned upside for developers and things that worked in previous versions were completely broken when Gutenberg was released.One of the things that were broken in WordPress was the ability to hide a meta box on the editor screen. Instead of setting meta_box_cb to false and expecting the box the hidden, nothing happens.meta_box_cbWell, there is a way you can hide meta boxes by using the following snippet of code inside of functions.phpfunctions.phpadd_filter('rest_prepare_taxonomy', function ($response, $taxonomy, $request) {
    $context = !empty($request['context']) ? $request['context'] : 'view';

    // Context is edit in the editor
    if ($context === 'edit' && $taxonomy->meta_box_cb === false) {

        $data_response = $response->get_data();

        $data_response['visibility']['show_ui'] = false;

        $response->set_data($data_response);
    }

    return $response;
}, 10, 3);add_filter('rest_prepare_taxonomy', function ($response, $taxonomy, $request) {
    $context = !empty($request['context']) ? $request['context'] : 'view';

    // Context is edit in the editor
    if ($context === 'edit' && $taxonomy->meta_box_cb === false) {

        $data_response = $response->get_data();

        $data_response['visibility']['show_ui'] = false;

        $response->set_data($data_response);
    }

    return $response;
}, 10, 3);By leveraging the rest_prepare_taxonomy filter, we can modify the visibility of a meta box.rest_prepare_taxonomyOn your register_taxonomy call you can supply ""meta_box_cb"" =&gt; false, and using the above code, the box will be hidden.register_taxonomy""meta_box_cb"" =&gt; false,",432
How to programmatically populate a TinyMCE Advanced Custom Fields field in WordPress,"The Advanced Custom Fields plugin for WordPress is invaluable. It has filled a gap in WordPress for the better part of a decade and is one of the first plugins that I install in a new WordPress installation (I have a lifetime developer licence).The ACF plugin provides a Javascript API available using acf it has numerous methods for interacting with ACF fields in your WordPress installation. You can programmatically get and set field values, observe for changes and react accordingly.acfHowever, I encountered a use case recently where I wanted to programmatically update a WYSIWYG editor field. Using the standard acf.set method does not visibility update this value. To do this, we need to interact with the TinyMCE editor instance call the TinyMCE editor method setContent.acf.setsetContentfunction updateAcfTinymce(fieldId, value) {
    const f = acf.getField(fieldId);
    const tinyID = f.$el.find(""textarea"").attr(""id"");
    const tinyInstance = tinyMCE.editors[tinyID];
    
    tinyInstance.setContent(value);
}function updateAcfTinymce(fieldId, value) {
    const f = acf.getField(fieldId);
    const tinyID = f.$el.find(""textarea"").attr(""id"");
    const tinyInstance = tinyMCE.editors[tinyID];
    
    tinyInstance.setContent(value);
}I’ve created a simple function you can use which takes the field key (the name of the field or the field key) and the second the value you want to set.",350
Programmatically Clicking the “Convert to Blocks” Button in WordPress Gutenberg on the Edit Screen,"In the beginning, I was one of the loud developers protesting WordPress Gutenberg. But, now that Gutenberg has been around for a while, I’ve grown to like it. I can’t tell if it’s because of Stockholm syndrome or if it has actually improved.Anyway.One of the looming problems users of WordPress will still face is the fact if you programmatically insert content into WordPress using wp_insert_post or however else you insert content (say from a CSV or API) you’ll find that WordPress will put it into a Classic Editor block. You will then be prompted to “Convert to blocks” in the editor.wp_insert_postNow, clicking this button isn’t difficult. You focus on the content and click it. But, I needed a way to automate this when the WordPress post edit screen loads. It turns out it’s a little more involved than firing a click event on the convert to blocks button.(function () {
    const block = wp.data.select('core/block-editor' ).getBlocks()[0];

    if (block) {
        wp.data.dispatch( 'core/editor' ).replaceBlocks(block.clientId, wp.blocks.rawHandler({ HTML: wp.blocks.getBlockContent( block ) }));
    }
})();
(function () {
    const block = wp.data.select('core/block-editor' ).getBlocks()[0];

    if (block) {
        wp.data.dispatch( 'core/editor' ).replaceBlocks(block.clientId, wp.blocks.rawHandler({ HTML: wp.blocks.getBlockContent( block ) }));
    }
})();
The code assumes the first block in our editor is a classic block (which is the case for non-converted Gutenberg posts). We get the block using the WordPress JS API. If the block exists we can call the replaceBlocks method and pass in the block ID, then replace it using the rawHandler method to generate our blocks.replaceBlocksrawHandlerThrow this code into a Javascript file and use admin_enqueue_scripts to load it on your edit screen. Here is how I do it inside of my functions.php file.admin_enqueue_scriptsadd_action( 'admin_enqueue_scripts', function() {
  global $pagenow;

  if( 'post.php' == $pagenow ) {
    wp_enqueue_script('block-convert-js', get_stylesheet_directory_uri() . '/convert-to-blocks.js', array(), filemtime( get_stylesheet_directory() . '/convert-to-blocks.js'));
  }
} );add_action( 'admin_enqueue_scripts', function() {
  global $pagenow;

  if( 'post.php' == $pagenow ) {
    wp_enqueue_script('block-convert-js', get_stylesheet_directory_uri() . '/convert-to-blocks.js', array(), filemtime( get_stylesheet_directory() . '/convert-to-blocks.js'));
  }
} );",616
Waiting for an Element to Exist With JavaScript,"There are many different ways to solve this use case. You want to wait for an element to exist on the page and run a callback function when it does. It seems simple enough, but what’s the easiest way?By using an async function and a while loop, we can wrap setTimeout in a promise, then await it, and then resolve the promise when the timeout completes. As you can see, it will continue the loop until we reach the timeout passed into the function (or the element is found).async function waitForElement(selector, timeout = 15000) {
  const start = Date.now();

  while (Date.now() - start < timeout) {
    const el = document.querySelector(selector);
    if (el) {
      return el;
    }
    await new Promise(resolve => setTimeout(resolve, 1000));
  }

  return null;
}async function waitForElement(selector, timeout = 15000) {
  const start = Date.now();

  while (Date.now() - start < timeout) {
    const el = document.querySelector(selector);
    if (el) {
      return el;
    }
    await new Promise(resolve => setTimeout(resolve, 1000));
  }

  return null;
}There are lots of other ways you can achieve this concept, from using setInterval to using a MutationObserver, but this approach is cleaner and easier to understand.setIntervalMutationObserver",315
Stop Forcing People Back Into the Office,"Remote work (also known as WFH) is a hot topic in 2022. As pandemic-era mandates and restrictions come to an end, there has been a new battle forming.In the left corner, we have companies that want their employees to come back into the office, and in the right-hand corner, we have employees who have been allowed to work remotely for the last two years being asked to come back into the office.Futuristic visionary Elon Musk isn’t a fan of remote work and claims that remote workers are just pretending to work. A few months ago, Musk famously sent an email telling people they must be in the office or get fired, even going as far as receiving absenteeism reports as staff are surveilled. And as a result, Tesla is struggling with its mandate.remote workers are just pretending to workmust be in the office or get firedmandateWhile Musk is one of the biggest opponents of remote work, there are many others who are of the same opinion as Musk.Ironically, all of these companies fighting remote work are doing so during very low unemployment levels. In Australia, the current unemployment rate is 3.5% and companies are struggling to find skilled candidates. In the USA, coincidentally, the unemployment rate is also 3.5%. It’s not historically low, but it is quite low.You don’t get to complain about a skills shortage if you’re also forcing people to be back in the office. The remote work experiment companies were unwillingly forced to participate in during the pandemic showed that it works and many enjoyed it.Some of us either worked remotely before the pandemic (like myself) and those who got the taste for it want to continue working remotely. If I did the math on the money I’ve saved in fuel, car maintenance costs, work coffee runs and eating out, it would be a lot of money I’ve saved staying at home.We need to let go of some misconceptions and ill-informed notions about remote work:Remote workers are not lazyIt’s not difficult to manage a full or part remote teamJust because someone isn’t in the office doesn’t mean you can’t measure their outputRemote work won’t open the floodgates for companies to replace their employees with cheaper outsourced workersBeing in the office doesn’t mean employees won’t waste company time or slack offRemote workers are not lazyIt’s not difficult to manage a full or part remote teamJust because someone isn’t in the office doesn’t mean you can’t measure their outputRemote work won’t open the floodgates for companies to replace their employees with cheaper outsourced workersBeing in the office doesn’t mean employees won’t waste company time or slack offWe have had the tools to measure output and efficiency on projects for years now. Tools like Jira, Trello and others can provide you with visibility into work. A company that trusts its tools and sets them up correctly has clarity around work.The only people that stand to gain from forcing people back into the office are commercial landlords that lease office space. The next time you read an anti-remote work article, look deeper into the author because you’ll most likely discover a bias.While I don’t think everyone should be forced to work remotely, I do believe that companies need to offer a choice. Let people come into the office if they want and if they want to stay home, let them. It shouldn’t be one way or the other.As for me, why would I want to commute into the office just to Slack the person in the same office? And if you think that’s an exaggeration, ask anyone who works in an office that uses Slack. We’ve been remote for years, just because we’re in the office doesn’t mean we talk to one another.",908
Passing Environment Variables Into Your Code With Webpack,"In some use cases, it can be beneficial to pass environment variables into your code. In my case, at build time I pass an environment variable to Webpack in the form of --env production and so forth. I wanted to get this in my code so I could load different configuration files depending on the built environment.--env productionI have three environments I build for:ProductionStagingDevelopmentProductionStagingDevelopmentWhile there are code solutions you can implement such as checking the URL, I wanted something handled in the build itself.In my Npm scripts in package.json I have the following:package.json""scripts"": {
  ""build"": ""rimraf dist && webpack --env production"",
  ""build:dev"": ""rimraf dist && webpack --env development"",
  ""build:stage"": ""rimraf dist && webpack --env staging"",
}""scripts"": {
  ""build"": ""rimraf dist && webpack --env production"",
  ""build:dev"": ""rimraf dist && webpack --env development"",
  ""build:stage"": ""rimraf dist && webpack --env staging"",
}Using the DefinePlugin which comes with Webpack, we can then add this to our code at build time like this. I’ve omitted all of the other stuff you would have in your Webpack configuration file so we can focus on the parts that matter.DefinePluginconst { DefinePlugin } = require('webpack');

module.exports = function(env, { analyze }) {
  return {
    plugins: [
      new DefinePlugin({
        'ENV': JSON.stringify(env),
      }),
    ]
  }
}const { DefinePlugin } = require('webpack');

module.exports = function(env, { analyze }) {
  return {
    plugins: [
      new DefinePlugin({
        'ENV': JSON.stringify(env),
      }),
    ]
  }
}Now, inside your code ENV is a global variable you can access. I can then perform checks like this:ENVif (ENV?.development) {
}if (ENV?.development) {
}If you use TypeScript as I do, then you will want to add a declaration for our magic variable.declare const ENV: { development?: boolean; production?: boolean; staging?: boolean; };declare const ENV: { development?: boolean; production?: boolean; staging?: boolean; };Now you can pass in values to your code from your Webpack configuration. It’s great for build environment variables or you can even pass through the configuration itself from the build step. I just choose to use if checks and do that myself.",572
Rate Rise Calculator,"As a homeowner with a mortgage, interest rate rises have become an area of interest for my wife and me. As the Reserve Bank of Australia (and the rest of the world) sees continued rate hikes, it’s important to know how much extra money you’ll need to cough up.I wanted a simple tool that wasn’t trying to get me to provide my contact details and sell my information to banks. A simple calculator that gives me an approximation of what my monthly repayments will be with rate increases.So, I built something. Rate Rise Calculator will ask you for your loan amount, current rate and new rate. It will provide you with an approximation of what your new repayment could be. It works for both rate increases and rate decreases.Rate Rise CalculatorThere is no API, just an Aurelia 2 front-end and some basic styling.",203
Fixing WiFi issues with the Neural DSP Quad Cortex,"For some Quad Cortex users, you will encounter an issue with the WiFi not working. While your first thought might be yoy have a defective unit, the issue might be more simple than you think.The Quad Cortex only operates on the 2.4GHz band of WiFi. This band offers slower speeds than 5GHz WiFi, but is more reliable and can penetrate obstacles better.Chances are your WiFi isn’t setup to work with the 2.4GHz band and you could be transmitting on 5 instead.You can experience this problem if you’re connecting to a personal hotspot on your phone. Many smartphones use 5GHz by default and if you do not enable compatibility mode, it won’t transmit on 2.4GHz.If you’re not using a hotspot, then it’s possible your router/modem is setup to support multiple bands and is hopping between bands. This was the case for my Google Nest router and wireless points, my QC wasn’t working with my WiFi properly due to the lack of 2.4GHz configuration.Still, despite the fixes, you will find that the WiFi on the Quad Cortex is still a weak point. I am not sure if it is the casing, weak antenna inside of the device or software related, but even with full reception at 2.4GHz, it can still be quite slow.Hopefully, these issues are resolved with the Quad Cortex desktop editor and we can update it and interact with the cloud through it.",331
How to Extend the Window Object in TypeScript,"In many cases when you’re working with TypeScript, there are type definitions available for almost every package out there.However, in some circumstances, you might find yourself working with a third party that adds a property to the window object. Think a script tag like how Google Analytics works by adding in the ga property to the window.gaIt doesn’t matter if you are using Angular, React, Vue, Svelte, Aurelia, or any other framework or library, this approach works for all TypeScript projects.Presumably, all of your source code is in a folder called src (it’s the most common place). You can name this file anything, but create a new file called index.d.ts and add in the following.srcindex.d.tsexport {};

declare global {
  interface Window {
    globalProp: any;
  }
}export {};

declare global {
  interface Window {
    globalProp: any;
  }
}This will add type support for window.globalProp and stop TypeScript complaining about this property not existing. In the above example, we use “any"" for situations where you just want it to work so, you can compile your code, but it’s better to properly type your defined properties where possible.window.globalPropany""It’s rare that I need to add new properties to the window object, so this post is more a reference for myself and anyone else who can’t remember the right way to do this.",337
Build Aurelia 2 Plugins and Bundling With Only TypeScript,"The Aurelia 2 makes command ships with the option to scaffold applications and plugins. However, the plugin scaffold uses Webpack and a bunch of dependencies for building plugins. There are reasons that the plugin skeleton uses Webpack.makesFirstly, HTML imports need to be inlined in bundled code or you’ll encounter issues with HTML files not being supported by Aurelia conventions when you consume them in your plugins. The same thing applies to CSS styles, you’ll want to inline those as strings too.I just wanted to build a plugin using TypeScript to compile the code and use a Node package to copy any static assets over.Change your importsFor my HTML and CSS styles, I simply rename them to ts files. For example, if my component had a stylesheet called my-component.css, then I would rename it to be my-component.css.ts and inside I would use a default export with template literals.tsmy-component.cssmy-component.css.tsexport default `.my-class { color: red; }`;export default `.my-class { color: red; }`;The same thing for HTML. If I had my-component.html then I would rename it to be my-component.html.tsmy-component.htmlmy-component.html.tsexport default `<button disabled.bind=""disabled"" class=""button \${color} \${size}"" title.bind=""title"" type.bind=""type"" part=""button \${color} \${size}"" click.trigger=""innerCallback()""><slot></slot></button>`;export default `<button disabled.bind=""disabled"" class=""button \${color} \${size}"" title.bind=""title"" type.bind=""type"" part=""button \${color} \${size}"" click.trigger=""innerCallback()""><slot></slot></button>`;This is a real example from my Cardigan UI library.Cardigan UI libraryThe beautiful thing is inside of your components if you were previously importing your CSS and HTML files, the imports look the same.import styles from './cardigan-button.css';
import template from './cardigan-button.html';import styles from './cardigan-button.css';
import template from './cardigan-button.html';It appears we are importing a CSS and HTML file, but they’re TypeScript files with default exports. When your plugin is consumed inside of an Aurelia 2 application, the HTML and styles will also work.There is some discussion about Aurelia supporting conventions in plugins so you don’t have to inline HTML and CSS, but for now, this is the most reliable way to do it if you want to avoid using a bundler like Webpack to build your plugins.If you want to see the above put into practice, my Aurelia 2 Plugins repository uses this approach and showcases how to use TypeScript for compiling an Aurelia 2 plugin.Aurelia 2 Plugins repository",647
Moving All My Aurelia Plugins Into a Monorepo,"Over the last few years, I have created a few plugins for Aurelia, mostly for Aurelia 1. However, with Aurelia 2 on the horizon (possibly released if you’re reading this in the future), I have decided to clean house and adopt a new strategy.My approach has always been to put my plugins into separate repositories. In my mind, separating issues/pull requests and code made it easier. In reality, having a lot of repositories is a maintenance nightmare.I initially adopted the singular repository approach as I’ve begun porting my plugins to Aurelia 2 and some community plugins. However, it became apparent it’s the wrong way to maintain a fleet of plugins.Part of the problem is I have been building these plugins during the alpha of Aurelia 2. This means that every so often, there is a new alpha release, I have to go and update dependencies in 20,000 packages (not literally, but you get the idea), and it takes a lot of time.Enter monoreposThe concept of a monorepo (which I am sure you’re already aware of) is to have one codebase comprised of sub-packages/sub-modules. Aurelia itself is a good example of a mono repository. a good example of a You’ll notice that all of Aurelia’s modules exist in a packages directory. You can manage shared dependencies at the root level, and those packages do not have any node_module directories.The beauty of this approach is that it is easier to update package versions and means less space used on my machine. Instead of multiple projects with the same dependencies in different node_modules directories, I now have one directory of dependencies. I’ve saved gigabytes in drive space.The repositoryLong story short, you can find all of my Aurelia 2 plugins going forward in this repository here. They’re still published independently, but issues, pull requests and maintenance just got about 100x easier for me to manage.this repository here",472
The Weird Saga of the Optus Cybersecurity Breach,"In case you weren’t aware, recently one of Australia’s large telecommunication companies Optus suffered one of the largest cybersecurity breaches to date. While the extent of the data breach has yet to be revealed, Optus has 9.7 million subscribers and the data taken allegedly could go back to 2017 and involves former customers.Optus suffered one of the largest cybersecurity breaches to dateAllegedly, the data could contain data on over 11 million current and former Optus customers. In any other part of the world, this would have resulted in very large fines. In Australia, no such legislation exists to hold any company accountable for a breach like this.The data that was taken wasn’t just some names and emails. Some of the data also included passports, driver’s licences and other pieces of identification that could be used to steal someone else’s identity or commit fraud.It’s an absolute mess. The only thing that rivals the unbelievable scope of the attack is how Optus handled it, or more specifically, did not handle it. Customers had to find out from news stories about the attack. Optus didn’t disclose to customers that their identities might have been compromised.To date, Optus has not revealed how the attacker got access to the data. But, a journalist contacted the attacker and they confirmed they leveraged a publicly available API endpoint that was unauthenticated. They didn’t need to log in.While this is being called an attack, it’s incompetence on Optus’ part. The hacker didn’t break into anything, they didn’t even have to perform any complex vulnerabilities or phish an employee to get access.
The Optus hacker says they accessed an unauthenticated API endpoint. This means they didn't have to login. The person says: ""No authenticate needed. That is bad access control. All open to internet for any one to use."" #infosec #auspol pic.twitter.com/l89O8w1oCO— Jeremy Kirk (@Jeremy_Kirk) September 24, 2022
The Optus hacker says they accessed an unauthenticated API endpoint. This means they didn't have to login. The person says: ""No authenticate needed. That is bad access control. All open to internet for any one to use."" #infosec #auspol pic.twitter.com/l89O8w1oCO— Jeremy Kirk (@Jeremy_Kirk) September 24, 2022The Optus hacker says they accessed an unauthenticated API endpoint. This means they didn't have to login. The person says: ""No authenticate needed. That is bad access control. All open to internet for any one to use."" #infosec #auspol pic.twitter.com/l89O8w1oCO#infosec#auspolpic.twitter.com/l89O8w1oCOSeptember 24, 2022This kind of attack is so laughably simple, a high school kid with an interest in computers could have performed the same attack and taken data. Your grandma with a little guidance and limited computer knowledge could have been coached to perform the same “attack”. That’s how basic this appears to have been.How embarrassing. Optus suffered a massive data breach because they left an endpoint open. What makes it even worse, is they clearly had no monitoring in place. Any company with proper monitoring would have noticed what would have been a high number of requests to their api.optus.com.au endpoint, which would have been suspicious. Alarm bells should have been going off when a spike appeared on their graph showing a higher than usual load on their API server.And Optus isn’t a small company, they have millions of customers. For a company that made $2.07 billion in profit for the 2021-2022 year, it’s even more embarrassing. $2.07 billion in profit for the 2021-2022 yearMaybe Optus needs to spend some of that profit on some cybersecurity staff. A search on Seek for Optus cybersecurity jobs yields a single result at the time of writing this and that’s for their Fetch TV service.yields a single resultIn contrast, after Uber got attacked in a large cybersecurity attack, they posted over 80 job listings for numerous cybersecurity roles.over 80 job listingsAnd if your sides weren’t hurting from the laughter already, the parent company that owns Optus, Singtel also owns a cybersecurity company called Trustwave. And for a real belly laugh, all you have to do is read the blurb on that linked page:owns a cybersecurity company called Trustwave“Trustwave, a Singtel company, is a global leading cybersecurity provider with an extensive portfolio of services, and a proven track record of helping businesses securely embrace digital transformation.“Trustwave, a Singtel company, is a global leading cybersecurity provider with an extensive portfolio of services, and a proven track record of helping businesses securely embrace digital transformation.Trustwave, a Singtel company, is a global leading cybersecurity provider with an extensive portfolio of services, and a proven track record of helping businesses securely embrace digital transformation.Anyway, after the breach, the silence from Optus and panic, the hacker went online and made a ransom demand for one million dollars in Monero cryptocurrency. As this all unfolded, people confirmed to be implicated in the attack evidenced by the 10,000 users in the leaked data confirmed their data was accurate. And yet, barely a notification from Optus until days after.made a ransom demand for one million dollarsbarely a notification from Optus until days afterIn a pathetic attempt to stem the PR nightmare currently enveloping Optus, they offered customers a free subscription to an identity protection service. The laughable thing about their shallow offer is you can already monitor your credit file and other facets of your identification for free. Furthermore, only certain customers are being offered 12 months.offered customers a free subscription to an identity protection serviceWhat happens after 12 months? Do hackers stop exploiting data after 12 months? Given the size of the data, if it did get out, it could last crime rings years. Passports and driver’s licences can have notably long expiries.And just when you thought the situation could not get even more convoluted and weird, the hacker after posting the data of 10,000 former and current customers posted an apology and deleted the original thread and data.posted an apology and deleted the original thread and dataThe hacker even went as far as apologising for the inconvenience in their forum post. How do you go from ransoming a large and valuable dataset to deleting it and dropping your ransom demands?If you want to see the thread yourself and the subsequent replies from other users, you can see the thread here on Breach Forums.here on Breach ForumsThe looming threat of the Australian government and Australian crime agencies hunting you down might have something to do with it. While IPs were allegedly traced to Europe it makes me wonder if the hacker is actually Australian.Why would a European hacker be afraid of Australian crime agencies? The threat of deportation is real, but it seems likely to me the hacker is Australian (or in the vicinity) and worries the walls are closing in on them. The post starts off with, “Too many eyes” which indicates they know they might be found eventually.Although, here is a thought. What if Optus did pay the ransom? And the attacker just agreed to keep it quiet and deleted the data? While it appears the attacker is scared, makes you wonder if payment was made and now they’re just holding up their end of the agreement.After releasing a subset of the data (10,000) the sudden U-turn the attacker made seems very suspicious. And, if Optus did indeed make this payment, would they be obligated to report it publicly?Or, perhaps thinking about it even deeper, did someone else buy the data before Optus? Maybe someone offered way above the $1m to buy the data and the attacker sold it. It would be the perfect cover, right?Perhaps the only saving grace of all this is Australia is planning to overhaul its privacy laws. Doing what many other countries have already done to protect its citizens. With these laws, we can only hope the threat of fines and other forms of punishment for serious breaches such as these are bestowed upon companies like Optus who suffer attacks of this scale.Australia is planning to overhaul its privacy laws",2049
Programming on a Curved Monitor,"A few years ago, I ditched the concept of a dual monitor setup as 30″+ displays started to come down in price, and getting a single large monitor instead of multiple smaller ones made more sense.After using my trusty Samsung SJ55W 34″ widescreen for a while, I recently tired of the widescreen monitor. It’s not that it didn’t meet my needs; it’s the fact that I program and do stream and screen recording. Samsung SJ55W 34″ widescreenThe resolution on my beloved Samsung widescreen was 3440×1440 Pixels, which doesn’t lend itself to streaming. You can implement workarounds in Streamlabs and other streaming software, but you either lose some of your display or end up with black bars.This might seem strange, but I opted for a curved widescreen monitor. I didn’t want to spend too much, because I wasn’t sure if I would like a curved display. So, I purchased the Dell 32 S3222HG monitor.Dell 32 S3222HG monitorIt’s smaller than my 34″ widescreen (which I have kept and now use in a vertical orientation), but I find it easier to work on. Not to mention, it fits on my desk better. The curve means I don’t need to have such a deep desk or position it in a way where it’s comfortable to look at.Some of my colleagues and friends rave about curved monitors. I had sat on the fence for a while because the concept felt like a gimmick. It’s not until you use a curved monitor you realise it’s not a gimmick.The display’s curvature when you’re staring at it for hours feels natural and less fatiguing. I wasn’t sure if I imagined this, so I did some research, and the display’s curvature is more natural and easier for your eyes to focus on due to the field of value compared to a flat widescreen.The true test of any monitor for me is programming. After some calibration, I found the monitor wasn’t any worse to work on despite being a smaller resolution. Sure, given it’s only a 1920 x 1080 monitor, it’s not as crisp, but it’s still a great monitor that fits my programming needs and streaming and gaming.Inevitably, I’ll probably look for a higher resolution curved monitor in the future. But, I don’t miss the increased resolution, and I find programming with a curved monitor to feel nicer.",548
I tried TailwindCSS for the first time,"After avoiding Tailwind for such a long time, I finally decided to sit down and see what the hype was all about and use it with Aurelia 2.There are some pros and cons, some complications but it was a surprisingly positive experience.

",59
Aurelia 2 vs Svelte — The battle of two front-end underdogs,"In this video, I compare some basics like reactivity, component creation and events in Aurelia 2 and Svelte. You’ll notice some similarities between the two but a few differences in the approach to bindables and component creation.

",58
It’s 2022. We’ve Suffered Enough: Developers Use Whatever You Want,"I am an avid reader of Medium, and it’s no secret that the quality of Medium articles has gone downhill over the last couple of years. Clickbait articles are intentionally titled and written to garner a response but lack substance. Amongst the shining gems, is a pile of faeces.One recently caught my eye. An article titled. It’s 2022, Please Don’t Just Use “console.log” Anymore.It’s 2022, Please Don’t Just Use “console.log” AnymoreYou are probably already rolling your eyes at the title without even reading the article.And one would assume the article would propose console.log alternatives like debugging and other developer tools we can use to debug code. But, no, it proposes methods like, console.table basically, alternative console methods.console.logconsole.tableconsoleAnd, this is not a hate on Medium author article. There are plenty of those around already. This article’s point is to stop letting people tell you what you are doing is wrong.We see it in development all of the time, especially front-end. You’re told not to use Javascript classes and only to write pure functions. You’re told not to write CSS and use CSS-in-JS solutions. Everyone has an opinion that challenges the way some developers like to work.Proposing alternative solutions to problems is fine, but outright telling them they shouldn’t be doing something is wrong. Does it matter if you use console.log or a debugger? No. Sure, there are better tools and ways of doing things, but we need to move away from telling people to stop.console.logdebuggerIt’s 2022: haven’t we suffered enough? Let developers use whatever they want. Whatever gets the job done is good, in my opinion.",417
A review of the Elgato Facecam,"Before the pandemic, webcams were not really on the mind of most. Many used the inbuilt camera on their laptops or phones for any video-related calls. But, as we were all asked to stay home and virtual meetings became the norm, the pandemic was very kind to the webcam industry.For streamers and content creators, webcams aren’t an afterthought. While the top-tier professionals use expensive DSLR cameras and links, many use a good old-fashioned webcam.Perhaps like some of you reading, I’ve owned my fair share of webcams. Ranging from terrible cheap Logitech webcams to a few of the more expensive ones.In the range that the Elgato Facecam falls into, you have a few other heavy-hitters:Elgato FacecamLogitech Brio (capable of 4K 30fps and 1080p 60fps)Razer Kio Pro (1080p 60fps)Dell UltraSharp 4k (4k 30fps, Full HD 30 and 60fps, plus a few others)Logitech Brio (capable of 4K 30fps and 1080p 60fps)Razer Kio Pro (1080p 60fps)Dell UltraSharp 4k (4k 30fps, Full HD 30 and 60fps, plus a few others)The Logitech Brio has arguably been the king of high-end webcams for a while now, and surprisingly, not many have challenged the Brio very well. That is until Elgato got into the webcam game.Still, despite some top-tier webcams making some nice promises, even the best webcams like the Logitech Brio found a way to disappoint me (maybe my standards are just too high for a webcam). Weird skin smoothening, poor low-light performance, focus issues.I am not going to beat around the bush here. If you’re looking for one of the best webcams you can buy for streaming (Twitch, YouTube, etc.), the Elgato Facecam is the webcam for you. If you’re looking for a webcam to make you look like you’re on a Hollywood movie set (minus the explosions), this is the webcam for you. If you’re looking for a webcam to record video tutorials and do webinars, this is the webcam for you.Elgato FacecamOne of the Facecam’s best features is the nice big lens. It is followed by its impressive sensor, which performs much better than you would expect a webcam to perform in dimly lit conditions. It will not be as good in low light as a camera, but it’ll be the best webcam you’ve ever used. Unlike other cameras, it doesn’t compress anything. So you get full HD uncompressed picture quality.The sensor inside the Elgato Facecam is the Sony STARVIS sensor, which is primarily used in security cameras because of its great low-light performance.That is the important thing to remember here. The Elgato Facecam is not a camera; it’s a webcam. It does one thing, and it does it well.While the Facecam is arguably one of the best webcams around, it still won’t do some of the things you might associate with a camera:No autofocusNo built-in microphoneFixed lens (although it does support digital zoom)It can’t do bokeh (the creamy background blur effect)Image quality is not the same as a camera. If you want a camera-quality streaming camera, you’re better off going with the Sony ZV-E10 (considerably more expensive).No autofocusNo built-in microphoneFixed lens (although it does support digital zoom)It can’t do bokeh (the creamy background blur effect)Image quality is not the same as a camera. If you want a camera-quality streaming camera, you’re better off going with the Sony ZV-E10 (considerably more expensive).Sony ZV-E10The Camera Hub software is possibly some of the best-bundled hardware-software I’ve used, especially for a webcam. One of the first things you might notice is that the camera will overexpose you by default. This is, fortunately, a software issue that you can fix.It’s also worth mentioning this small feature: your settings don’t save to your machine. They are saved onto the Facecam itself. Maybe other webcams do this too, but if you use your Facecam with another machine, your settings come with you. I think that is pretty cool.Here is a photo taken with the exposure settings on auto (the default setting).And here is a photo taken with the auto settings turned off (set to ISO 100).You can notice the difference in the skin tone on the face. While it’s a bit duller of a photo (due to lack of good room lighting), it highlights that the exposure settings are a little too over-the-top in a dimly lit room.Fortunately, I’ve also done a recording with the Facecam. I did a coding video on YouTube using Streamlabs OBS. My webcam is in the right corner.coding video on YouTube

I had taken the Facecam out of the box maybe three hours before I recorded this. So, I was still working out what settings worked best, but I think the result turned out quite well.The best feature of the Elgato FacecamArguably, I believe the best feature of the Elgato Facecam is the ability to change the ISO setting (under the exposure section). I can’t recall ever seeing a webcam that offers ISO adjustment. It allows you to prevent the sensor from introducing noise into your picture and video.My best bit of advice for the Facecam is to turn off automatic exposure, turn the ISO down to 100 and then use a ring light (or other forms of lighting). Because the Facecam doesn’t compress anything, you’ll get the best quality possible this way.If you don’t have access to lighting, turning the ISO up a little bit won’t hurt. I’ve noticed that you don’t start to see noticeable noise until you get into the 300 range. Even then, you have to look for it. Avoid the auto exposure settings, or it’ll wash you out (especially if you have light behind you).ConclusionIs the Elgato Facecam expensive? Yes. Is it the best performing webcam you can buy right now? Undoubtedly yes. Elgato FacecamWhile the Facecam might not be justifiable for everyone, given the cost is well above what you would pay for a cheap Logitech webcam or even the one that comes with your laptop, I think the difference between the Facecam and others is night and day.Suppose you’re a streamer who cannot justify the cost of an entry-level DSLR or vlogger camera (like the Sony offerings). If your streaming career takes off, you’ll be able to afford a proper camera. Or, perhaps, you work remotely and want to stand out from the rest of your peers. In that case, the Elgato Facecam is the best getting-started streamer webcam.Elgato Facecam",1550
Building with Aurelia 2: Episode #1 — Search autocomplete with term highlighting,"Because I love punishing myself and loading my plate with more things, I have embarked on a new Aurelia 2 series called Building with Aurelia 2. During this series, viewers will see how easy and fun it is to build Aurelia 2 user interfaces and applications.Since developers have many choices when it comes to frameworks and libraries, I figured a series where developers are shown with real examples of how easy it is to build with Aurelia 2 would be better than more blog posts telling you how great it is.In episode #1, we take a Dribbble design concept someone created for a text search autocomplete with search term highlighting and turn it into a functional UI.You can watch it below or find it on YouTube here (make sure you enable HD, or the text might not be legible due to my widescreen monitor).here

",203
What Button Covers to Use on Your Quad Cortex?,"The Quad Cortex is an amazing modeller, and since buying one in October 2021, I use it many times a week (the perks of having it on your desk). However, despite how incredible the modeller is, the rotary stomps can be a little small for non-desktop use.If you’re gigging or using it with your feet, the small stomps can be tricky, and you can accidentally press the wrong rotary button.The solution is to use button toppers. Now, there are a lot to choose from out there. But, the best toppers to use with your Quad Cortex are the Mooer ‘Shroom toppers.Mooer ‘Shroom toppersThese toppers are designed for devices like pedals and modellers with LEDs on them (which the Quad Cortex has for each rotary stomp). They are also cheaper than some of the alternatives, which are made of metal.Unlike other toppers, they will not scratch or damage your Quad Cortex rotary stomps. They’re plastic that offers a snug fit. There are no screws to tighten to secure the rotary stomps, just pop them on, and you’re good to go.They might seem a little pricey, but if you own a Quad Cortex, you want to protect your expensive investment, and these toppers are well-cast, smooth and high quality. Get the Mooer ‘Shroom Toppers, and don’t bother with anything else. These work well.Mooer ‘Shroom Toppers",321
Reliably waiting for network responses in Playwright,"Playwright makes what used to be complicated in end-to-testing almost too easily, especially waiting for network requests.However, in the case of single-page applications (SPA) where a click on a link or button maybe fire a request but not a page load, you can encounter a situation where Playwright encounters a race condition between clicking and waiting for a response.The issue is sometimes the waitForResponse in situations where the request might be faster than the click event finishing, it acts as if it never took place and times out. This issue is acknowledged in the Playwright documentation for page.waitForResponse.waitForResponsePlaywright documentation for page.waitForResponseThe Playwright docs give an example of using Promise.all, but the syntax is a little less clean for my tastes.Promise.all// Note that Promise.all prevents a race condition
// between clicking and waiting for the response.
const [response] = await Promise.all([
  // Waits for the next response with the specified url
  page.waitForResponse('https://example.com/resource'),
  // Triggers the response
  page.click('button.triggers-response'),
]);// Note that Promise.all prevents a race condition
// between clicking and waiting for the response.
const [response] = await Promise.all([
  // Waits for the next response with the specified url
  page.waitForResponse('https://example.com/resource'),
  // Triggers the response
  page.click('button.triggers-response'),
]);The proposed solution works, but we can make it cleaner by wrapping it in an asynchronous function instead:export async function clickWait(page: Page, locator: string, url: string) {
  const [response] = await Promise.all([
    page.waitForResponse(url),
    page.click(locator)
  ]);

  return response;
}export async function clickWait(page: Page, locator: string, url: string) {
  const [response] = await Promise.all([
    page.waitForResponse(url),
    page.click(locator)
  ]);

  return response;
}And then, inside of your test cases, use it like this:test('load button is clicked and WordPress posts are loaded', async ({ page }) => {
    await page.goto('http://localhost:8080/');
    await clickWait(page, '#load-posts', 'https://myapp.com/wp-json/wp/v1/post');
 })test('load button is clicked and WordPress posts are loaded', async ({ page }) => {
    await page.goto('http://localhost:8080/');
    await clickWait(page, '#load-posts', 'https://myapp.com/wp-json/wp/v1/post');
 })This approach also works for waitForRequest and any other race condition situation you might encounter in Playwright. Just wrap in a function.waitForRequest",652
How To Test Your Web Applications Using Playwright and Support 2fa Tokens,"At work, I’ve been migrating us over from Cypress to Playwright for end-to-end tests. In that time, we’ve enabled two-factor authentication functionality in our application for security.These TOTP tokens are great for security but provide an additional challenge for testing.While Playwright supports saving state, our application tokens have a short expiry. I needed to log in to our application, enter a one-time password, and test.The logging-in part was easy. We created development environment credentials. But because 2fa is enabled in all environments, we still had the issue of needing a TOTP token.Well, there is a way to use a Node.js package for generating tokens: Totp Generator. All you need is your private seed key to generate token values and pass it to the package to generate a valid TOTP token.Totp GeneratorMy global setup file looks like this:import { chromium, FullConfig } from '@playwright/test';

import totp from 'totp-generator';
const token = totp('JVGEGDFGDBFSDFDHHGVCHGHDASDRWERRTY');

async function globalSetup(config: FullConfig) {
  const { baseURL, storageState } = config.projects[0].use;
  const browser = await chromium.launch();
  const page = await browser.newPage();
  
  await page.goto(baseURL!);

  await page.locator('input[name=""loginEmail""]').fill('someemail@gmail.com');
  await page.locator('input[name=""loginPassword""]').fill('password!');
  await page.locator('text=Sign In').click();

  const twoFactorField = await page.locator('input[placeholder=""Enter the 6-digit Code""]');
  if (twoFactorField.isVisible()) {
    await twoFactorField.fill(token);
    await page.locator('button[name=""loginButton""]').click();
  }
  
  await page.context().storageState({ path: storageState as string });
  await browser.close();
}

export default globalSetup;
import { chromium, FullConfig } from '@playwright/test';

import totp from 'totp-generator';
const token = totp('JVGEGDFGDBFSDFDHHGVCHGHDASDRWERRTY');

async function globalSetup(config: FullConfig) {
  const { baseURL, storageState } = config.projects[0].use;
  const browser = await chromium.launch();
  const page = await browser.newPage();
  
  await page.goto(baseURL!);

  await page.locator('input[name=""loginEmail""]').fill('someemail@gmail.com');
  await page.locator('input[name=""loginPassword""]').fill('password!');
  await page.locator('text=Sign In').click();

  const twoFactorField = await page.locator('input[placeholder=""Enter the 6-digit Code""]');
  if (twoFactorField.isVisible()) {
    await twoFactorField.fill(token);
    await page.locator('button[name=""loginButton""]').click();
  }
  
  await page.context().storageState({ path: storageState as string });
  await browser.close();
}

export default globalSetup;
In this logic, we determine if we require an OTP, and if we do, we generate a token value to log in and perform our test. Most of the logic is the totp import at the top and the function call.totp",733
How to debug the output from within WordPress filters,"WordPress has a great actions and filters system allowing you to create injection points for modifying parts of your code (especially for plugin authors). However, you probably arrived here because you’re trying to echo or print_r something from within a filter and not seeing the output.echoprint_rBecause WordPress operates on a post/redirect approach, it means you’re not seeing the output because the page has been reloaded. You need to kill the script from within your filter callback function to see the output.add_filter('sh_pre_process_body', function($body, $assets_path, $article_file) {
	$load_file = file_get_contents($article_file);
  
  	echo $assets_path;
  
  	die;

	return shand_fix_content_paths($assets_path, $load_file);
}, 10, 3);add_filter('sh_pre_process_body', function($body, $assets_path, $article_file) {
	$load_file = file_get_contents($article_file);
  
  	echo $assets_path;
  
  	die;

	return shand_fix_content_paths($assets_path, $load_file);
}, 10, 3);By using die you will be able to see the output or other equivalents that kill script execution. However, the result will mean your site will break until you remove this script killer.die",294
How to Use Bootstrap JavaScript Components Inside of Shadow DOM Web Components,"Despite doing this front-end thing for over a decade, I still encounter new problems thanks to the ever-evolving web specifications. One of the newer specifications is Web Components.Now, my situation was I wanted to see Bootstrap 5 Javascript components. Because of the closed-wall nature of Shadow DOM means, the global approach Bootstrap takes by default will not work for components.Fortunately, it is possible to use Bootstrap components with Shadow DOM, albeit programmatically.import { Dropdown } from 'bootstrap'; 

const dropdownElement = this.element.shadowRoot.querySelector("".dropdown-toggle-product"");
    
const dropdown = new Dropdown(dropdownElement);import { Dropdown } from 'bootstrap'; 

const dropdownElement = this.element.shadowRoot.querySelector("".dropdown-toggle-product"");
    
const dropdown = new Dropdown(dropdownElement);In my example, I am using Aurelia 2, but the premise is the same. You access the shadowRoot of the element that contains the markup, you want to enable Bootstrap on. It’s a new paradigm, shadowRoot becomes the documentshadowRootshadowRootdocumentYou probably want to ensure your components are in open mode, as I haven’t tested if this approach would even work in closed mode (presumably not).",311
I Joined Truth Social Using a VPN and Editing Some HTML to Bypass the Phone Verification,"After being deplatformed a little while ago, Donald Trump created his Twitter clone, Truth Social. As you can imagine, Truth Social launched to about as much fanfare as a fart in an elevator. Still, my curiosity got the better of me.For whatever reason, the site is currently restricted to Canada and the United States. And, it is worth pointing out that this restriction is Truth Social themselves restricting which countries can access the site. I am not sure what the reason is for restricting access. Seems ironic given it’s meant to be a Twitter-esque site for free speech.Step one: operation geoblockStep one was bypassing the geoblock using a VPN. To the credit of whoever built the site, I had to cycle through a couple of VPN regions in Private Internet Access before I found one that worked. The couple I tried gave me an access denied message, most likely a CDN block.I doubt whoever built the site created their own geo detection and block functionality. You can achieve this using Amazon Cloudfront and other online services that make this kind of task trivial.The registration process is rather convoluted. Once you enter your date of birth and email address, you are sent a verification email. Once you verify, you must go back to the homepage and click “Create an account” again to be presented with the next step.Step two: operation text message bypassOnce you confirm your email address and go to the next step, you’re asked to enter your phone number.The two options for country code are +1 and +44. I live in Australia, which is country code +61. Notably absent from the list.My first thought was to try entering +61, followed by my phone number, into the field, which didn’t work.So, the next thing I tried was editing the HTML of the page using Developer Tools. It’s rather simple, right-click the select input and click “Inspect element.”As you can see, the values are numeric without the plus in front. So, I edited 1 to be 61 to match my country code. I then entered my mobile number and clicked next.Once you edit the value, click off the value (select the other option) and then the one you selected. Otherwise, the change won’t be set and still be +1.The phone number gets posted to this endpoint: https://truthsocial.com/api/v1/pepe/verify_sms/requesthttps://truthsocial.com/api/v1/pepe/verify_sms/requestThe value sent up is the country code with plus followed by your number. In my case (and this is not my real number), it sent up something like this:{
    phone: ""+61432123456""
}{
    phone: ""+61432123456""
}Step three: profit???I was able to register, even though I don’t live in Canada or the US. The title gives things away, but here is my verification text message. Easy peasy.
Whatever restriction they have on country and region appears superficial and easy to bypass. Not many would go to the effort to join Truth Social by editing HTML to bypass an SMS block. Maybe this is intentional?The bigger question this raises is, what else are they not properly validating in their API? If I can get around their geoblock, even the text message part, were other shortcuts taken during the development of this site?Oh, if you’re curious what Truth Social looks like:It looks like someone took a design concept from Dribbble and turned it into a real site. Other than that, it’s a pretty Twitter-esque-looking social media platform. Perhaps most surprising was seeing real media outlets like Washington Examiner using the platform.Bonus round: the old switcherooThen my curiosity went further. What would happen if I logged in and turned off my VPN?Uh oh. The site goes into a death spiral, it seems. I get a tonne of, “The request was a legal request”, and the site gets confused.Going back into Developer Tools paints a funny picture:The site seems to know I am an authenticated user, so it keeps trying to request to load my timeline. But, looking at the payload, the response is the homepage with the denied message because the geoblock is kicking in.",997
Creating Site Meta Search In WordPress Multisite using Site Queries,"WordPress is incredibly powerful, and every so often, a new feature gets added that goes under the radar that can dramatically change how you build sites. In WordPress 5.1, an addition of a new WP_Site_Query class was created. It allows you to query your network of sites but goes beyond just getting IDs and making you loop over them to use with switch_to_blog($id)switch_to_blog($id)Here is another super-specific use case I had with WordPress Multisite recently.I have a network comprised of thousands of sites. Each site has an options page that uses Advanced Custom Fields Pro. The information is things like the site’s name, country, etc.I needed to implement a search that allowed me to query one or more of these meta fields. In my situation, each site belongs to a continent, country, state/region, local government area or city/town. I needed to retrieve a list of these sites based on a search keyword.Here is how I did it.First, I created a custom REST endpoint.add_action( 'rest_api_init', function () {

    register_rest_route( 'utilities/v1', '/sites/search/(?P<search>([a-zA-Z]|%20)+)', array(
        'methods' => 'GET',
        'callback' => 'rest_search_sites',
        'permission_callback' => '__return_true'
    ) );
  
});add_action( 'rest_api_init', function () {

    register_rest_route( 'utilities/v1', '/sites/search/(?P<search>([a-zA-Z]|%20)+)', array(
        'methods' => 'GET',
        'callback' => 'rest_search_sites',
        'permission_callback' => '__return_true'
    ) );
  
});A pretty standard REST API route. Take note of how we create our parameter regex, though. We want to allow letters but also allow spaces as well. If someone types “United States of America,” it can see the whole term.And here is the callback function for our route.function rest_search_sites( WP_REST_Request $request ) {
    $value = $request->get_param( 'search' );

    $site = new WP_Site_Query();

    $sites_found = $site->query([
        'number' => 9999,
        'meta_query' => [
            'relation' => 'OR',
            [
                'key'     => 'site_name',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'country',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'continent',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'state',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'region',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'local_government_area',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'city-town',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
        ]
    ]);

    $result = new WP_REST_Response([]);

    if ( $sites_found ) {
        $sites = [];

        foreach ($sites_found as $s) {
            switch_to_blog($s->blog_id);

            $details = get_blog_details();
            $meta = get_site_meta($s->blog_id);

            $obj = new stdClass;

            $obj->blog_id = $s->blog_id;
            $obj->blogname = $details->blogname;
            $obj->siteurl = $details->siteurl;
            $obj->home = $details->home;
            $obj->meta = site_meta_object($meta);

            restore_current_blog();

            $sites[] = $obj;
        }

        $result = new WP_REST_Response( $sites );
    }

    $result->set_headers(array('Cache-Control' => 'max-age=3600'));

    return $result;
}function rest_search_sites( WP_REST_Request $request ) {
    $value = $request->get_param( 'search' );

    $site = new WP_Site_Query();

    $sites_found = $site->query([
        'number' => 9999,
        'meta_query' => [
            'relation' => 'OR',
            [
                'key'     => 'site_name',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'country',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'continent',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'state',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'region',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'local_government_area',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
            [
                'key'     => 'city-town',
                'value'   => $value,
                'compare' => 'LIKE'
            ],
        ]
    ]);

    $result = new WP_REST_Response([]);

    if ( $sites_found ) {
        $sites = [];

        foreach ($sites_found as $s) {
            switch_to_blog($s->blog_id);

            $details = get_blog_details();
            $meta = get_site_meta($s->blog_id);

            $obj = new stdClass;

            $obj->blog_id = $s->blog_id;
            $obj->blogname = $details->blogname;
            $obj->siteurl = $details->siteurl;
            $obj->home = $details->home;
            $obj->meta = site_meta_object($meta);

            restore_current_blog();

            $sites[] = $obj;
        }

        $result = new WP_REST_Response( $sites );
    }

    $result->set_headers(array('Cache-Control' => 'max-age=3600'));

    return $result;
}You are probably thinking I am crazy, but it works. I query the network using an OR meta query, meaning that only one of these has to match to return a result, and I do it for a few meta fields I have (created using ACF Pro).The bit at the end is where I loop over the IDs. I get information about the site and construct an object of metadata and other details I might need on the front end.It’s remarkably simple but powerful. This opens up the possibility of creating sites that are categorised by meta values and those values being searchable.",1600
What is flex: 1 shorthand for?,"If you have used Flexbox before in CSS, you might have used the shorthand property flex: 1. And if you’re like me, you might have been using it but forgotten or not even known what this is shorthand for.flex: 1. And if you’re like me, you might have been using it but forgotten or not even known what this is shorthand for.I know to use this shorthand when I have Flexbox items that I want to take up the rest of the space. It’s perfect for situations where you have an image on the left and a box of text on the right (product image and product name + price).The shorthand translates to the following:flex-grow: 1flex-shrink: 1flex-basis: 0flex-grow: 1flex-grow: 1flex-shrink: 1flex-shrink: 1flex-basis: 0flex-basis: 0We won’t explain what each property does in detail because you have most likely used the non-shorthand versions before. But, we know this tells the browser we want an element to grow and shrink to the available space inside the Flexbox element.",241
Programmatically Create New Sites In WordPress Multisite,"In WordPress, creating new sites from the admin interface can be tedious, especially if you want to add custom metadata to sites/ACF option fields.I had a scenario where I needed to create 1800 sites from a spreadsheet. Doing it one-by-one was not going to cut it, so I needed a code solution where I could loop through these sites and create them without needing the UI.Like almost everything in WordPress, there is a function you can call. It’s called wpmu_create_blog — I have a Multisite subdomain install, so this code won’t work for directory-based multisites (it’s not hard to change, though). And I found the documentation to be quite poor. But, here is what I ended up doing.wpmu_create_blog// The mysubdomain is the part that determines the subdomain of your site
// maindomain.com would be your primary domain your subdomains are using
$domain = ""mysubdomain.maindomain.com"";

// Keep this
$path = ""/"";

// Name of your site
$title = ""My Awesome Site"";

// User ID of the owner of the site
// User ID 1 is the first account on a WordPress install
// You can either create a new user or use an existing ID
$userId = 1;

wpmu_create_blog($domain, $path, $title, $userId);// The mysubdomain is the part that determines the subdomain of your site
// maindomain.com would be your primary domain your subdomains are using
$domain = ""mysubdomain.maindomain.com"";

// Keep this
$path = ""/"";

// Name of your site
$title = ""My Awesome Site"";

// User ID of the owner of the site
// User ID 1 is the first account on a WordPress install
// You can either create a new user or use an existing ID
$userId = 1;

wpmu_create_blog($domain, $path, $title, $userId);While I did not include the loop in this example, this is what I would be calling inside of the loop.The wpmu_create_blog function returns the ID of the newly created site on success or a WordPress error object on error.wpmu_create_blogI would recommend doing something like this:$result = wpmu_create_blog($domain, $path, $title, $userId);

if ( !is_wp_error($result) ) {
  	// The message, ""Site created! ID""
    echo ""Site created! "" . $result;
} else {
  	// It's a WordPress error object
    print_r($result);
}$result = wpmu_create_blog($domain, $path, $title, $userId);

if ( !is_wp_error($result) ) {
  	// The message, ""Site created! ID""
    echo ""Site created! "" . $result;
} else {
  	// It's a WordPress error object
    print_r($result);
}That’s all there is to it.",610
Unpkgd Is Down (Again),"After an outage seven days ago, unpkgd.com, a widely used CDN for NPM packages, is again down.This time, the outage is more severe. At the time of writing, unpkgd.com has been down for hours. Even the official status page is down.status pageWith two outages in such a short period, I am starting to doubt whether I will continue to use unpkgd in the long term. It’s a free service, so I can’t complain really. But, it does highlight the fragility of relying on free services.The benefit of using a popular CDN is, if someone else has already cached the script you’re using from the same CDN, it loads faster on your site because the visitor already has it.Setting up your own CDN is a trivial thing to do. You lose the benefit of global caching through a popular shared domain, but you are in control when things go wrong.",206
A simple solution to Netflix’s subscriber loss: stop cancelling TV shows,"If I had a dollar for every time my wife and I discovered a great show on Netflix only to discover it was cancelled after one or two seasons, we would be stupidly rich.Netflix recently revealed they’re losing subscribers. You probably already knew this because you’ve either cancelled your Netflix account, considered cancelling it or seen the numerous news stories about their untimely demise.they’re losing subscribersWhat some people don’t realise is Netflix, like other companies, pulled out of the Russian market over its unwarranted invasion of Ukraine and lost 700,000 paid subscribers. Between April and July, Netflix lost one million subscribers.Netflix lost one million subscribersWhile losing subscribers is significant because it’s the first time since 2011 they have lost subscribers, it’s hardly the doomsday sky is falling scenario that people feared before the figures were released. Losses stemmed thanks to shows like Stranger Things.One of Netflix’s biggest problems is its cancellation fetish. One of Netflix’s biggest problems is its cancellation fetish. Numerous shows have not given the appropriate amount of time to breathe on the platform. Despite being one of the best Marvel TV show adaptions to date, I am still quite upset over the fact that they cancelled The Punisher. Nobody else will ever play Frank Castle as Jon Bernthal did.Despite making it to an impressive three seasons, Daredevil didn’t fare much better. But it was starting to get good and had a great cast too. Allegedly, Daredevil has been revived and coming to Disney+. Whether it retains the essence of the Netflix series will remain to be seen.Perhaps the cruellest cancellation of them all was The OA. Allegedly the fanbase wasn’t that big, but from the backlash I saw online when it was cancelled, it seemed pretty big. The fanbase was highly-dedicated, even creating a change.org petition to try and save it, alongside a hashtag and other fruitless efforts.creating a change.org petition
Another show cancelled after only one season was Messiah. A TV series about a man claiming to be Jesus, who could do things like walk on water. It was a thrilling show that had the potential to grow into something much bigger. It was allegedly cancelled because of the international cast and different countries it was shot in because of the pandemic.Another sad cancellation is Chilling Adventures of Sabrina. Growing up, like many my age, the nineties version of Sabrina The Teenage Witch was a staple watch. The Netflix version was darker, had a great spin on Sabrina, also well cast. It made it to an impressive four seasons before being cancelled, a lifetime on Netflix.Like The Punisher cancellation, another TV show called Altered Carbon which was set 360 years into the future where people could go into new bodies called skins, was compelling television and cancelled before its prime. Netflix seems to have a thing for cancelling its good sci-fi shows.If Netflix can’t even commit to its content outside of Stranger Things, how can it expect its customers to commit to a subscription? There has to be a term for it when you find a great new show and discover it’s cancelled, on hiatus, or after you become invested gets cancelled.While some continue to predict the fall of Netflix, I think they will be fine in the long term. Netflix has survived this long. I am sure they will work things out. Although, in the current economic climate, Netflix might want to think carefully about how they approach future content and retain subscribers.",884
Fixing the color-adjust shorthand is currently deprecated warning in Autoprefixer,"As much as I love front-end development, the ecosystem can sometimes inflict unnecessary pain. Given the front-end ecosystem relies on very few packages for a lot of modern development, when something changes and packages that rely on those don’t update: it’s a disaster.One such issue is Autoprefixer.You most likely arrived here searching Google for the error message: autoprefixer: Replace color-adjust to print-color-adjust. The color-adjust shorthand is currently deprecated.I have encountered this issue twice in the last few months. The first was Bootstrap (the issue has since been fixed in Bootstrap 5.2). And the most recent is MDB UI Kit which is material design for Bootstrap.Many of the solutions you will find to this problem will tell you to downgrade the Autoprefixer package to version 10.4.5. Either saving the exact version or adding a resolution entry into your package.json. This is because the change was introduced in version 10.4.6 of Autoprefixer.package.jsonThe issue itself is rather trivial. It’s a deprecated CSS shorthand property being used.I didn’t want to downgrade Autoprefixer.In my situation, I fixed the issue by forking the MDB UI Kit repository and submitting a pull request. If you’re a developer and you run into these kinds of issues, a PR to fix them would be welcomed by any active repository.submitting a pull requestYou can downgrade your package, but the best fix is to make the change to the package yourself in a fork on GitHub. While you wait for the maintainer to merge and release, you can tell Npm or Yarn to use your fork as the source to get around the warning.",404
How to change the theme directory on a WordPress installation,"The beauty in WordPress is not only its ecosystem. It’s the ability to customise almost every facet of it using a filter or hook.I had a scenario recently where I wanted to put a web application theme in a less painful directory to access. I wanted a folder called themes in my root directory.themesInstead of wp-content/themes/my-app-theme I wanted themes/my-app-theme instead.wp-content/themes/my-app-themethemes/my-app-themeThe structure resembles something like this:themesmy-app-themewp-contentwp-includesindex.phpwp-config.phpWe can tell WordPress about a new place to look for themes. It’s not a filter or hook, it’s a function called register_theme_directory.register_theme_directoryTo configure the theme’s location, we must make the change outside of our theme. We will create a simple plugin that will go into wp-content/mu-plugins which get automatically loaded without requiring activation. I called mine modify-theme-path.php call yours whatever you want.wp-content/mu-pluginsmodify-theme-path.php<?php
    register_theme_directory( ABSPATH . 'themes' );
?><?php
    register_theme_directory( ABSPATH . 'themes' );
?>Now, WordPress will look in my root directory for a themes folder but will also continue to look in wp-content/themes as well.wp-content/themes",319
Thoughts on Bun,"Say what you will, but since its introduction in 2009, Node.js has been the undisputed king of server-side Javascript. Created by Ryan Dahl, Node.js had virtually no competition for years.Until recently, the only person to truly challenge Node.js was Ryan Dahl with his runtime Deno that improved upon some of the flaws that Ryan saw in Node.js which that were systematic and difficult to fix.Well, there is a new competitor NOT written by Ryan this time called Bun. Who said 2022 had no surprises left?BunIt’s a silly name, but what isn’t silly is how impressive Bun is.From their own website metrics, you can see Bun greatly exceeds Node and Deno in performance quite substantially. Always do your own research and take metrics like these with a grain of salt, but even if only 50% of what is reported is still better.Ryan Dahl and the investors who have poured millions into Deno must be sweating a little bit right now.Not only is Bun a way faster runtime, it also bridges some of the gaps that Deno left (and I believe hindered its adoption).Where Bun differs from others is it uses Javascript Core instead of the Google V8 engine.Javascript CoreV8 engineNot only does Bun replace Node and Deno, it also seems to go further and replace a tonne of other things.A bundler that replaces WebpackTranspiler that supports TypeScript compilation (a la Deno)Support for Npm packagesTask runnerTest runnerCoverage of most of the Node API’s (currently 90%)A bundler that replaces WebpackTranspiler that supports TypeScript compilation (a la Deno)Support for Npm packagesTask runnerTest runnerCoverage of most of the Node API’s (currently 90%)most of the Node API’sHonestly, Bun is incredible. It is what Deno should have been when it launched. Being so new, it’s inevitable that there might be bugs or certain features missing that make it less plug and play than it portrays, but for a project that will only get better: it’s impressive.Bun feels like a project that has a very bright and promising future ahead of it. If Deno doesn’t do something to catch up, then my money would be on Bun.",522
Fixing mysqldump unknown table COLUMN_STATISTICS,"I am by no means a database guy. I am barely even a server guy. But, recently, I was tasked with exporting a sizeable database from Amazon RDS for use with local development servers.After running the following mysqldump command:mysqldump --host hostname -u username -p dbname > dbdump.sqlI am asked for the database password and then proceed to get an error complaining about an unknown table called COLUMN_STATISTICS. Unknown indeed, I had no idea what this table was even for or why mysqldump was trying to query it.In a classic case of RTFM, I discovered that it’s some newly added thing in MySQL client 8.classic case of RTFMmysqldump --column-statistics=0 --host hostname -u username -p dbname > dbdump.sqlWe add the column-statistics flag to our command and set it to 0 to disable it, and the error goes away. The moral of the story is read the documentation.",216
The metaverse is a scam,"There’s been a lot of talk about the metaverse over the last year. According to its advocates, it will be a revolutionary new platform that will let us interact with each other in ways we never could before. But is the metaverse all it’s cracked up to be? I’m not so sure.Admittedly, I don’t really know what the metaverse is supposed to be. Is it a virtual world, or is it a social network for VR headsets? Is it a term to describe a bunch of different futuristic concepts? There doesn’t seem to be a clear definition as to what it is even supposed to be. But, everyone is excited nonetheless.First of all, let’s look at who’s behind the metaverse. It’s Facebook founder Mark Zuckerberg. Do you really trust him to create a safe and secure online space for us? I don’t. Mark Zuckerberg already has a track record of abusing our privacy. Do we want to trust him with even more of our data?The metaverse is based on a false premise. The idea that we can create a virtual world that’s better than the real world is a fantasy. The metaverse will never be able to replace the real world because it’s not realistic.At the Connect 2021 conference where the rebrand was announced, Zuckerberg did a little demo where he went into his Matrix-like world full of Wii avatars. Virtual reality with Wii graphics. For reference, the Nintendo Wii was released 16 years ago.

Immersive realistic experiences have been the promise of virtual reality for decades, but have never been delivered. Current virtual reality technology is still very primitive and the high-end headsets require expensive PC’s. There’s no way that it could support the kind of complex world that the metaverse proposes.I have always been fascinated by virtual reality and I would love to see it succeed and reach the point where it doesn’t feel like you’re looking at poorly rendered cartoon characters without legs. But, it feels like VR has been that way for a long time now.I’ve bought into almost every type of VR headset over the years. From those early PC versions which were shutter shades to Oculus, Playstation VR, HP Reverb and so forth. Despite the fact the technology is getting better, it’s not getting better fast enough to achieve realistic VR experiences.Oh, there is also a serious problem that plagues many virtual reality headsets: VR sickness. This article goes into more depth, but it appears that many of us experience VR sickness after 15 or more minutes. It also appears to get worse with age.This articleWhy replace the real world with a virtual one, when the real world is always going to be better?",646
Is your LinkedIn feed predicting a recession?,"In my LinkedIn feed of late, I see a lot of layoffs. Most of those layoffs are talent acquisition roles, some development and design, but largely talent acquisition.If you’re not familiar with the role of talent acquisition, it’s a fancy way to describe an internal recruiter, someone who seeks out candidates for a company.Well, Twitter just laid off 30% of its talent acquisition team. While the exact number isn’t known, 30% is quite a sizeable percentage of people to layoff as Twitter is amidst a hiring freeze and cutting back on new hires.Twitter just laid off 30%It got me thinking, could LinkedIn be used to predict a recession?One of the first things you’ll see during a pending recession is companies cutting back on expenses to reduce their burn rate, which is a fancy way of measuring how much money you’re spending and how fast you’re spending it.We have seen numerous companies cutting back their workforce these past few months. Elon Musk cut 10% of Tesla’s workforce because he had “a super bad feeling” about the economy and is saying that he believes a recession is coming.We’ve seen Coinbase lay off 20% of its workforce, driven partly by the collapse of cryptocurrencies and the turbulent economy. Many other companies have also reduced their workforce out of fear of a severe economic downturn.LinkedIn would be flashing red now if LinkedIn were an economic indicator. But this is only my feed. Do you see a lot of layoffs in your feed too?",366
Do Developers Rely Too Much on Frameworks and Libraries?,"In recent years, there has been an explosion of front-end development frameworks and libraries. While this has made development more manageable and efficient, it has also led developers to become increasingly reliant on these tools. As a result, when something goes wrong with the library or framework, it can be difficult to determine the source of the problem and fix it.You can cross your fingers and hope someone in a comment on a GitHub issue has a workaround or there is a pull request with a fix. But, I’ve seen how fragile the front-end ecosystem can be when a single library lags behind the updates of other packages it depends on and things can quickly fall apart.This reliance on frameworks can also lead to stagnation in terms of skill development. Since developers are not required to learn the underlying code that makes the frameworks work, they can become complacent and rely on the framework to do all the work. This can be dangerous, as new technologies and trends may emerge that require different skills than what is currently being used in frameworks.While frameworks and libraries have made development easier and more efficient, they have also created some problems. I’ve seen this happen since jQuery first got popular, and when I worked with Ruby on Rails, I saw many using it without even knowing the Ruby language.I have seen many developers try to learn frameworks and libraries without first understanding the underlying language concepts. This often leads to frustration because they don’t understand why their code isn’t working or why they are getting errors. Learning the language concepts first makes learning frameworks and libraries much easier. It also allows you to implement new functionality or workaround limitations in a framework or library.I am a fan of most modern frameworks and libraries, and I think it is hard to go wrong regardless of whether you choose Angular, React, Vue or Aurelia. However, I don’t think that everyone needs to learn any of those to be a good developer.Learn the language, not the abstraction. If you know Javascript and take the time to understand it correctly, you can work with any framework or library. Those skills are transferrable to the next new thing that comes along, and you avoid locking yourself into something that may not exist in five years.",582
Is Bitcoin prepared for the next financial crisis?,"Bitcoin was created as a way to bypass the traditional banking system. But can it survive a financial crisis?Cryptocurrencies are still relatively new and haven’t been tested in a significant financial crisis. If a global recession arose and banks started to fail, would people still trust Bitcoin? Would Bitcoin prove its independence from fiat?Bitcoin is not backed by any government or central bank, which means it is not subject to the same regulations as traditional currencies. This also makes it vulnerable to wild fluctuations in value. In 2013, the price of a Bitcoin went from $13 to over $1,000 in just a few months. And then, just as quickly, the price crashed to $200.Bitcoin is no stranger to crashes. It has had several crashes since 2011. But, it has always bounced back in somewhat favourable market conditions. So, when talk of Bitcoin dying arises, people who believe it will bounce back can be forgiven.several crashes since 2011Investors might see Bitcoin as a safe haven when traditional currencies lose value. Although, the current price teetering between $19-20k seems to suggest people are wary. While this volatility makes Bitcoin a risky investment, it could also be its saving grace during a financial crisis. This volatility is also what makes it attractive to investors; the potential for high returns can be very tempting.The issue is there are two schools of thought. Those who believe Bitcoin is a currency want to replace fiat currency in day-to-day transactions, to use it as it was intended: a currency. And those who see it as an asset, commodity or security to invest into.Bitcoin was created in the wake of the 2008 Global Financial Crisis. It launched at the right time when trust in traditional financial institutions was at an all-time low. It was the perfect anarchy-like response to greed that cost some people everything.But over the years, Bitcoin has gone from a niche virtual currency used to buy pizzas to big business with investment funds, billionaires like Elon Musk and even your relatives wanting to buy into the promise of getting rich quickly.used to buy pizzasYou could argue that Bitcoin became the very thing it swore to replace. Time will tell if Bitcoin is truly unpegged from fiat as the US and other global economies begin to head into recession territory.",580
The RBA Pushes Australia One Step Closer to the Edge of Recession With 0.5% Rate Hike,"As everyone predicted and expected, the Reserve Bank of Australia (RBA) just announced a July 2022 rate hike of 0.5%. Taking the official cash rate to 1.3%.In the official statement, Philip Lowe details the decision and an insight into what the RBA believes is causing Australia’s inflation.the official statementI’ve bolded some important parts of the announcement we’ll explore.At its meeting today, the Board decided to increase the cash rate target by 50 basis points to 1.35 per cent. It also increased the interest rate on Exchange Settlement balances by 50 basis points to 1.25 per cent.

Global inflation is high. It is being boosted by COVID-related disruptions to supply chains, the war in Ukraine and strong demand which is putting pressure on productive capacity. Monetary policy globally is responding to this higher inflation, although it will be some time yet before inflation returns to target in most countries.

Inflation in Australia is also high, but not as high as it is in many other countries. Global factors account for much of the increase in inflation in Australia, but domestic factors are also playing a role. Strong demand, a tight labour market and capacity constraints in some sectors are contributing to the upward pressure on prices. The floods are also affecting some prices.

Inflation is forecast to peak later this year and then decline back towards the 2–3 per cent range next year. As global supply-side problems continue to ease and commodity prices stabilise, even if at a high level, inflation is expected to moderate. Higher interest rates will also help establish a more sustainable balance between the demand for and the supply of goods and services. Medium-term inflation expectations remain well anchored and it is important that this remains the case. A full set of updated forecasts will be published next month following the release of the June quarter CPI.

The Australian economy remains resilient and the labour market is tighter than it has been for some time. The unemployment rate was steady at 3.9 per cent in May, the lowest rate in almost 50 years. Underemployment has also fallen significantly. Job vacancies and job ads are both at very high levels and a further decline in unemployment and underemployment is expected over the months ahead. The Bank's business liaison program and business surveys continue to point to a lift in wages growth from the low rates of recent years as firms compete for staff in the tight labour market.

One source of ongoing uncertainty about the economic outlook is the behaviour of household spending. The recent spending data have been positive, although household budgets are under pressure from higher prices and higher interest rates. Housing prices have also declined in some markets over recent months after the large increases of recent years. The household saving rate remains higher than it was before the pandemic and many households have built up large financial buffers and are benefiting from stronger income growth. The Board will be paying close attention to these various influences on household spending as it assesses the appropriate setting of monetary policy.

The Board will also be paying close attention to the global outlook, which remains clouded by the war in Ukraine and its effect on the prices for energy and agricultural commodities. Real household incomes are under pressure in many economies and financial conditions are tightening, as central banks increase interest rates. There are also ongoing uncertainties related to COVID, especially in China.

Today's increase in interest rates is a further step in the withdrawal of the extraordinary monetary support that was put in place to help insure the Australian economy against the worst possible effects of the pandemic. The resilience of the economy and the higher inflation mean that this extraordinary support is no longer needed. The Board expects to take further steps in the process of normalising monetary conditions in Australia over the months ahead. The size and timing of future interest rate increases will be guided by the incoming data and the Board's assessment of the outlook for inflation and the labour market. The Board is committed to doing what is necessary to ensure that inflation in Australia returns to target over time. COVID-related disruptions to supply chainsthe war in UkraineGlobal factors account for much of the increase in inflation in Australia capacity constraints in some sectorsfloods are also affecting some pricesInflation is forecast to peak later this year and then decline back towards the 2–3 per cent range next yea the behaviour of household spending household budgets are under pressure from higher prices and higher interest ratesThe Board will also be paying close attention to the global outlookclouded by the war in Ukraine and its effect on the prices for energy and agricultural commodities There are also ongoing uncertainties related to COVIDIt is refreshing to see the RBA finally admit that much of the inflationary pressure is caused by cost-push inflation. A lot of what is causing Australia’s inflation is the price of fuel, energy, materials and supply-chain issues around goods and services (conditions beyond the Australian consumer’s control). The extreme weather we’ve faced these last few months is also acknowledged.Despite mentioning numerous global factors (COVID and the Ukraine war), the RBA seemingly cannot accept the fact it is fighting a type of inflation that rate increases are not guaranteed to curtail. It can’t help but gaslight Australians into thinking they are also to blame (despite domestic inflationary pressures being quite soft).This excerpt from the official statement says it all:“One source of ongoing uncertainty about the economic outlook is the behaviour of household spending.”One source of ongoing uncertainty about the economic outlook is the behaviour of household spendingWhile interest rates could not stay low forever, the RBA is overly aggressive in a desperate attempt to fight cost-push inflation, aka imported inflation. I guess most countries lifting rates are doing the same thing.The only outcome of the RBA’s fast-paced successive rate increases is increased cost of living pressures that will have flow-on effects on the economy. All the while doing nothing to alleviate the pressure driving inflation.If Australia manages to avoid a recession, it will be a miracle. By increasing rates so aggressively, it feels all but a sure thing Australia will fall into recession.Ironically, despite interest rates going up, we will most likely see rate cuts in 2023. As the economy sharply reverses due to the aggressive rate increases, central banks like the RBA will have no choice but to undo some of their aggressive monetary policies.",1704
Here’s why I stopped believing in the future of GraphQL,"If you had asked me for my opinion on GraphQL in 2019, I would have told you it was the future of working with data. Like many developers, I got caught up in the hype and believed that GraphQL would kill REST and other forms of data endpoint representation.Fast-forwarding to 2022, my opinion has changed on GraphQL. REST is still very much alive and the preferred approach for API design.There is a lot to like with GraphQL, and the things it promises on paper are incredible.No longer having to create new controllers and routes to expose data on the front-end? CheckShifting the power from the server-side to the front, allowing developers to query for what they need? CheckPrecise queries allowing you to query for data you need resulting in smaller payloads? CheckType checking and schema introspection? CheckNo longer having to create new controllers and routes to expose data on the front-end? CheckShifting the power from the server-side to the front, allowing developers to query for what they need? CheckPrecise queries allowing you to query for data you need resulting in smaller payloads? CheckType checking and schema introspection? CheckMaybe it’s because I am a front-end developer and my perspective on things is warped. But, having implemented GraphQL quite a few times over the years (especially for blockchain-based projects), you eventually realise that GraphQL is a lot of work.Now, this is subjective. Some of you using GraphQL might disagree. Perhaps you’ve found a solution that minimises the time it takes to implement GraphQL, and you’re thinking, “Takes more work. What are you on about?”Using the route/action/controller paradigm found in the traditional REST approach at first seems like more work than the query approach GraphQL promises until you start encountering issues long-resolved in the REST world like caching, security and performance.For every project I used GraphQL on, there was a recurring theme. I would seemingly get caught up on things long solved in traditional HTTP land. With Apollo Client, you get their hokey-pokey caching solution, but it’s more of a hack. It is possible to use GraphQL over HTTP where the POST requests become GET requests.Don’t get me started on the rabbit hole of batched queries and rate-limiting (especially when you throw auth into the mix). At this point, you will start delving into persistent queries and data management layers to avoid repetition or recursion. Then you have constraints around depth and query complexity to stop an end-user or developer in your team from writing queries that melt your server or spin up one too many EC2 instances.While it’s possible a lot has changed in the GraphQL world since I last used it, and many of these problems have been long solved, to set it up correctly, you needed to put a lot of effort in to get a result that is indistinguishable from a standard REST API.I didn’t give up on GraphQL because I thought it was terrible. I gave up on it because I felt like I was spending more time configuring and running through a checklist of items you don’t have to account for with REST. Once you get GraphQL working, it is fast, efficient and fun, but getting to that point is not fun and burned me out.Maybe one day, I will revisit GraphQL and see if any of these downsides have been addressed.",828
"Now More Than Ever, Developers Should Have the Choice Between Remote Work and the Office","After two years of pandemic restrictions and mandates, things finally started opening up in late 2021. By early 2022, most countries with the most aggressive restrictions started to ease things.It felt like we were returning to normality, and there was hope. Some companies allowed their employees to take a hybrid approach of a few days in the office and at home. Then some companies started forcing employees back into the office. And some companies threatened their employees to return.Despite working well for most companies that had no choice but to tell their workers to stay home, some companies were seemingly itching to get their employees back into the office upon the first whiff of the pandemic lifting.
Anyone else find it puzzling that big companies had record profits in 2021 while working from home and are now forcing employees back to the office ""to be more productive?""Maybe it's about control and not productivity.— Dan Price (@DanPriceSeattle) March 1, 2022
Anyone else find it puzzling that big companies had record profits in 2021 while working from home and are now forcing employees back to the office ""to be more productive?""Maybe it's about control and not productivity.— Dan Price (@DanPriceSeattle) March 1, 2022Anyone else find it puzzling that big companies had record profits in 2021 while working from home and are now forcing employees back to the office ""to be more productive?""Maybe it's about control and not productivity.March 1, 2022To the credit of some forward-thinking companies that get it, not all companies mandated a return to the office. Xero offers flexibility (hybrid or remote work). Airbnb announced a work and live anywhere policy. Dropbox announced a “virtual first” strategy offering permanent remote work. Atlassian offers a permanent remote work option too. Companies like Slack, Spotify, Twitter and Square also offer the option of permanent remote work.Xero offers flexibilityAirbnb announcedDropbox announced a “virtual first” strategyAtlassian offers a permanent remote work optionThen you have the companies that learned nothing from the pandemic-induced remote working revolution.In March 2022, Google told workers in its San Francisco Bay Area offices they’ll have to be back by April 4, 2022, offering a hybrid approach of three days in the office.Google told workers in its San Francisco Bay Area offices
have people pushing for return-to-office considered that work is one million percent better from bed— ely kreimendahl (@ElyKreimendahl) March 1, 2022
have people pushing for return-to-office considered that work is one million percent better from bed— ely kreimendahl (@ElyKreimendahl) March 1, 2022have people pushing for return-to-office considered that work is one million percent better from bedMarch 1, 2022It’s worth acknowledging that not everybody thrived during the pandemic. For some, being forced to stay at home was a nightmare that they wanted to end. When schools closed, working and being a parent was a tricky, stressful exercise. Ikea was out of stock for such a lengthy period (getting a desk or office chair was a difficult task in 2021), and some didn’t have proper working spaces.And herein lies the problem: employees want flexibility and choice.For some, working from home will be the preferred option. For others, a hybrid approach where employees will prefer a few days here and there in the office (mainly for social reasons) and at home. And for a minority, going back into the office full-time.If you run a company or work for one that spouts nonsense such as “collaboration or “culture” as reasons for returning to the office, it’s a red flag the company you work for has culture and trust issues. Forcing your employees to return to the office is a recipe for disaster and a sure-fire way to drain your company of talent. As we have seen, some companies have embraced the changing face of working by offering employees the choice of what works best for them.We have the tools, methodologies and technology to make remote work a success. The last couple of years has been excellent training for many. The record profits companies were making seem to suggest that remote work wasn’t a bad thing at all. It’s the outdated in-office mindset that some companies need to let go of. Workers want choice. Give it to them, or they’ll get it elsewhere.",1086
Why are we not fixing the real causes of inflation?,"First, let me preface this with the fact that I am not an economist. I am an avid armchair researcher who delves into topics like monetary policy and other facets of life that may affect or interest me.Around the world, central banks are hiking interest rates. The United States recently saw interest rates increase by .75 basis points. Canada, Australia and New Zealand saw .50 basis point increases. With more increases planned for most countries, things will get worse before they get better.In Australia, we keep being told that interest rates are increasing to combat inflation. We are also being told that inflation is homegrown and not imported.Nobody is arguing that interest rates shouldn’t increase from their historic lows. However, more people should be disputing the reasons they are being given by the central bank when it comes to the reasons for inflation. COVID-19 has had a well-documented effect on global supply chains. We saw shipping prices go through the roof as shipping containers started piling into the atmosphere at ports due to lockdowns and COVID-19 restrictions and the logistics built around these things.Even after we saw pandemic-era restrictions and mandates start to lift, things still have not improved as much as you would think.The war in Ukraine and $11 lettuces have highlighted that the just-in-time (JIT) supply chain model is flawed and broken. Despite being championed by the likes of Toyota and yielding great success, it comes at the cost of being vulnerable when one or more of those pieces fails (like a natural disaster, pandemic or conflict).Take lettuce, for example. It’s not $11 because people rush out with their excess pandemic savings and splash big on lettuce instead of flat-screen TVs. Catastrophic widespread flooding that has been hammering many parts of Australia since early 2022 wiped out numerous crops. Similarly, other vegetables are also seeing sharp increases.Central banks (the Reserve Bank of Australia included) are attempting to reduce demand for goods and services through aggressive rate increases to shock the economy and reduce the amount of money people have to spend. The flow-on effect is that businesses might shrink their workforce due to reduced spending, and you have a new problem.Those increases will come at the cost of jeopardising the economy but not fixing the underlying issue.Central banks are doing this because most of the tools in the monetary first aid kit are blunt. An interest rate increase is a hammer and blunt chisel, attacking from the bottom like a block of ice.In their attempt to fight inflation, central banks will attack food, energy and fuel expenditures because it’s all they can do.When you have people deciding between eating dinner and heating their house through a brutal winter, filling up their car so they can drive to work or going without basic human needs, you have a real crisis on your hands that interest rate rises will exacerbate not fix.To return inflation to the targeted “normal” range, the supply of goods and services needs to return to normal to avoid a recession. And right now, all signs point to a recession in Australia and other developed nations as no end is in sight for the global economies’ supply problems.The question posed in the title has an answer: because we can’t. Driven by decisions made during the pandemic (loose and unchecked bipartisan economic policy) and the impact the pandemic had on supply chains and the Ukraine war, there is no simple fix.",875
The Whisky Club: A Review,"Unbeknownst to some, I am an avid brewer and whisky enthusiast, besides programming and music. Finding your preferred type of whisky can be a big undertaking. The world of whisky is intricate, and exciting and can be intimidating to newcomers.Before joining The Whisky Club, I saw ads for it constantly in my Facebook feed and wrote them off initially. I suspect the Facebook ads are how many people find their way to the holy grail of Whiskydom.The Whisky ClubWhile always drawn to nice drams, I’ve never been a whisky genius. Honestly, my approach used to be a mixture of price, brand and the shape and label on the bottle. There are so many whiskies out there. Even price can be the wrong indicator of a good bottle (many of my favourites are under $100 a bottle).If you are unaware of what The Whisky Club is, they call themselves “Australia’s biggest whisky subscription club” — think of them as the Netflix for whisky, except the whisky selections are curated and it’s one showcase bottle a month.Using their sizeable subscriber count, The Whisky Club broker deals with distilleries to get some incredible bottles. Each month they have a featured whisky, giving you the option to receive one bottle or double-up (get two bottles). A lot of single malts, some unique blends from renowned distilleries and other gems can be found.I finally joined The Whisky Club in December 2020, and since then, my whisky collection has grown astronomically. It takes me a while to get through a bottle, but whisky tends to last a long time if you store it correctly.When writing this review, this month’s whisky is Lark Rising Tide.Lark is one of those established and respected Australian distilleries, and for July 2022, they have created what appears to be an incredible dram for subscribers. Some month’s they get something special, and you will want to double up in those instances (subject to allocation).It’s quite a simple process. You are not obligated to buy any whisky. Suppose the whisky of the month doesn’t appeal to you; you opt out. Otherwise, they assume you want that month’s whisky. There are no contracts. You’re not forced into anything. Admittedly, some months I opt out because of the number of whiskies in my cupboard.As you can see in a snippet of my order history, I sometimes skip one or more months.On top of the monthly whisky, they also do additional optional add-ins. Sometimes it will be the whisky from the previous months or a special bottle to add in.Cost-wise, you are looking, on average, $130 a bottle, depending on what it is. Some of these whiskies are worth more than that. The special bottles they offer to bundle in your order can be double the price, but they’re optional.Like a real whisky club, they also do virtual tastings, and there is an active community of whisky lovers. They also offer discounts and access to events through their whisky network. Admittedly, I haven’t taken much advantage of those. Unlike other subscription services, you feel like you’re a part of something special, not just a fish in a big sea of whisky brokerage power.When I joined in December 2020, I told myself I would only try it out for a month (a monthly whisky subscription seemed excessive). Well, almost two years on, and I am still a member. It’s nice not having to linger in the aisles of Dan Murphy’s looking through their whisky cabinets for something to try.The thing about The Whisky Club is I don’t just treat it as a means of stocking up my overflowing whisky selection. I also gift some bottles I’ve bought as birthday and special occasion gifts (Father’s Day, Mother’s Day, birthdays, Christmas).When our dog ran away recently, we thanked the people who found him and returned him to us with one of those bottles of Lark Rising Tide from the July 2022 offering. I ended up ordering a replacement bottle because I enjoyed this bottle of Lark so much. It is handy to have some spare whisky around when you need it (not just for drinking).Understandably, given the rising cost of living, subscriptions like The Whisky Club might be a bit expensive to justify regularly. However, The Whisky Club is a great option if you’re looking for a way of trying new delicious whiskies or a gift for someone. You can always cancel or pause your subscription after the first bottle.If you do decide to try The Whisky Club, by using my referral link here, we both get free whisky. I have no idea what the free whisky is. They say it depends on availability, but if you were going to join anyway and try it out, free whisky is free.using my referral link here",1144
Web 3.0 may have died before it even started,"The buzzword of 2021 was unmistakenly Web 3.0.Facebook, Instagram, Medium, TikTok, Twitter: Web 3.0 dominated the discourse. Investment funds were flocking to invest in any company loosely affiliated with the hottest new trend on the web.Depending on who you spoke to or what you read, Web 3 would kill Facebook, Twitter and every other popular website and application. In its place, Web 3 decentralised alternatives would take their place.Web 3 is the ultimate crypto enthusiast wet dream.Except not one single project has delivered the promise of a decentralised alternative to anything; instead, Web 3 has been nothing more than a bunch of rug pull scams run or endorsed by celebrities looking to make a quick buck (look up Floyd Mayweather and his crypto scams).You see, Web 3 is a problem looking for a solution. People don’t care if the platforms they use are walled gardens or decentralised blockchains. People who use Instagram never stop and wonder if their images are being stored in some decentralised ledger of mathematical problems.Most of the Web 3 projects I’ve heard about are nothing more than whitepapers or shower thoughts. Despite VC funding finding its way into the blockchain space, is there a single worthy project that doesn’t require you to buy their scam ERC-20 tokens to use?I leave you with this final gift: https://web3isgoinggreat.com/https://web3isgoinggreat.com/",349
Deno Raises $21M – but is anyone using it yet?,"After raising a $4,900,000 seed investment back in March 2021, Deno has just announced quite a substantial round of Series A investment of $21 million. The funding round led by Sequoia brings its total investment to $26 million to date.has just announced quite a substantial round of Series A investment of $21 millionDeno will mainly use the cash to build their commercial offering Deno Deploy. Deno DeployAdmittedly, I shamefully wrote Deno Deploy off as a Deno-specific Heroku. Still, after the announcement, I looked deeper at Deno Deploy, and it’s so much more than that. While the name might sound like a continuous integration tool, it’s a Runtime As A Service (RaaS) platform allowing you to run scalable code.I could see Deno Deploy being useful for scalable APIs for IoT devices and other use-cases.Sadly, despite Deno 1.0 being released over two years ago now, Deno still feels like a niche Node.js alternative that people have thought about using or looking into but haven’t taken the leap into building something with it.Deno 1.0In case you’re not aware, Deno is the brainchild of Node.js creator Ryan Dahl and is seen as a continuation of Node.js to address some of the fundamental flaws (security and environment-related) Ryan has been very vocal about over the years.I mean no ill intent towards Deno because I want it to succeed. Everything about it on paper is excellent, and the brief experimentation I have had with it was promising. In my opinion, the native support for TypeScript is a very underrated feature.Still, despite wanting to build something Deno, I do not have enough time to dive into it. The company I work for hasn’t got the time or money to look into a new Javascript environment, even if it is leaps and bounds better than Node.js.I also know nobody in my network using Deno, and it’s a shame. I still intend on building something with Deno when I get the time, but the events over the last couple of years with the COVID-19 pandemic have thrown a spanner in the works for many businesses, and people who had their lives turned upside down.Are you using Deno, or have you even tried it yet?",532
Speeding Up WordPress Using Local JSON,"As much as I love WordPress, there are some fundamental flaws in how it works. For the average user, WordPress out-of-the-box will do everything you want and can be run on affordable hardware. For the project I have been working on, scaling considerations have reached a code level.I needed to speed up some WordPress REST API endpoints in my use case. Between the core heft, plugin weight and size of the database causing some queries to be slower than your average site, I had to look for a solution that functioned like a cache.Reading and writing JSON filesInstead of expensive reads and writes for what is simple data, I explored using JSON files hosted on Amazon S3 as a potential solution knowing that using Amazon S3 could introduce latency into the application; it could still be faster than the weight of the core/plugins/database.The code below defines the REST API endpoint. It takes an array of post IDs and a blog ID (this is for a WordPress Multisite installation) and saves their position (it’s a story queue).register_rest_route( 'queue/v1', '/update/(?P<name>[a-zA-Z0-9-]+)', array(
    'methods' => 'POST',
    'callback' => 'rest_pinning_queue_positions',
    'permission_callback' => '__return_true'
) );

function rest_set_queue_positions( WP_Rest_Request $request ) {
    $body = json_decode( $request->get_body() );
  
  	$category = $request->get_param('name');
  
    $ids = (array)$body->ids;
    $siteId = (int)$body->siteId;
    $file_name = ""{$siteId}-{$category}"";
  
  	$options_order = [];
  
    foreach ($ids as $id) {
        $options_order[] = [
            'id' => (int)$id->id,
            'blog_id' => (int)$id->blog_id
        ];
    }
  
    write_to_json_file_on_s3($file_name, json_encode($options_order));

    return new WP_REST_Response($options_order, 200);
}register_rest_route( 'queue/v1', '/update/(?P<name>[a-zA-Z0-9-]+)', array(
    'methods' => 'POST',
    'callback' => 'rest_pinning_queue_positions',
    'permission_callback' => '__return_true'
) );

function rest_set_queue_positions( WP_Rest_Request $request ) {
    $body = json_decode( $request->get_body() );
  
  	$category = $request->get_param('name');
  
    $ids = (array)$body->ids;
    $siteId = (int)$body->siteId;
    $file_name = ""{$siteId}-{$category}"";
  
  	$options_order = [];
  
    foreach ($ids as $id) {
        $options_order[] = [
            'id' => (int)$id->id,
            'blog_id' => (int)$id->blog_id
        ];
    }
  
    write_to_json_file_on_s3($file_name, json_encode($options_order));

    return new WP_REST_Response($options_order, 200);
}In the above example, we call a function called write_to_json_file_on_s3 that takes our JSON encoded array of values and saves them inside a JSON file.write_to_json_file_on_s3function write_to_json_file_on_s3($name, $body) {
    $credentials = new Aws\Credentials\Credentials('key', 'key');

    $s3 = new S3Client([
        'region'      => 'your-region',
        'version'     => 'latest',
        'credentials' => $credentials,
        'debug'       => false
    ]);

    $filename = ""queues/{$name}"" . "".json"";

    try {
        $s3->putObject(array(
            'Bucket' => 'your-bucket-name',
            'Key' => $filename,
            'Body' => $body
        ));
    } catch (S3Exception $e) {
        echo $e->getMessage() . ""\n"";
    }
}function write_to_json_file_on_s3($name, $body) {
    $credentials = new Aws\Credentials\Credentials('key', 'key');

    $s3 = new S3Client([
        'region'      => 'your-region',
        'version'     => 'latest',
        'credentials' => $credentials,
        'debug'       => false
    ]);

    $filename = ""queues/{$name}"" . "".json"";

    try {
        $s3->putObject(array(
            'Bucket' => 'your-bucket-name',
            'Key' => $filename,
            'Body' => $body
        ));
    } catch (S3Exception $e) {
        echo $e->getMessage() . ""\n"";
    }
}Using the AWS SDK we installed using Composer, we create an instance of the S3Client and then interface with Amazon S3, where we store our JSON file using the putObject method.putObjectSimilarly, we create a function that can read from Amazon S3 as well to get these queues:function get_queue_from_s3($name, $siteId) {
    $url = ""https://my-bucket.s3.my-region.amazonaws.com/queues/{$siteId}"" . ""-"" .$name . "".json"";
    $response = wp_remote_get($url);

    if ( is_wp_error($response) ) {
        return $response;
    }
    
    $body = wp_remote_retrieve_body($response);

    return json_decode( $body, true );
}function get_queue_from_s3($name, $siteId) {
    $url = ""https://my-bucket.s3.my-region.amazonaws.com/queues/{$siteId}"" . ""-"" .$name . "".json"";
    $response = wp_remote_get($url);

    if ( is_wp_error($response) ) {
        return $response;
    }
    
    $body = wp_remote_retrieve_body($response);

    return json_decode( $body, true );
}Using JSON files on Amazon S3, requests went from taking upwards of four seconds to sub one second. There could have been other avenues to explore and ways to reduce those long requests, but this solution was the easiest and allows for the content being saved to be accessed by other applications outside of the WordPress site too.",1301
DJI’s botched Mini 3 Pro launch: what went wrong?,"Over one month ago, the DJI Mini 3 Pro drone was launched. Those who got on the order bandwagon right away, fortunately, got their units without delay (like I did).However, one thing that DJI has botched about the launch is the availability of accessories. Most notably, the Fly More and Fly More Plus kits. The Fly More Plus kit features the extended-range batteries that many who plan on using the Mini 3 Pro for photography or travel purposes want.Since the Mini 3 Pro launch in mid-May, the fly more kits, as well as batteries and propeller replacements, have been delayed multiple times. First, it was meant to be by the end of May, then early June, then late June, and now many of these accessories have been delayed until late July.While my experience is contained to Australia, this is a global issue due to supply chain problems. It sounds rather entitled, given what is happening in the world now. Still, it’s also frustrating that DJI has once again launched a product without any additional batteries or the ability to even replace the propellers.The botched launch has had the biggest impact on official third-party retailers. In Australia, the official seller for DJI products is D1 Store.D1 StoreI have had an order for Mini 3 Pro Propellers since May 20, 2022. I asked support when I could expect them, and they said they don’t have an ETA for when they’ll arrive from DJI. I then went to DJI’s store (which you can also confusingly order from), and they told me they had some in stock to be dispatched the next day.For the Fly More Kit Plus, I was told that they could ship from July 12, 2022. I had an order with D1 Store from May 20 as well. I have asked them when they expect the shipment to arrive, and they said they don’t have an ETA for that either. Presumably, it’s late July like DJI.As you can see in my conversation with a D1 Store representative, they don’t know when the Mini 3 Pro Propellers are coming. Nor do they know when the Fly More kits are coming either.Some people have obtained these kits in other parts of the world. In the UK, allegedly, retailers have received shipments of Mini 3 Pro accessories, and Australia seems quiet on the Western front.It’s so bizarre that DJI has launched a new drone but seemingly didn’t have the necessary stock, nor were they ready to even ship the accessories for this drone. Be careful if you do get a Mini 3 Pro. Without the ability to get replacement propellers, you might be unable to fly your new expensive gadget until July.",626
The Great Crypto Collapse Is Upon Us,"Bitcoin and other cryptocurrencies are no stranger to meteoric price drops. In the blink of an eye, a coin can increase hundreds of percent and plummet to near zero. Cryptocurrency is an emotional and monetary rollercoaster.Coinbase has just announced it is laying off 18% of its workforce immediately. Although, it is apparent Coinbase executives didn’t seem to have as high hopes as the wider crypto community, selling off $1.2 billion in shares since it was listed.laying off 18% of its workforceselling off $1.2 billion in sharesCoinbase overhired and spent way too much on talent than the market could support. The recklessness in which Coinbase has been run has resulted in a company sensitive to crypto turbulence. The current downturn isn’t the first; we saw this exact situation in 2017. It appears Coinbase learned nothing.Also worth noting, Crypto.com also laid off staff, 260 staff (around 5% of its workforce). And not too long ago, Gemini laid off 10% of its workforce, saying it was preparing for “crypto winter”Crypto.com also laid off staffGemini laid off 10% of its workforceUnlike in 2017, there is more to lose this time around. While 2017 disproportionately affected average traders (many buying in because of the inescapable hype), the 2022 plunge has financial institutions, hedge funds, companies and even countries invested.In simple terms: there is a lot more money invested, not just Uncle Jack, who bought some Bitcoin because he saw it on the news or discussed it at the Thanksgiving dinner table.What contributes to the drop is dramatically rising inflation fuelled by the Russia/Ukraine conflict and parts of Asia still trying to fight COVID, resulting in supply chain logistics issues that have a flow-on effect on the rest of the world.Experts are sounding the alarm that we’re possibly heading for a global recession. Many have been sounding that alarm for two years or more.Ironically, Bitcoin was born out of the 2008 Global Financial Crisis. It was meant to be a new start, designed to address the problems with fiat currency. It appears that Bitcoin and its offspring prove that even cryptocurrency is not immune to external economic conditions.Many Bitcoin proponents (myself included at certain points in my crypto life) have argued Bitcoin is a long-term store of value, and that it’s the digital equivalent of gold. Sorry, but gold doesn’t drop tens of percent like Bitcoin can. Price fluctuations and corrections are a part of a healthy market, but these drops are anything but healthy.I mean, look at this Bitcoin price graph. This is from January 2022 to June 2022 (halfway through June). It’s pretty damn harrowing to look at. Bitcoin has fallen off a cliff, and like it always does, it drags pretty much the rest of the crypto market along with it.But we’ve seen this all before. Bitcoin has had a handful of massive drops in value over the years, including one in 2021. Unlike previous falls, we are seeing some occur closer together. Bitcoin has been falling for a while now. A 50% drop isn’t cause for concern for some Bitcoin hodlers who have been around longer than a couple of years.If we do see parts of the global economy fall into recession, a repeat of what happened in 2008 (or worse), we will see just how resilient cryptocurrency can be during a real financial crisis. So far, cryptocurrencies have yet to experience a recession. Will hodlers really keep their positions or will they sell to keep the electricity turned on, so they don’t freeze in the winter?Love him or hate him, respected investor Warren Buffet has been sounding the alarm on cryptocurrency since 2018 and most recently again. You could argue that old money doesn’t understand new money, but it’s hard to argue that Berkshire Hathaway hasn’t had an incredible run with its investments and management of funds.most recently againWhile Coinbase might be one of the biggest, this is only the beginning. In the coming months, you’re going to see exchanges completely fail. Many exchanges have been operating like the bull run would last forever. Stupidly hiring large swathes of talent instead of cautiously operating with some kind of buffer.Don’t be surprised if Coinbase lays off more of its workforce in another month or two. Other companies will soon be following suit.For some, the “blood in the streets” will lure die-hards who haven’t already been liquidated into buying more. Others will encourage others to buy more despite the possibility that Bitcoin may break beneath $20k USD and trigger a catastrophic sell-off. We are already nearing a terrible marker: undoing five years’ worth of gains.
You’re telling me we should buy more #BTC? https://t.co/jwvn0A1kTb— Nayib Bukele (@nayibbukele) June 14, 2022
You’re telling me we should buy more #BTC? https://t.co/jwvn0A1kTb— Nayib Bukele (@nayibbukele) June 14, 2022You’re telling me we should buy more #BTC? https://t.co/jwvn0A1kTb#BTChttps://t.co/jwvn0A1kTbJune 14, 2022The famous question: is Bitcoin dead? will undoubtedly be asked for the millionth time. With global economies falling and inflation running rampant, it does make you wonder how many cryptocurrencies will die as a result? Bitcoin will likely live, but it will be trimmed down in the short to medium term. Where is the bottom?",1320
Thoughts on the Flipper Zero,"After seeing the Flipper Zero was finally shipping, I tried to get one. Unfortunately, in Australia, getting the Flipper Zero officially was impossible. There seems to be a lot of demand for this little gadget. Fortunately, there were a few on eBay. The original Kickstarter campaign is here if you want to read about it. As you can see, it was behind schedule, missing its delivery deadline by about a year. But, to the credit of the team, they delivered on every single promise (which is rare).hereI ended up paying quite a premium as the Flipper Zero is a little hard to obtain, at least here in Australia. I am sure they’ll be easier to get in a few months and hopefully cheaper.After getting my beloved Flipper Zero, I set out, like most people who get these gadgets, to see what I could do. For such a small device, it can do infrared, NFC, GPIO, iButton, 125 kHz RFID and most importantly: the ability to transmit and receive sub-GHz frequencies.Perhaps the most surprising thing when I started playing with my Flipper Zero was how far it could transmit. The antennas in this thing would not be that big, but the distance is quite admirable for such a tiny device.My first use of this device was trolling my father. I cloned his TV remote and started messing with his TV. Because the device was so small, he had no idea. This was a hilarious first use of the Flipper Zero, which can be an exceptional troll gadget.It’s important to taper your expectationsThe Flipper Zero is not some magical Watch Dogs-inspired hacking device that will allow you to control traffic lights, control security cameras or make ATMs give you money. It’s quite limited in what it can do and is very much a gadget for light pentesting and a gentle introduction to the world of sub-frequencies.The Flipper Zero most interested me for its ability to work with sub-GHz frequencies (frequencies below 1 GHz). This allows you to read key fobs like modern wireless car key fobs, garage door openers, ceiling fan remotes and many other devices that use sub-GHz frequencies.Before buying the Flipper Zero, you should know that many modern sub-GHz devices, such as garage doors, leverage a security concept called rolling codes. Simply put, it means your remote and device that receives the button presses are keyed and, leveraging some secret seed, transmits a different code each time.My first attempt to clone my wireless car key fob and garage door failed. You’re presented with a lock icon in the Sub-GHz menu when you do a read if it’s using rolling codes. The stock firmware will not let you save these, but third-party firmware (I recommend one below) will.The Flipper Zero will not allow you to bypass rolling code encryption. There is a third-party tool with a free option called Kaiju, which claims to be able to decrypt rolling codes, but I haven’t tried it, and I am not sure if it will ever be possible.KaijuThis is perhaps the first disappointment you encounter while using the Flipper Zero. And, I want to point out it’s through no fault of the Flipper itself. Manufacturers implementing rolling code security prevent people like you and me from opening their garage doors and unlocking their vehicles. Fair call, I’d say.Some remotes and fobs still use fixed codes, but most modern sub-GHz devices use rolling codes. Still, it’s a fun device. I recommend getting some Tesla captures that allow you to open the charging port on Tesla vehicles. You haven’t lost your Flipper Zero virginity until you’ve popped a Tesla charging port.Reliving my fun as a child when I bought a universal remote and would mess with the TVs at school and the neighbour’s house. On the first day, I received the Flipper Zero; I turned off some TVs. I then started reading things like my bank card, Amiibos and anything else with an NFC or RFID chip.If you want to experience the next level of fun, go into an electronics store (I went into Costco) and become God by turning off all TVs, turning up the volume and trolling the entire store.Because this gadget works with frequencies and is sold, the FCC licences it (a legal requirement). Therefore some frequencies are restricted in the stock firmware to comply with region restrictions on what frequencies you’re allowed to transmit or receive. However, many third-party firmware removes this restriction and adds new features and frequencies. My favourite is Roguemaster.RoguemasterAnother use case I have used a lot is the Amiibo support. Amiibos can give you new characters and other features in certain Switch games. This has allowed me to pretend to own numerous Amiibo without buying them for my Nintendo Switch.Overall it’s a fun device and seems to be regularly updated. Seeing the community release unlocked firmware and features gives me hope in the future, this little device will be able to do so much more than it currently can.The Flipper Zero has taken me down the software-defined radio (SDR) rabbit hole. I placed an order for a HackRF One and a Portapack a few days after getting my Flipper, so wish me luck as I go down what could be quite an addictive path of working with radio and frequencies.Excuse me. I have some TVs to turn off at my local McDonald’s again.Extra ReadingIf you are like me and you’ve become addicted to the world of frequencies, I highly recommend going deeper and considering other complementary devices. Shortly after buying the Flipper Zero, I bought a HackRF One. This device allows you to do similar things to the flipper, except it supports much larger antennas.HackRF OneYou can buy third-party HackRF One devices, which are a fraction of the cost of the official one; your quality may vary. Make sure you get yourself a Portapack to make your HackRF One portable, and you have a powerful radio device that can even do GPS.There is also a similar device to the Flipper called PanwaRF, which claims to be able to use Kaiju (I linked above) to break rolling code encryption.PanwaRF",1487
Stopping PHP From Stripping out Hyperlinks From a NITF XML Response While Parsing the XML,"This is another of those particular posts that might help one or two people out. If I can save you some time working with the News Industry Text Format in PHP, I’ll be glad that you didn’t experience my frustration. News Industry Text FormatWhile working with the Associated Press API, I recently ran into a situation where ingested content from the NITF format they supply was being stripped out in PHP.The code in question looked like this:function download_story_nitfy($nitf_href) {
	$nitf_file = file_get_contents($nitf_href . ""&include=view_default&apikey="" . AP_API_KEY);
    $nitf_file = str_replace(array(""\n"", ""\r"", ""\t""), '', $nitf_file);
  
    $nitf_xml = simplexml_load_string($nitf_file);
    $nitf_json = json_encode($nitf_xml);

    return json_decode($nitf_json);
}function download_story_nitfy($nitf_href) {
	$nitf_file = file_get_contents($nitf_href . ""&include=view_default&apikey="" . AP_API_KEY);
    $nitf_file = str_replace(array(""\n"", ""\r"", ""\t""), '', $nitf_file);
  
    $nitf_xml = simplexml_load_string($nitf_file);
    $nitf_json = json_encode($nitf_xml);

    return json_decode($nitf_json);
}The Associated Press API will return the XML, where the content is contained within the &lt;block> element inside of the XML response. Inspecting it in Postman and the browser is fine; however, the content being ingested suffered from missing links, thus breaking the content.&lt;block>I knew the API response was OK, so I set out to debug and realised that the simplexml_load_string call was stripping out the links inside of the content.simplexml_load_stringThis is a snippet of what the code looked like:<block>
              <p>KEY DEVELOPMENTS IN THE RUSSIA-UKRAINE WAR:</p>
              <p>— <a href=""https://apnews.com/article/russia-ukraine-kyiv-moscow-d01152d589a482b52f1072ce9886fbe1"">Scars of war</a> seem to be everywhere in Ukraine after 3 months</p>
              <p>— <a href=""https://apnews.com/article/russia-ukraine-government-and-politics-de1d3ccf3ef990a046cafd7209d4653d"">Saving the children</a>: War closes in on eastern Ukrainian town</p>
              <p>— Sweden, Finland delegations go to Turkey for <a href=""https://apnews.com/article/russia-ukraine-middle-east-turkey-98d9b2bf7de63b3044d118e833626b13"">NATO talks</a></p>
              <p>— US to end <a href=""https://apnews.com/article/russia-ukraine-janet-yellen-government-and-politics-20dbb506790dddc6f019fa7fdf265514"">Russia's ability to pay</a> international investors</p>
              <p>— UK <a href=""https://apnews.com/article/russia-ukraine-putin-roman-abramovich-mlb-politics-710a500504e940db9d60ce3e674da346"">approves sale of Chelsea</a> soccer club by sanctioned Abramovich</p>
</block><block>
              <p>KEY DEVELOPMENTS IN THE RUSSIA-UKRAINE WAR:</p>
              <p>— <a href=""https://apnews.com/article/russia-ukraine-kyiv-moscow-d01152d589a482b52f1072ce9886fbe1"">Scars of war</a> seem to be everywhere in Ukraine after 3 months</p>
              <p>— <a href=""https://apnews.com/article/russia-ukraine-government-and-politics-de1d3ccf3ef990a046cafd7209d4653d"">Saving the children</a>: War closes in on eastern Ukrainian town</p>
              <p>— Sweden, Finland delegations go to Turkey for <a href=""https://apnews.com/article/russia-ukraine-middle-east-turkey-98d9b2bf7de63b3044d118e833626b13"">NATO talks</a></p>
              <p>— US to end <a href=""https://apnews.com/article/russia-ukraine-janet-yellen-government-and-politics-20dbb506790dddc6f019fa7fdf265514"">Russia's ability to pay</a> international investors</p>
              <p>— UK <a href=""https://apnews.com/article/russia-ukraine-putin-roman-abramovich-mlb-politics-710a500504e940db9d60ce3e674da346"">approves sale of Chelsea</a> soccer club by sanctioned Abramovich</p>
</block>The XML parser call in PHP would remove those links. They saw those as not being valid, or the parser wasn’t accounting for child nodes. It was a rather frustrating issue, and despite extensive Googling, I found no easy solution. Many were saying to use cdata markers around the links in other use cases.cdataIn the end, that is what I did. Using a regular expression, I wrap all links in the response in CDATA markers.CDATAfunction download_story_nitf($nitf_href) {
    $nitf_file = file_get_contents($nitf_href . ""&include=view_default&apikey="" . AP_API_KEY);
    $nitf_file = str_replace(array(""\n"", ""\r"", ""\t""), '', $nitf_file);

    $pattern = ""/<a (.*?)>(.*?)<\/a>/i"";
    $nitf_file = preg_replace($pattern, ""<![CDATA[<a $1>$2</a>]]>"", $nitf_file);
    
    $nitf_xml = simplexml_load_string($nitf_file);
    $nitf_json = json_encode($nitf_xml);

    return json_decode($nitf_json);
}function download_story_nitf($nitf_href) {
    $nitf_file = file_get_contents($nitf_href . ""&include=view_default&apikey="" . AP_API_KEY);
    $nitf_file = str_replace(array(""\n"", ""\r"", ""\t""), '', $nitf_file);

    $pattern = ""/<a (.*?)>(.*?)<\/a>/i"";
    $nitf_file = preg_replace($pattern, ""<![CDATA[<a $1>$2</a>]]>"", $nitf_file);
    
    $nitf_xml = simplexml_load_string($nitf_file);
    $nitf_json = json_encode($nitf_xml);

    return json_decode($nitf_json);
}I am not the world’s best coder, but this did the trick. Nothing else I tried worked. While this was for working with NITF XML, I assume this issue might crop up in other scenarios. So, this fix might work in your case too.",1338
DJI Mini 3 Pro Review,"I have always been fascinated by and loved drones but at a distance. Like many reading this, I resisted the temptation to spend $1k+ on a drone that I knew would crash into a tree or into a body of water where it is doomed to rest for eternity.With the DJI Mini 3 Pro, many of those valid concerns have been alleviated by releasing one of the most beginner-friendly drones, which also offers pro features. While this is not the first small-sized drone DJI has released, it is one of the best.A drone capable of taking high-resolution photos and high-quality videos and stopping you from crashing it into a tree or freefalling into the ground at top speed. While the Mini 3 Pro isn’t an enterprise-grade drone, it offers enough professional-like features.Specs-wise, the Mini 3 Pro boasts a respectable 1/1.3″ sensor and a fast F1.7 lens. It has three-direction obstacle avoidance and a gimbal capable of shooting landscape and social media-friendly portrait orientations. It can shoot 4K/60p video, 34 minutes of flight time and a plethora of other features you would expect to find in a much more expensive drone—all the while weighing in at 249g (with the battery fitted).Where the Mini 3 Pro heads into the beginner-friendly territory is obstacle avoidance. You can configure the behaviour to brake or avoid. I set it to brake. I tried my hardest to crash this drone in neutral mode and could get it to happen.; In sport mode, where obstacle avoidance is disabled, you’re on your own.The sub-250g weight is essential. In most countries, drones under 250g do not need to be registered (unless you’re using them professionally). The separate purchase of the fly more kit will take the weight over 250g, but the base drone you buy is 249g, meaning you can fly without needing to take an online drone course or pay for certification.The Mini 3 Pro being my first drone, was an experience when I first turned it on. Seeing it connect to GPS satellites, present a map with my location on it, and a view from the camera made me feel like I was operating a device that was a lot more expensive.I highly recommend the RC Controller. The display is nice and bright, but it’s incredibly responsive too. The RC Controller is running Android and can only run the DJI Fly app, from what I could tell.My hands were shaking on the first flight I took. It’s crazy that a drone resembling a children’s toy can make you feel this way. As you go higher and higher, anxiety and doubt kick in that it will crash or fall to the ground. Learning to trust the technology on something like this takes time.It took about 5 hours of flying to work up the confidence to go to the legally allowed 120 metres into the air. The one thing that surprised me was how visible the Mini 3 Pro is at obscene heights. You could still make it out on a clear sunny day.One of the first photos I took of my neighbourhood is below.defaultAlthough you shouldn’t be flying a drone at night, I am a terrible photographer. When it comes to nighttime performance, despite boasting of an F1.7 lens, you’ll still get a bit of noise. The photo and video quality is acceptable, I tried to take a photo of the moon in auto mode, and it didn’t work out too well.defaultIn the hands of a more capable photographer, you might get better results. The daytime performance is exceptional, even for a terrible photographer like me.Oh, and when I said this thing was almost uncrashable earlier, it’s true. Although, it can get confused with areas with a lot of tree coverage, and there are no sensors on the top. I found this out the hard way. On the second day of flying, after my confidence grew, I took off at the front of my house and crashed into the underside of the roof. The drone fell about 2m. I ended up needing to replace two of the blades.There are obstacle avoidance sensors on every part of the Mini 3 Pro except the top (I would love to know why). So, be aware that if you go flying straight up and it’s not clear, the drone won’t brake and continue upwards.Some users have reported issues with the range, but I haven’t even seen the signal bar drop by one bar in my use. I have also flown quite far with this in my area, with no signal issues whatsoever. I am sure DJI will improve signal performance over time with future updates.The rules from CASA state you’re not allowed to fly without a visual line of sight. But, we all know that drone users get curious and tend to break that rule once or twice. Speaking of rules, in Australia, you are only allowed to fly 120m into the air. Although, scarily enough, it appears that the Mini 3 Pro doesn’t impose a limitation on height, and it is possible to go higher than that.Now, the battery life. I know DJI included this particular battery because it keeps the weight under 250 grams. Still, I wish they included the extended batteries instead and made the lighter battery an optional addon that law conscious operators can buy.I wanted the Fly More Kit (which I have pre-ordered) that comes with the extended range batteries and extra blades, but strangely enough, it wasn’t available until after May 26, 2022. You get a good 34 minutes quoted battery life, but this will depend on your flying conditions. In semi calm weather, I saw closer to 27 minutes (which is pretty damn close).Overall, the Mini 3 Pro has been an excellent introduction to the world of drones. It’s not the cheapest, and there are existing DJI models that might also fit the bill. For me, the obstacle avoidance, range and quality of the photos and videos convinced me to make the purchase.",1393
How To Override WordPress Gutenberg Core Blocks Output,"WordPress ships with a bunch of neat core Gutenberg blocks. However, there may be situations where you need to change the output of a Gutenberg block. In my use case, I needed to modify the core/image block to add an image credit field I created using Advanced Custom Fields.core/imageLike most things in WordPress, this is possible using an action and a few lines of code. Some solutions around on the internet make you call register_block_type to override the registration of the core block (which is wrong, in my opinion).register_block_typeThe register_block_type_args filter is called from within class-wp-block-type.php, which allows you to override the render_callback property.register_block_type_argsclass-wp-block-type.phprender_callbackadd_filter( 'register_block_type_args', 'core_image_block_type_args', 10, 3 );
function core_image_block_type_args( $args, $name ) {
    if ( $name == 'core/image' ) {
        $args['render_callback'] = 'modify_core_image';
    }
    return $args;
}

function modify_core_image($attributes, $content) {
  	// Modify core image return content or something new here
	return $content;
}add_filter( 'register_block_type_args', 'core_image_block_type_args', 10, 3 );
function core_image_block_type_args( $args, $name ) {
    if ( $name == 'core/image' ) {
        $args['render_callback'] = 'modify_core_image';
    }
    return $args;
}

function modify_core_image($attributes, $content) {
  	// Modify core image return content or something new here
	return $content;
}Simply replace the name if statement value with the block you want to check for.",398
Speeding Up Slow WordPress CLI Commands,"When performing intensive or long-running operations on a WordPress website, the admin panel is terrible. Have you tried to delete 100 posts from the posts screen? It’ll time out and delete maybe  10-20 if you’re lucky. This is one example of many.Naturally, I opted for the WP CLI (WordPress CLI), which allows you to perform operations on your WordPress site from the terminal. I needed to delete custom post types from within a date range in my use case.At first, I ran the command, which looked like this:wp post delete $(wp post list --post_type='news' --posts_per_page=25000 --format=ids --meta_key=""story_promoted_from_ingest"" --meta_value=""0"" --start_date=2020-01-01 --end_date=2022-03-31) --forcewp post delete $(wp post list --post_type='news' --posts_per_page=25000 --format=ids --meta_key=""story_promoted_from_ingest"" --meta_value=""0"" --start_date=2020-01-01 --end_date=2022-03-31) --forceThis worked, but the performance was terrible. I was averaging maybe 20 posts per minute. It was taking forever to delete these. Not able to find a solution quickly, I deep-dived the WP CLI docs and found a heap of flags you can pass to it, most notably --skip-plugins and --skip-themes--skip-plugins--skip-themesWhat contributed to the slowness was some plugins triggering filters and actions as the posts were deleted.wp post delete $(wp post list --post_type='news' --posts_per_page=65000 --format=ids --meta_key=""story_promoted_from_ingest"" --meta_value=""0"" --start_date=2020-01-01 --end_date=2022-03-31) --force --skip-plugins --skip-themeswp post delete $(wp post list --post_type='news' --posts_per_page=65000 --format=ids --meta_key=""story_promoted_from_ingest"" --meta_value=""0"" --start_date=2020-01-01 --end_date=2022-03-31) --force --skip-plugins --skip-themesBy adding those two flags, it was a night and day difference. I was averaging close to 60 per minute (sometimes more). Skipping plugins and themes allows WP CLI to shed weight and run faster. Problem solved.",495
Neural DSP Reveal Details About the Long-Awaited Quad Cortex Desktop Editor,"Since the release of the Quad Cortex modeller, most people have been asking for a desktop editor for the Quad Cortex. While the interface of the Quad Cortex makes for seamless editing, a desktop editor can speed up the process.In Discord, Doug revealed some details about the highly-anticipated desktop editor.Here is what we know:Here is what we know:There has been a dedicated team working on the desktop editor for monthsThe desktop editor will require the Quad Cortex to be connected to use (like some other modellers)Doug is strictly against a single-file preset format like what the Helix and other modellers offerThere might be a future feature to save and load setlists into the Quad CortexThere has been a dedicated team working on the desktop editor for monthsThe desktop editor will require the Quad Cortex to be connected to use (like some other modellers)Doug is strictly against a single-file preset format like what the Helix and other modellers offerThere might be a future feature to save and load setlists into the Quad CortexAdmittedly, the lack of file format is disappointing. For such an expensive device that I am a huge fan of, the lack of open file format isn’t ideal and I hope they change it. It seems as though Neural DSP is taking an Apple-inspired approach to their hardware, locking people into using their cloud and future marketplace.",342
I am already bored of Battlefield 2042,"In the lead up to Battlefield 2042, I was genuinely excited, and it looked like a step in a new direction. Despite having more bugs than a cheap motel mattress, I even played the beta briefly and enjoyed it immensely.When launch day finally arrived, I jumped right in and invested quite a few hours into Battlefield 2042. The maps are big and beautiful, and they shock you with their sheer scale. But, eventually, the shine begins to wear off, and you’re left with a game that doesn’t have the same impact previous Battlefield games did, like Battlefield 4.I have concluded that the maps are far too big, even for 128 players. You feel like an ant floating in the solar system.I initially saw the lack of cover in the maps as a good thing; it means players can’t camp. However, the large open nature of the maps makes them feel big and empty; if you’re fortunate enough to get a transport vehicle, great. If you’re not, prepare to walk (even if you spawn near the action).Be prepared to do a lot of aimless wandering until you’re taken out by a hovercraft or attack plane flying by.As fun as the Portal mode is on Battlefield 2042, I have yet to find a custom created mode that I love enough to keep opening up the game and playing. Maybe I am just getting old, but Battlefield 2042 doesn’t do it for me. I can still happily play older Battlefield games, but 2042 feels like a chore and does not spark joy when I open it up.",356
Announcing Cortexpresets.com — Buy custom Quad Cortex Presets,"At present, the Cortex Cloud does not have a marketplace feature. There is no officially supported way to sell your preset creations for preset creators like myself. The process works for sharing private presets is you have to befriend someone, then they can share private captures and presets with you.I have a knack for creating clone presets based on famous artist sounds. Think bands like Monuments or Linkin Park, the kind of tones people want but might lack the knowledge to create themselves. So, I decided to build something called cortexpresets.com.cortexpresets.comYou choose the Quad Cortex presets you want, and then you get your purchases shared with you. It’s a hokey pokey process at present, but it’s the only way. All you do is befriend my account on Cortex Cloud, provide your Cortex Cloud username, and share the presets with you.While I will be selling presets, I will also continue to create free presets. When the official marketplace is launched, people will be migrated over to the new marketplace.If you are looking for paid Quad Cortex presets, check the site out: cortexpresets.comcortexpresets.com",281
Is Web 3.0 Just Another Crypto Hype Scam?,"The term Web 3.0 is being thrown around a lot, not just those in the crypto sphere, but investors and everyday folk are starting to talk about it. You know, when your Uber driver or barber is talking about Web 3.0, it has permeated the fabric of society.People have differing opinions on Web 3.0, what it will do for the web, how it will change everything. There seems to be an aura of excitement around Web 3.0, with a hint of ambiguity.According to Elon Musk, Web 3.0 sounds like bullshit.
Web3 sounds like bs— Elon Musk (@elonmusk) December 2, 2021
Web3 sounds like bs— Elon Musk (@elonmusk) December 2, 2021Web3 sounds like bsDecember 2, 2021Given the hype surrounding Web 3.0 and the turbulence of cryptocurrencies and blockchain, you can understand Elon’s perspective. We are talking about a guy who became a God to many in crypto because of his half-serious support of Dogecoin. If you want to see the fanaticism in full effect, read some of the replies Elon Musk’ Tweets get. You’ll cringe yourself a stomach ulcer.What is Web 3.0?You have heard the term thrown around, but what does it mean? Web 3.0 refers primarily to decentralisation. Many see it as the next evolution of the internet, decentralised web applications not controlled by big tech companies like Facebook, Google or Twitter. An online utopia where nobody owns your data, where you remain in control.It’s a noble goal; decentralisation is good in all facets of life, not just the internet. A handful of companies shouldn’t have all of the power. Many countries have antitrust laws, but they seemingly don’t do much these days.With web 3.0, users are in control of their data again. Everything is transparent on the blockchain, with no bad big-tech influence or agenda. It all sounds good on paper, and venture capitalists agree. An estimated $17 billion was poured into Web 3.0 companies in the first half of 2021.$17 billion Echoes of 2017 IPO’sIn 2017, it was the year of cryptocurrency IPO’s. Anyone with a slapped together whitepaper, made up advisory team, and a good mascot/leader who was the project’s face could make tens of millions running these fundraising rounds.As you might be aware, many of the projects that ran IPO’s in 2017 are not around anymore. And, the ones that are, they didn’t offer the returns some were expecting. Promises didn’t become a reality for a lot of crypto projects.It feels like a minority of people in the blockchain space care about web 3.0, and the majority are in it to see the value of their favourite shitcoin go up 1000%. And before you start accusing me of sounding bitter because I missed out, I’ve been in crypto since 2016 (I was one of the early investors in Monaco Card, now known as Crypto.com). I currently hold BNB, ADA and BTC (mostly). I have small amounts of other coins that I leave sitting there in my Binance account. I’m not a professional investor; most of my success in crypto has been luck.The thing that makes me err on the side of caution is the inescapable hype. Like anything good in blockchain, the “moon bois” hype things up and blow them out of proportion, and it’s only when the market begins to slide backwards that you see who the true believers are.Knowing that VC’s are swarming to Web 3.0 as they did with NFT’s, I’m cautiously optimistic. I want to see a decentralised web, but I am dubious that it’ll ever happen on a large enough scale to have a meaningful impact beyond making some Ethereum and other obscure blockchain hodlers richer.Before we dream of a future where blockchains power web applications and our daily lives, maybe we should get the systematic security issues sorted out first? Many Defi projects have experienced attacks due to poorly programmed smart contracts or flawed governance models.A recent attack on a platform aiming to bridge blockchains was hacked, and with it, $600 million approximately in cryptocurrency was stolen. A few days ago, BadgerDAO was hacked for $120 million. I can imagine the first famous wave of Web 3.0 applications will be full of flaws. The blockchain might be secure, but if your smart contracts are crap, it doesn’t matter how secure your blockchain is.$600 million approximately in cryptocurrency was stolenBadgerDAO was hacked for $120 millionWhen Web 3.0 solutions become more mainstream, they will not be 1:1 replacements. Any Web 3.0 solution will be more of integration within existing sites and applications. Think of logging into a site like Twitter with a blockchain identity instead of an email address, that kind of thing. Even so, thanks to browser extensions like Metamask, that is already possible.Barriers to adoptionBut then you have the most significant barrier to adoption: fees. This is the part of Web 3.0 I can’t work out; who pays the fees? I think we know that the only blockchain around right now that could handle the next Twitter or Facebook is Ethereum, where the gas fees are exorbitant.If these alleged Facebook and Twitter replacements are blockchain-based, users will not spend money posting a status update on their microblog or uploading an image to blockchain Instagram. Imagine being charged a fee to post something and having to wait a couple of minutes for whatever you post to appear?Once you solve the fees problem, you have to solve the onboarding problem next. Onboarding into most blockchain-based applications is like pulling your teeth out with a pair of pliers. Strangely convoluted confirmation screens, remembering long seed phrases, confirmation times.There are solutions to some of these problems, but for other problems such as the fluctuating and sometimes exorbitant gas fees, not so much.",1412
Cache Busting WordPress Enqueued Styles and Scripts,"In WordPress, you add theme styles and scripts using the trusty wp_enqueue_script and wp_enqueue_style methods. However, by default, your enqueued scripts and styles will be added to your WordPress site as straight scripts.What happens (and the reason you’re probably here) is the browser will cache your scripts, which is what we want to happen.There is nothing more frustrating than making a change, only for the client or QA team to say they can’t see the changes, for you to go back then and tell them to do a “hard refresh.”What we want to happen is for the version number to change on our scripts and styles, ONLY when they change. We want the browser to cache our files because it takes the load off of the server (or CDN).To determine when a file has been updated, we can use the PHP method filemtime, which will return the UNIX timestamp of when a file was last updated.filemtimeFor the wp_enqueue_script and wp_enqueue_style methods, the fourth argument is the version number which is where we will use the filemtime function to auto-generate a version number for us.wp_enqueue_script('theme-js', 
                  get_stylesheet_directory_uri() . '/assets/js/theme.js', 
                  array(), 
                  filemtime( get_stylesheet_directory() . '/assets/js/theme.js'));wp_enqueue_script('theme-js', 
                  get_stylesheet_directory_uri() . '/assets/js/theme.js', 
                  array(), 
                  filemtime( get_stylesheet_directory() . '/assets/js/theme.js'));In this example, we require a file called theme.js from assets/js. Whenever this file changes, the filemtime value will change as well.For styles, it looks pretty much the same:wp_enqueue_style ('theme-css', 
                  get_stylesheet_directory_uri() . '/style.css', 
                  array(), 
                  filemtime( get_stylesheet_directory() . '/style.css'));wp_enqueue_style ('theme-css', 
                  get_stylesheet_directory_uri() . '/style.css', 
                  array(), 
                  filemtime( get_stylesheet_directory() . '/style.css'));With these simple changes, WordPress will cache bust our files when they change and the browser will cache accordingly. No more file cache mismatch issues!",560
Building a High Availability WordPress Website Using AWS and Hyperdb,"WordPress is the most popular CMS on the planet, powering over 40% of the internet and continually growing. Despite what you have been told, WordPress isn’t dying, and it’s not the terrible mess some dramatic PHP hating developers make it out to be.powering over 40% of the internetI was tasked with building a WordPress capable of sustaining millions of visitors per minute (a project I can’t reveal the details of just yet). The brief was a platform that could handle upwards of 100,000 visitors per second. Keep in mind this is across hundreds of WordPress Multisite domains, not one single site.With some simple configuration, you can create a WordPress site (or Multisite) that is capable of handling millions of visitors (think Black Friday sales as a huge traffic spike).In Beanstalk, I have a PHP-based load-balanced application that operates a minimum of 10 t4g.large EC2 instances and a dedicated instance that ingests content from third-party API’s and writes it to the database.For my load balancer scaling triggers, I am using the CPUUtilization metric, using an average statistic. When the health of my instances drops, I have set it to scale up to three instances at a time (also t4g.large). I might change this based on network traffic, but I find CPU Utilisation works best as my php-fpm processes consume a lot of CPU under load.Using Amazon Elastic Beanstalk and some auto-scaling will get you part of the way, but you will encounter database bottlenecks at this scale; this is where MySQL replication is your friend. Amazon Aurora in AWS RDS is your friend, creating database clusters with a write and read instance, allowing you to create multiple reader instances, which you can then split the traffic across.For years, the plugin appeared to be neglected, but what needs to be updated? Fortunately, Automattic (the company behind WordPress) offers a free plugin called Hyperdb, which allows you to split your read and write operations across multiple database instances. It works well and was recently updated.HyperdbThe downside of Hyperdb is that the documentation sucks.the documentation sucksthe documentation sucksIn my scenario, the primary database is for write operations, where authored content, pages, comments, and other write-oriented data will hit. Because multiple sites are using WordPress Multisite, the write database must be only available for write data.In my db-config.php file this is my primary database configuration:db-config.php$wpdb->add_database(array(
	'host'     => DB_HOST,
	'user'     => DB_USER,
	'password' => DB_PASSWORD,
	'name'     => DB_NAME,
	'write'    => 1,
	'read'     => is_admin() ? 1 : 0,
	'dataset'  => 'global',
));$wpdb->add_database(array(
	'host'     => DB_HOST,
	'user'     => DB_USER,
	'password' => DB_PASSWORD,
	'name'     => DB_NAME,
	'write'    => 1,
	'read'     => is_admin() ? 1 : 0,
	'dataset'  => 'global',
));One nifty little trick I picked up from somewhere (I think it was a GitHub Gist) is making the primary database only for write operations and allowing it to be used for read operations if we are in the administration panel using the is_admin function.I then have two reader instances currently, both of which are just copies of the main database.$wpdb->add_database(array(
	'host'     => DB_HOST,
	'user'     => DB_USER,
	'password' => DB_PASSWORD,
	'name'     => DB_NAME,
	'write'    => 1,
	'read'     => is_admin() ? 1 : 0,
	'dataset'  => 'global',
));

$wpdb->add_database(array(
	'host'     => 'reader-1.rds.amazonaws.com',
	'user'     => DB_USER,
	'password' => DB_PASSWORD,
	'name'     => DB_NAME,
	'write'    => 0,
	'read'     => 1,
	'dataset'  => 'global'
));

$wpdb->add_database(array(
	'host'     => 'reader-2.rds.amazonaws.com',     // If port is other than 3306, use host:port.
	'user'     => DB_USER,
	'password' => DB_PASSWORD,
	'name'     => DB_NAME,
	'write'    => 0,
	'read'     => 1,
	'dataset'  => 'global'
));$wpdb->add_database(array(
	'host'     => DB_HOST,
	'user'     => DB_USER,
	'password' => DB_PASSWORD,
	'name'     => DB_NAME,
	'write'    => 1,
	'read'     => is_admin() ? 1 : 0,
	'dataset'  => 'global',
));

$wpdb->add_database(array(
	'host'     => 'reader-1.rds.amazonaws.com',
	'user'     => DB_USER,
	'password' => DB_PASSWORD,
	'name'     => DB_NAME,
	'write'    => 0,
	'read'     => 1,
	'dataset'  => 'global'
));

$wpdb->add_database(array(
	'host'     => 'reader-2.rds.amazonaws.com',     // If port is other than 3306, use host:port.
	'user'     => DB_USER,
	'password' => DB_PASSWORD,
	'name'     => DB_NAME,
	'write'    => 0,
	'read'     => 1,
	'dataset'  => 'global'
));HyperDB will then spread out the reads to these reader instances. The primary instance is reserved for write operations (which are more expensive); this affords us an admin panel that is highly available and insulated somewhat from the front-end as we can rely on it for reading operations, even if the other two instances go down.I have this on my to-do list, but the plan is to eventually shard out some parts of the database as it grows in size. As more sites are added to the platform, database size will be a concern, and sharding will be needed, which Hyperdb also quickly provides for us.",1300
How to Enable HTTPS for Amazon Elastic Beanstalk,"Have you ever been stuck on a problem that makes you feel so stupid, you get a serious case of impostor syndrome? Welcome to another instalment of Amazon Beanstalk bad UX.You go to the listeners section in the load balancer configuration section, you get to this popup:You choose HTTPS, you enter port 443 and you hit add. You see “pending create” in the list and assume that you’ve just configured your load balancer to support HTTPS. All is well.You attempt to visit the https version of your site and nothing works. You reload the configuration screen and the https listener you just added is nowhere to be seen. Maybe you repeat this a few times before you start to feel really stupid. Welcome to my life.You are not stupid. You are doing everything right. It’s Amazon and their terrible UX who are to blame. In fact, it turns out this horrible career questioning UI has been around for years.After you hit the add button, you haven’t added anything until you hit the apply button on the configure screen. Click “add” and then scroll to the bottom to the following buttons:Click the “Apply” button and you’ll add the https listener for real this time. Seriously, Amazon. Fix this terrible UI already.",301
Which Neural DSP Archetype Plugins Should You Buy?,"Neural DSP are hosting one heck of a Black Friday (like years prior) where all of their plugins are 50% off. There is a lot of overlap between their Archetype plugins especially, so what should you buy if you can’t afford them all?TL;DR buy the Gojira, Henson and Fortin Cali plugins (if you can afford all of them). Can only afford one? The Fortin Cali is a solid purchase.Archetype: GojiraIf you play metal, this is the plugin for you. For such a long time, the Archetype: Nolly reigned supreme (particularly the Blood Eagle preset), but times have changed and now Gojira is the plugin you want.

You can get some absolutely filthy sounds from the Gojira plugin, but clean it up when you want something lighter. The octaver is one of my favourite pedals in this plugin.I used to use the Fortin Nameless plugin, but I found that it required a lot of additional EQ to get it to sound right. The Gojira just sounds so much cleaner and can be used without needing EQ.Archetype: Tim HensonOne of the newest Archetype plugins, and by far one of the best. You can get some really crisp clean tones with this plugin, or make them a little gritty with some crunch. The multi-voicer alone makes this a worthy purchase.The Henson plugin is great for acoustic-style tones and clean break stuff. It’s not overly flexible in that you can’t create rib tingling chug riffs or djent with it either.

Where things get confusing for people is what is the difference between the Plini and Henson plugin? The Henson plugin can do a lot of what the Plini offers, where they differ is the Plini plugin allows you to create heavier and more lead type tones as well as cleans.Sadly, the Plini has been eclipsed by Henson. It’s not what the Plini plugin is terrible, because it can still give you incredible soaring leads and Tesseract djent style delayed reverb cleans, but the Henson offers that and more. The Henson does a way better job at cleans.Archetype: Fortin CaliSecond only to the Archetype: Henson, the Fortin Cali Archetype is a close second favourite. It can do somewhat clean tones (not as clean as Henson), it can do rock, punk, metal and a bunch of different genres.

If your budget only allows for buying one Archetype plugin, I would recommend the Fortin Cali, especially with its recent 2.0 update which really improves the plugin greatly.I know some people are of the opinion that Archetype: Nolly is the do-it-all plugin that you should buy, but I’ve found the Fortin Cali is slightly more of an all-rounder, not overly tilted towards metal like the Nolly plugin is. And once again, Fortin Cali sounds great without needing EQ.",656
Seamless Preset Switching On the Quad Cortex: It’s Complicated,"So, you bought the Quad Cortex. A floor modeller marketed as the most powerful modeller on the planet, but you’ve noticed the delay when switching between presets.You might not know this, but every competing modeller from the Line 6 Helix through to the Axe-FX suffers from delay when switching presets. The current blocks need to be unloaded, the new ones loaded.The solution is to use scenes. Think of presets and scenes like this.Preset = songScene = song partPreset = songScene = song partAlthough, you can also leverage scenes to build multi-faceted presets where scenes are used as faux presets. On Cortex Cloud, there are some clever all-in-one presets that take this approach which work well.In most situations, it’s rare you would ever have a need to change presets mid song. The delay, while not huge, would be noticeable. Changes mid performance would definitely be better suited to scenes.",225
Where to Buy a Stand for Your Neural DSP Quad Cortex,"The Quad Cortex is great for live use as well as studio settings. If you’re like me, you have the Quad Cortex sitting on your desk, and you interact with it using the screen.The device isn’t angled, so it can be tricky to adjust things on the screen or get to the inputs and outputs on the back.Fortunately, the Quad Cortex is similar in size to an Apple Macbook. So, naturally, the solution for a Quad Cortex stand is a laptop stand, specifically, a stand that can be angled and has a flat bottom.There is a lot to choose from, but, this is the one I opted for and it’s just the right size. I got this exact laptop stand from Amazon here.I got this exact laptop stand from Amazon herePardon the smudgesAs you can see the stand is the right size for the Quad Cortex. It doesn’t stick out on the back and it perfectly holds it into place.The stand without anything on itThe only regret that I have buying this stand is getting the black colour. It doesn’t look terrible, but I should have bought this silver one instead. I think it would have matched the colour of the Quad Cortex a lot better.this silver one instead",279
"Battlefield 2042 PC Review: Same, but different","The Battlefield game series is arguably one of the best series that has always prided itself on realism, but at times never been afraid to step outside of the bounds to give fans something different. In Battlefield 2042, we see the series take another step forward, giving Battlefield gaming fans the things they love about the previous entries and some new things as well.Since the introduction of the Frostbite 3 engine, Battlefield games have been getting a tad more realistic. The levolution mechanic introduced in Battlefield 4 saw maps no longer being static. You could affect them by blowing up buildings, destroying walls and pipelines. Actions that had map-wide consequences.levolution mechanicIn Battlefield 2042, much of what players familiar with Battlefield 4, 5 and 1 will be accustomed to is still there. It’s just a lot bigger. Maps can now have up to 128 players, which result in absolute carnage and chaos. The increased map size means you don’t feel claustrophobic, and the increased player count makes games way more fun.One of the most significant changes in Battlefield 2042 is that it is now an online-only game. Most of you are reading this probably never finished the Battlefield campaigns anyway and instead opted to play online, ranking up and causing mayhem. Battlefield 2042 is set in some bleak futuristic world where climate change has wreaked havoc and left two opposing parties to duke it out; the USA and Russia.If you played the beta, you might have had concerns that the game was anything but ready for release. The game has had some delays so it can be polished, and in their favour, they did take beta feedback on board and address it.One of the most controversial changes introduced is the class system from previous games is gone. You no longer choose to be a medic or recon soldier anymore. There are ten specialists to choose from (presumably more to come in the future). Each specialist has a unique ability, like a grappling hook or a cool wingsuit.I like the new specialist system. It adds a newfound sense of freedom to the game. Classes had their share of fun, but you were always limited in what you could do. You have active and passive abilities now. You can even hack vehicles and other never before seen things in a Battlefield game.If you want to play medic or sniper, you still can. There is a recon specialist, but anyone can choose to equip a sniper rifle and pick people off from hidden parts of the map (if you can find those).Another change to get used to is despite the maps being quite large, they are pretty open. Many of the maps lend themselves to vehicular combat (which has always been my favourite thing in Battlefield games) but means those on foot are easy targets (especially if you get an unlucky spawn).Balance wise, the game seems to be pretty balanced. There are some minor grievances like Hovercraft driving up walls (I’m sure that will be fixed soon). I have yet to encounter any guns that seem overpowered, but I still have limited access because I am low ranked.Speaking of balance. Unlike Battlefield 4, there aren’t any overpowered vehicles. I’m sure some of you might remember that attack helicopters in Battlefield 4 were incredibly annoying because they were almost impossible to destroy. A good helicopter pilot in Battlefield 4 could run a large kill count without a single death.I did encounter my fair share of good pilots in Battlefield 2042, but they were a lot easier to flank.The maps can feel sprawling and isolating at timesI first noticed that maps are so big and sprawling (like Hourglass) that you can be the only person around and away from the action.One of the problems with the open nature of the maps is that vehicles are quite limited. Everyone rushes for the attack planes and tanks, meaning you’ll have to hitch a ride with someone else or walk. I tried walking in a game for fun, and it was painful and tedious… until a tank took me out.The spawn points also appear to be a bit tricky, like in previous Battlefield games. Despite the maps being larger, it’s still easy to accidentally spawn into the wrong part of the map and find yourself in a 30 man gunfight (like the Stadium in Hourglass).I was waiting to be revived, only for an enemy tank to stop right before my dying corpse and then proceed to drive over me.And then there is the concept of reviving. If you’re fortunate enough to be in a busy part of the map, you might be revived. In most cases, nobody was around to revive me. It seems the large size of the maps has resulted in the revive feature being almost redundant. Not completely useless, but in most cases, you’re just waiting for the timer to run down.Being a Sniper in a lot of the maps can also be tricky. Because everything is so open, finding a good hiding spot can be rather difficult. My favourite map Hourglass has a few good spots you can hide, but there aren’t many.Visually pleasingBattlefield 2042 despite looking bleak a lot of the time, is beautiful in its own way. The reflections, the buildings and their ability to convey scale are incredible.The skyscrapers accurately convey a sense of scale, like you’re in a desert city.Look how beautiful this game is.The way the light reflects off the trees and how they move in the wind conveys serious realism.The minor things that make this game pleasing to the eye make such a huge deal. The way in which the trees react to the weather and sway, the lighting.Those blue irrigation systems move along tracks and you get a real sense you’re on a farm on this map.The weather and visuals aside, the other things like map actions also add to the realism. In the above screenshot, those blue irrigators on tracks move up and down watering the crops like you’re on a real farm.There are performance issuesThe game is surprisingly a lot more optimised than it was during the beta. Performance is generally good across the board, except in some cases. I know this can be hardware dependent, but most performance cases were witnessed server-side by other players.I’m rocking an AMD Ryzen 3900x with an Nvidia 3070 and 64GB of RAM, playing on a 1TB NVME drive. Not the best, but not exactly entry-level hardware either.The addition of weather events into maps, despite looking cool, does result in frame rate drops. I can go from a stable 50fps down to 38fps with a weather event. The rolling sandstorms regularly wreak performance havoc, which I am sure will be fixed in a subsequent patch.Depending on the number of players in the game, the initial spawn can be laggy. Spawning on a squad member in a busy area can also result in frame rate drops.For the most part, the game was playable and fun. Nothing stopped me from playing; it was really a minor inconvenience.Hazard ModeWithout a doubt, one of my favourite modes in Battlefield 2042 is Hazard Mode. It’s a squad-focused mode that sees squads of four trying to secure data drives and extract them. It’s a combination of Battle Royale and Escape From Tarkov, which is new territory for a Battlefield game.I know some people will have strong opinions about Hazard, but I really enjoyed the different approach to a Battlefield game mode. It doesn’t go full Escape From Tarkov, but it has a progression system where you level up with the currency to buy new weapons and rinse repeat.The one thing that lets the Hazard mode down a little bit is the AI players. There are a lot of AI players in this game, and they don’t seem to be that smart or consistent. Sometimes they’ll be highly annoying and precise. Other times they’ll shoot at you and miss you a lot. I wish they made the AI have a purpose; they’re just another environmental hazard like the weather, but killing them gives you nothing.And not to dampen the fun, but there is a balance issue with Hazard. Because the currency and progression are based on first-mover advantage, it means new players with limited load-outs will instantly be at a disadvantage over someone who has been playing since launch and amassed a tonne of credits.If they care enough, these issues can be resolved in subsequent updates and maybe expand this mode to have more consequences when you die. Make death mean something.Battlefield PortalPerhaps one of the most significant additions to a Battlefield game by far is Battlefield Portal. The Battlefield Portal feature is a sandbox mode allowing you to create custom game modes and unique maps. It offers the new stuff from 2042 and modernised maps, vehicles, and weapons from existing Battlefield games.The portal editor is publicly accessible, allowing anyone to build experiences even if they don’t own the game yet. As you can see, it has a cool step based editing processing and even offers a logic editor allowing you to run subroutines, affect the game in different ways and create unique rules.While you can’t create maps from scratch, you can mod existing maps and make them into unique experiences. And to answer the question, everyone will be asking: no, you cannot build battle royale game modes just yet. Think of Battlefield Portal as a modding tool, not a creation tool.The Portal feature has the potential for the community to increase the longevity of 2042 and keep it fun. Previous Battlefield games, after playing quite intensively, begin to get a little repetitive and boring. I can foresee some creative additions from the community after launch.ConclusionI think Battlefield 2042 is a nice fresh new direction for Battlefield. Not everyone is going to be a fan of 2042, but if DICE can take what they have here and build on it, the portal feature alone could result in a game you play for more than a few months until you get tired of it.",2419
How to Use Neural DSP Archetype Plugins With the Quad Cortex,"The Quad Cortext by Neural DSP is an incredible floor modelling device with more inputs and outputs than a swiss army knife. One of the reasons you bought the Quad Cortext might have been the potential and small form factor, as well as the promised ability to run the Neural DSP Archetype VST plugins natively.Sadly, the Quad Cortex does not allow you to run your plugins on the device itself (yet), but Neural DSP is working on the functionality, and it will eventually come. Until it does, how can you use the plugins with your QC?There are two approaches you can take as a temporary stopgap.Reamping DIThe Quad Cortex is a modeller as well as an audio interface. You can send the processed signal into your computer and the direct input (DI), which is unprocessed and the direct signal from your guitar.Using a DAW like Reaper, you can create a track that uses the DI from the Quad Cortext. You then apply your Archetype plugin onto the track. It’s not the same as a native plugin on the Cortex itself, but it’s the same result.Capture the Archetype plugin(s)The flagship feature of the Quad Cortex is the ability to capture amps, pedals and a little known feature: capture VST plugins.You can dial in your favourite Archetype sound, whether it’s Plini or Gojira, then capture the preset by routing the plugin through to your Quad Cortex and capturing it.Admittedly, capturing is a lot more work, but the result will give you a sound capture that gives you that Archetype sound and without reamping or anything else.",380
How to Record With the Neural DSP Quad Cortex in Reaper (DI and USB Recording),"The Neural DSP Quad Cortex offers some pretty impressive input and output options. With it, you can record the DI signal (the guitar input without any processing) and the processed amp tracks inside a DAW like Reaper.It is worth noting that I am on Windows, so for Mac, this might be a little different. For Windows, you also need the official Quad Cortex Asio drivers from here. For macOS, no driver is required.from hereNow, inside your Reaper device preferences, make sure you have ASIO chosen as your audio system. Then, for the driver, you want the NeuralDSP USB Audio Device (pictured). Where I first went wrong was being confused by the input range.By default, your input range will be Input 1 and Input 2. These coincide with the guitar/instrument inputs on your NDSP. If you attempt to record from these, you’ll notice you can only hear the raw DI signal, not the modelled one.The first input can remain as Input 1, but for the last (because it works on a range), we want to choose USB Output 4. Our left and right modelled output will be USB Output 3 and USB Output 4. We keep Input 1 because it’s good practice to have both the dry signal and wet (modelled signal). You can reamp your DI tracks, so it’s great to have them around.Insert a new track into reaper and choose “Mono > Input 1” this is your DI signal we can reamp (if needed). Insert another new track and select the USB inputs which are our NDSP USB outputs.You should now have two tracks in Reaper. The first track is the DI and the second is the stereo track with your amps and effects.Going even further, if you do create a DI track, you can specify inside of your Quad Cortex to accept the output from that track back into the device, allowing you to reamp that dry guitar input with a new preset or model without needing to record the guitar again.",457
How to Fix Access Denied MySQL Error on AWS RDS,"If you’re getting the error `Access denied; you need (at least one of) the SUPER, SYSTEM_VARIABLES_ADMIN or SESSION_VARIABLES_ADMIN privilege(s) for this operation` while trying to import a database dump you have done using mysqldump, you might have encountered this issue.In my situation, I am using Amazon Lightsail and migrating a database to a newer database instance. I ran a mysqldump and then attempted to import the dump into the new database.You can fix the error by modifying the database dump file (.sql) which is being caused by SESSION.SQL_LOG_BIN and GLOBAL.GTID_PURGED lines. Or, you can use the force flag -f after the MySQL command. mysql -f -u dbname -p -h hostvalue -D databasename < dbdump.sql mysql -f -u dbname -p -h hostvalue -D databasename < dbdump.sqlYou will still get errors when you run the import, but it won’t prevent it from happening. Without -f, any errors will cause the MySQL command to fail. This flag tells it to ignore the errors and keep going.",246
A review of the Neural DSP Quad Cortex: is this the future of amp-modelling?,"I made the switch to amp modelling years ago. For such a long time, I had been an avid user of physical amp modellers. A few years ago, software amp modelling also started to catch up.It’s a head trip to think that there are up and coming guitarists out there who have probably never owned a physical amplifier and been purely modelling—never knowing the pain of connecting your pedals, identifying a bad cable in your signal chain and working out how to not only power everything but neatly run the cables.When the Neural DSP Quad Cortex was announced, my curiosity was piqued. Could this little device go toe-to-toe with the big boys in the amp modelling world? Could a Quad Cortex compete against an Axe-FX II or Axe-FX III? How about against a Kemper or Line 6 Helix?It was this hesitation that made me sit back and not make such an impulsive purchase. That hesitation was a blessing because it allowed me to honestly think about whether or not I needed the Neural DSP Quad Cortex or if I was making an impulse purchase.Stock levels for the Quad Cortex have been a bit of an issue. But, when a retailer here in Australia started advertising they had a shipment coming in at the end of October 2021, I jumped on it. I reached into the deepest darkest parts of my pockets and found enough to buy one of these beasty units.As you are probably aware, the Quad Cortex is not cheap. It’s in short supply too, so used prices are sometimes more than RRP for the moment.Should I buy the Neural DSP Quad Cortex, or wait for a bit?Should you buy the Quad Cortex? This is probably the question you came here to get an answer to. The answer is; it depends. I recommend that you keep reading to understand the Quad Cortex better, but there are some downsides (easily fixed in future updates) you need to be aware of before making such a large purchase.One thing you have to realise is you are buying into an early ecosystem. The Quad Cortex is essentially a computer with a powerful specialised chip for audio. It’s one of the most powerful modellers around at the moment (I’m sure competitors will catch up).The amplifier and effects selection pales compared to the Line 6 Helix (or even my old HD500x), Kemper or Axe-Fx III. The initial experience is a little thin, but given this thing only started shipping in March 2021 and the pandemic supply chain issues have made electronic component availability and shipping difficult, it’s early days.There seem to be more people joining the app each day, and thanks to its custom capture feature, captures are being regularly pushed to the cloud you can access, which effectively give you new amps and effects as you can capture both. The capture feature is the flagship feature of this thing, and we will get into that later on.The Line 6 Helix is going on seven years now, and despite its age, it’s still a solid modeller being updated and improved. There will undoubtedly be a Helix 2 coming soon, most likely going toe to toe with Neural DSP.If amp modellers were judged solely on amps and fx selection, the Quad Cortex right now would not win. The strength of the Cortex is the quality of the sound and the ability to capture amplifiers, pedals and even VST plugins.Build QualityThe Quad Cortex build is impressive. A complete metal chassis, a seven-inch colour touchscreen, switches that are both rotary and stomp. A 2GHz SHARC+ processor lives inside, giving you the ability to create sounds so wild and crazy, you’ll never experience a DSP limit ever again (something that happened to me a lot with the POD).Those foot stomp rotary switches are a gamechanger, in my opinion. I have no idea why no other device has thought of doing this, but it does make the QC feel like more of a hardware product than a software one.Time will tell if the rotary stomp switches hold up in the long-term, but they did allegedly test these with some robot that just hit them repeatedly until they broke, and they last for weeks (allegedly).What will pleasantly surprise you when you first get your hands on the QC is how small and light it is. Looks can be deceiving, it might be small, but it’s mighty. You could throw this thing down a flight of stairs, and it might get some scratches and dings, but I doubt it would break (I don’t want to find out).Could you tour with the Quad Cortex? Absolutely. Mike Stringer from Spiritbox has switched over to using the QC on tour, and this guy is known for previously using Axe-FX and Kemper.It’s pretty impressive that they’ve managed to make a device suited for the home studio but also rugged enough to tour with it. Usually, tour proof equipment isn’t known for being the prettiest. I recommend buying the carry case for the Quad Cortex (sold separately), which will protect your QC if you’re travelling with it.The touch screen is one of the most impressive things about the Quad Cortex. It’s highly responsive, similar in response to a phone or tablet display. I was sceptical, but it’s remarkable how quickly you get used to the touchscreen and rely on it. You soon forget you can’t create presets outside of the device yet.Sound QualityBefore the Quad Cortex became a reality, Neural DSP was already well-known for its fantastic VST plugins. The Archetype plugins are some of the most realistic and impressive modeller VST plugins around. I’ve been a massive fan of the Nolly plugin since it was released and then the Gojira one too.I was pleasantly surprised how good the Quad Cortex sounded out of the box. I mean, I turned it on and started going through the stock presets, and I was blown away by how good they sounded for stock presets. My point of reference for out of the box sound was the Archetype plugins.And then, I created my own preset. I created a preset to replicate the bluesy overdriven sound of some of Thrice’s earlier work (particularly the Beggars album). It was pretty easy to create a preset that combined two amps with some pedals.I am not an expert at creating patches yet, but Neural DSP has managed to get some of their artist ambassadors to create some captures and presets available on the cloud. The Rabea presets are incredible if you want some more rock-oriented sounds.

If you are looking for some metal rhythm and lead presets, Baris Benice has you covered with a selection of some punchy and unique metal presets.

Someone who is using the Quad Cortex to full effect is Mike Stringer from the band Spiritbox. He has switched over to the Quad Cortex for his live setup right now. And even better, he has presets and captures available on his profile. Just search for “mikespiritbox” — Mike uses a capture of an Omega Granophyre, and it’s one of my favourite captures/presets around at the moment.Inputs and outputsNeural DSP has really thought far ahead with the sheer number of inputs and outputs on offer. Furthermore, the configuration settings allow for all kinds of different combinations. Have outputs on different parts of your signal chain going into different tracks in your editor.You can seriously control everything. There is even support for 48v phantom power if you wanted to plug in a microphone and power it from the device. You can use the Quad Cortex as a powerful audio interface and modeller. It does everything.Video used from the Neural DSP website: https://neuraldsp.com/quad-cortexVideo used from the Neural DSP website: https://neuraldsp.com/quad-cortexOne of my favourite features of the Neural DSP is that you can run multiple instruments into it. Think of a scenario where you and another guitarist are doing a live show. Instead of lugging amp heads and speaker cabinets, you can both run straight into the Quad Cortex and have different amps/speakers/fx and tweak other facets of your sound individually. Effectively, giving you two amps in one.If you like to play to your favourite tunes on Spotify or play along with YouTube videos, you can use this as your audio output on your computer. Plug some headphones in and you can play along, with the ability to adjust the volume levels of the mix.You can even go a step further and become a one-man band, a guitar and a microphone. I wonder if you can run an electronic drum kit into this thing? User InterfaceThe user interface of the Quad Cortex takes some heavy inspiration from the Line 6 Helix. As the saying goes, good artists copy, and great artists steal. Line 6 are the Apple of the amp modelling world. They make beautiful devices and have been trendsetters in that regard.On the screen, colours are vibrant, and fonts are easy to read. Because of the decent screen size, the text is nice and large. In a dark room, the screen pops.The editing experience is currently done on the device with no PC or Mac editor available (although it is being worked on). As you can see in the following video, it’s pretty easy to create presets on the device:

Neural DSP is still expanding the items on selection. By tapping on parts of the UI, you can place blocks that can be amplifiers/cabinets/pedals/noise suppressors and pretty much anything else. The rotary stomp switches allow you to change the settings of your blocks, replicating the feel of traditional dials on an amp or pedal. Video used from the Neural DSP website: https://neuraldsp.com/quad-cortex  Video used from the Neural DSP website: https://neuraldsp.com/quad-cortexCapturingThe flagship feature of the Quad Cortex that sets it apart from the likes of the Line 6 Helix is the capture feature. You can use this to capture amp heads, combo amplifiers, effects pedals and speaker cabs. The capture process couldn’t be more straightforward. The device guides you through the process and shows you what to do.I don’t have much in the way of analog equipment laying around anymore, but I did capture my Positive Grid Spark Amp I had as a test. I was blown away by the accuracy. I couldn’t hear any discernable difference. I thought the Quad Cortex capture sounded better to my ears.I am probably going to start bugging my friends and family to borrow their equipment to capture it. I am even considering renting some equipment from music stores (the kind I could never afford to buy) and then capturing them. I can see this being an addiction.While this might be mentioned somewhere already, the capturing process can take a while. It’s not a two-minute thing. Given what the capture feature is doing, I’m sure you will be happy to wait for the final result.One thing you can allegedly do with the Quad Cortex is capture VST plugins. I have plans to try and capture some of the Neural DSP Archetype plugins onto the Quad Cortex. I want some of those Tim Henson sounds on my QC.Final thoughtsI love the Quad Cortex, but there are areas that can and I believe will eventually be improved. The effects selection is a bit dismal at present. I mentioned this earlier, but the Line 6 Helix and even other modellers like the Axe-FX or FM3 have more to choose from.The Helix has 38 distortion/overdrive pedals, for example, and the Quad Cortex only has a small selection at present. Some more amp and speaker cabinets would be great too (something I know that will happen over time).Unlike other modelling devices, you also cannot change things like the sag, hum, bias and other more advanced settings that most people probably would never touch. It’s worth knowing these are currently not available, but there is no reason why they couldn’t be added in future updates.The boot time for the Quad Cortex is also quite slow. A cold boot seems to take upwards of 40 seconds (it might not be that long, I haven’t physically timed it), but it’s not quick.Overall, it’s a solid unit right now. To answer the question of whether or not you should buy the Quad Cortex. If you’re happy to buy a device that will take another year or two to reach its full potential, buy it.",2952
A Happy Meal at McDonald’s in Australia Is Cheaper Than a Small Cheeseburger Meal,"Can someone explain this one? A cheeseburger happy meal at McDonald’s here in Australia is $4.95 (currently). A small cheeseburger meal is $6.95. They are the same thing.A cheeseburger happy meal comes with a cheeseburger, fries (or apple slices), a toy and a small drink. It’s a small cheeseburger meal with an included toy and the option of apple slices over fries. A small cheeseburger meal comes with all of the above (minus the toy). You don’t get the option of Apple slices, but that’s it.Do I misunderstand the economics of this? The Happy Meal comes with a box and a toy, both presumably costing money. The small cheeseburger meal comes in a paper bag with minimal printing on it. Is the happy meal some loss leader, where they know adults will be ordering over the top to make up for the lost margins?I somehow doubt a well oiled corporate machine like McDonald’s would ever run a loss on anything that it sells, especially Happy Meals which are the popular young child option.Something tells me that McDonald’s are cutting down their profit margin (which is probably still high) because they know a happy meal is statistically going to be accompanied by an adult meal or drink purchase.",299
How to Get the Viewmodel Instance From the Aurelia 2 Au-Compose Element,"Dynamic composition is a crucial part of developing robust user interfaces in Aurelia. If you worked with the compose element in Aurelia 1, you might (or might not have) needed to obtain a reference to the composed view-model itself.While Aurelia 2 keeps many things the same, how dynamic composition works is a little different. We have the new au-compose custom element, which allows us to achieve the dynamic composition of components, including passing data into them.I am in the process of porting over an Aurelia 1 application, and it contains a wizard-based step form. The wizard acts as the containing element and queries the child step custom components, allowing specific methods inside of them to be called (validation callbacks and so on).Now, in Aurelia 1, you could write something like this:<template>
    <compose view-model=""hello"" model.bind=""myModel"" view-model.ref=""composeRef""></compose>
</template><template>
    <compose view-model=""hello"" model.bind=""myModel"" view-model.ref=""composeRef""></compose>
</template>Using view-model.ref, you could access the composed view-model instance. In Aurelia 2, it’s not much different, except the referenced view-model is not the composed view-model.<au-compose view-model.bind=""hello"" model.bind=""myModel"" view-model.ref=""composeRef""></au-compose><au-compose view-model.bind=""hello"" model.bind=""myModel"" view-model.ref=""composeRef""></au-compose>Inside of your view-model, which gets the composeRef value, you can access the composed view-model in the composition controller:attached() {
    this.composedViewModel = this.composeRef.composition.controller.viewModel;
}attached() {
    this.composedViewModel = this.composeRef.composition.controller.viewModel;
}The composition property on the provided reference is where the view-model and other associated values/properties for the composed custom element live.I have created an issue for this on the Aurelia GitHub repository. There is a chance that this might be simplified to be more in line with how Aurelia v1 works. The outcome will either be the viewModel property works like v1, or this behaviour will be better documented.created an issueviewModelAurelia users need to understand that several improvements have been made in Aurelia 2, which address some fundamental shortcomings in how v1 was designed. Aurelia 2 makes use of classes that it instantiates, opposed to the mess of strings and module resolution v1 used, which caused plenty of issues with bundlers and complicated implementing support for things like lazy loading and split bundling.",642
The Solution to Npm Package Hijacks & Malware Is Deno,"I think it has become abundantly clear that Node.js and how it deals with dependencies is flawed and has become a total liability now.Npm has become the bank vault of the web.On October 22, 2021, a popular Npm package was hijacked and exposed to anyone who downloaded it to a password harvester and cryptocurrency miner for 4 hours. This package is called UA-Parser.js. a popular Npm package was hijackedThere is a good chance you might not have even heard of this. However, this package is downloaded almost 8 million times per week. Allegedly some large companies like Amazon, Facebook and Google are using this package.this package is downloaded almost 8 million times per weekThanks to the work of the community and numerous partners, the issue was resolved in four hours. In a GitHub advisory, they detailed multiple versions of the rogue package that were created.they detailedIt would seem that these kinds of malware attacks are becoming increasingly common. Sonatype identified some rogue packages in their research, and Npm subsequently removed them within hours. In other research by Sonatype, they also revealed software supply chain attacks are up 650%.Sonatype identified some rogue packages in their researchsoftware supply chain attacks are up 650%It sounds confusing if you’re not familiar with the broken way in which Npm deals with dependencies. The issue with Node.js is that when you install a dependency, you’re not only installing that dependency but possibly hundreds of other dependencies that dependency requires. You might recall the Left Pad incident (not an attack) where the author just shut the dependency down, and it broke a heap of apps and packages.the Left Pad incidentIn this article, the author details how Node.js can be attacked with minimal effort. In research from 2019, it turns out you would only need to hack 20 accounts to control half of the Npm ecosystem. It’s chilling, it’s sickening, and people should be angrier about this than they are. How did we become so dependent on a fragile ecosystem prone to malware and other types of attacks?this articleyou would only need to hack 20 accounts to control half of the Npm ecosystemThe solution already exists, and it’s staring at you right in the face (it’s in the title, after all).DenoIn quite a famous talk in tech circles, Ryan Dahl introduced the world to Deno in a talk titled, “10 things I regret about Node.js.”

I am ignoring the obvious points about package.json and how comically large the node_modules directory can get, Ryan talks about the security of Node.js. The fact that it is not sandboxed by nature is one such problem. Although, I feel developers haven’t adequately considered this downside of Node.js.package.jsonnode_modulesI want to point out that I am only currently exploring Deno for my projects. I still primarily use Node.js because the front-end ecosystem has become highly dependent on it. Bundlers like Webpack rely on Node.js to function, and by extension, Node packages.I would love to see the front-end ecosystem embrace Deno, but to my knowledge, nobody has even tried to do that yet. Node.js has become the Java of the front-end world. It’s just expected that it’s there.The lack of sandboxing and security features is why these rogue Npm package attacks are so effective. They have full access to everything. The entire environment is up for grabs. It sounds insane saying it out loud, but Node.js provides so much power out of the box and doesn’t stand in your way.Another thing Deno does is remove the need for a centralised package repository like Npm. It allows you to import packages from URLs. A simple Javascript import to a TypeScript or Javascript file will suffice. It then gets cached in the project directory, and it’s encouraged you check that cache into your Git repository.Deno has yet to fully take off, despite releasing a stable 1.0 release in May 2020, because developers in the Node ecosystem are too heavily invested in Node.js. We also have a chicken and egg problem. A lot of the packages in the Node.js world are not available for Deno (yet). I bet developers would flock to Deno if it supported Npm’s insecure package management.despite releasing a stable 1.0 release in May 2020While Deno might not be entirely on par with Node.js in every facet of performance, it’s more than good enough to start using. We have a real problem in the ecosystem. Node.js cannot be trusted. That’s just an honest fact. It has become a liability.every facet of performanceI am also not saying that Deno is bulletproof and doesn’t have security holes that hackers could exploit of its own. The issue with Node.js isn’t Node itself; it’s the Node Package Manager. A centralised point of attack that has widespread influence and control over the Node ecosystem.Imagine if there was only one bank for the entire world? Instead of your credit unions and larger banks, it was one centralised bank that held everyone’s money. That sounds like a bad idea. But it is precisely what Npm is. It’s a centralised bank for Node packages.While the attacks haven’t resulted in catastrophic consequences just yet, I do believe it is only a matter of time before we see an Npm attack that causes some severe fallout. It just seems far too easy to compromise the account of a maintainer and then take control of their packages.Can you imagine if the attackers found a way to infiltrate the systems of big tech companies through the UA-Parser attack? Imagine if Google or Facebook had their systems penetrated on a deep level, even if only for four hours, that’s enough time to cause some damage.Or, here is another scenario to think about. What if a hacker infiltrates the Npm organisation itself? On paper, they look like a highly qualified candidate. In the shadows, they’re working for a hacker group or government entity with the intent to attack the ecosystem from the inside out. It might sound far fetched, but if you wanted to compromise a large part of the web ecosystem, is it that far fetched?Is it any wonder there is so much buzz around Web 3.0 and decentralisation? Time and time again, we keep being reminded why centralisation is terrible and dangerous.",1548
How to Stop Aurelia 2 From Stripping Your Aurelia Attributes From the Compiled HTML,"In Aurelia 1, you could debug and identify Aurelia applications based on specific attributes in the dom. In Aurelia 2, the default setting for compiled HTML is to strip away Aurelia framework attributes.What this means is if you need to debug your HTML, see what custom attributes are being passed to a custom element, and so on, you won’t see anything.Like everything in Aurelia, you can customise this. Inside your main.ts file, you can set debug mode to true to bring back the Aurelia specific HTML attributes and markup.import Aurelia, { ITemplateCompiler } from 'aurelia';

Aurelia
  .register(
    AppTask.beforeCreate(ITemplateCompiler, compiler => compiler.debug = true),
  )import Aurelia, { ITemplateCompiler } from 'aurelia';

Aurelia
  .register(
    AppTask.beforeCreate(ITemplateCompiler, compiler => compiler.debug = true),
  )This will make it, so your dom is noisy again. Just remember to turn it off once you’re done. It will prevent the dom from bloating and introduce potential performance issues due to an excessive size page.",262
Why I don’t miss working in an office,"Does anyone else remember offices? You know, those places you spent upwards of an hour commuting to in standstill traffic or overcrowded expensive public transportation? Shoulder to shoulder, bumper to bumper. Those places where sick coworkers would come into the office and kindly spread their sickness?Despite the pandemic destroying livelihoods, causing widespread mental health issues and changing the way we live, some good has come of the pandemic.The death of the office.The first casualty of the pandemic wasn’t the supply chain. It was the office. As COVID-19 spread, countries began to lock themselves down. People were encouraged to stay home, to only move for essential purposes. As a result, many companies shut their offices down and let their employees work from home.I know some of my friends struggled to work from home. Not because they didn’t trust themselves, but because the childcare/schooling situation became complicated. Many schools and places shut down for a while, burdening parents who had to figure things out. Disproportionately, women were left to deal with this problem.I also know people who struggle to work outside of a traditional office environment. That’s fine. Not everyone wants to work from home, but overwhelmingly it seems a large majority of workers do. This is why companies need to understand one thing: remote work and flexible hours are no longer perks. They’re expected.If you’re not giving your employees a choice, they’ll most likely be part of that Great Resignation movement I am sure you’ve heard about.Despite the challenges of kids, I love being at home. I was already working from home three days per week and 2 in the office (hybrid work). I never missed packages when they were delivered, we didn’t have to fear our HelloFresh boxes being stolen (a problem in the area we are currently in), and I got to see my kids growing up, play with them on lunch breaks.My eldest son is in school now, so I don’t see him during the weekdays. My daughter, who is two and a half and growing up fast, goes to kindergarten two days per week to prepare her for school. Those two days also help regain some of the time she lost as a toddler getting to socialise on playdates and other things you don’t realise are pretty crucial to the development of a child.But, I am so grateful I got to be at home with them for a bit, even if, at times, my wife and I were counting down the hours to bedtime (and the number of hairs we had left on our heads).I no longer waste as much moneyOne of my worse habits formed from working in an office was buying coffee every day. A regular cup of coffee will set up back on average AUD 5 these days. I was sometimes getting two coffees a day. You do the math per month. Even one cup per day over a month is $100 per month in coffee. And then there were the lunches. Despite taking lunch and being social with others, I would eat out at least once a week. A burger will easily set you back AUD 20. Throw in a drink, maybe $25. That’s another easy $100 per month to bond and be social with your teammates.You could argue that I didn’t need to buy coffee and that I didn’t need to buy lunch. But, I am sure you have seen that one person in the company who never partakes in team lunches or the quick out of office coffee trip. They get silently judged or viewed as a sort of outsider in the company (even if it’s never spoken about).When the pandemic hit, my wife and I decided to buy a coffee machine and start buying coffee beans. We make our fresh coffee each morning, and we’ve saved probably thousands now, not spending $5 a day on coffee. My wife and I still sometimes go out for lunch together, but it’s not once a week.I no longer have to be “always on”I am an extrovert, but even I find constant interaction exhausting and distracting.Some days in the office, whether mental health-related or just tired, you don’t want to have conversations about what you did on the weekend.  “What did I do? thanks for asking, Bob. I sat down with a bag of Doritos and binged Stranger Things with my wife. I thought I ate the whole bag, but then I got up and I found two that had rolled under the sheets, so that was a nice surprise.”Sometimes you want to do your job and then go home. We all have these days. But, if you start ignoring your teammates or being short, you won’t last long. I’m also polite, so I would never outright tell someone I don’t want to speak to them, even if I have a bad day/week. Another reason I don’t miss the office. Sometimes a coworker can suffer from unable-to-read-the-room-itis where they can’t take the hint you’re just not with it today. Which is fine; offices aren’t known for privacy and boundaries.And then there’s the pressure of always having work open on your screen. I don’t know about you, but I am constantly procrastinating. I consider myself a highly functioning procrastinator. I tend to get things done before they’re due. I love watching a YouTube video or even a conversation with a coworker over Slack.I worked somewhere once where I would come in early in the morning, and the first thing I would do was check the news. I am talking, maybe 7:50 am. My salaried start time wasn’t until 8:30 am, so I would read the news, check Hacker News and so on. The guy who was my manager pulled me into the meeting room one day and said, “I notice you’re always looking at non work things every morning, can you not do that? I want you to open up your task list and see what you’re going to be working on when you first get in.”I get why he said it. It’s what he did when he came in. He wasn’t browsing news sites or clicking Hacker News links. But it bothered me. He could only see his way of working, and technically, I didn’t even need to be there at that time.In response to what he said to me, I started arriving at work later. I would sometimes get in at nine instead because that’s what some other people did. You can probably guess I didn’t stay there for too long after that conversation.Working from home allows me to work more flexible hours. Sometimes I start at 7 am because I can and finish earlier. My wife and I sometimes go out for lunch together when both kids are at school. Best of all, the conversations can be on my terms.I am less distractedDespite just telling you I procrastinate like there is no tomorrow sometimes, I am focused when I am working on something. I sometimes get so focused. I can go for hours without breaks or eating. Some days I skip lunch, and it isn’t until it gets to 4 or 5 in the afternoon and I begin to finish up that I realise I am starving.I thrive in chaos. I grew up in a large family of five sisters (I am the only boy), so I know what chaos is. But, sometimes, you want to be left alone. There can not only be distractions (a coffee machine is a good example), but even if you have headphones on, some people still feel it’s okay to tap you on the shoulder to ask for something.I also have self-diagnosed Misophonia. When I hear people chewing loudly (chips especially), I clench my teeth and sometimes fists. It seems silly to get angry over the sound of chewing, but it genuinely makes me feel tense and like I need to get away from the situation. I never get violent or physical, but the feeling of anger that envelopes me is not pleasant. I haven’t experienced this feeling in a long time since working full-time remote.I get sick way lessGetting sick less could be attributed to masks, but I get sick way less than when I was in an office. Everywhere I have worked, there is always someone who thinks they’re the hero coming into the office with inflamed sinuses and chesty cough that makes them sound like they’re auditioning for Seal: the musical.Fortunately, I do have my own experience to compare to. Working remotely before the pandemic normalised, I got sick less being in the office two days a week. Keep in mind that I have kids, so the threat of sickness is always looming; hence, I think masks have contributed to people getting sick less.I never understood what people who were coming into the office sick pre-pandemic thought they were achieving. Were they hoping in their performance review, their manager would say, “I’ve noticed even when you are sick, you still show up and work hard. We appreciate that, so here’s a trophy, a $10k bonus and a week of paid leave you can take any time, no questions asked”?We can only hope this frowned upon behaviour never sees a return. However, there will always be someone who feels pressured (for whatever reason) to be in the office, even as they spread their nasty viral droplets around the office.I have more timeWhen you’re 70 years old, and you’re retired, you’re not going to be saying, “I really regret not going into the office more” or, “I really wish I commuted more” — I used to enjoy catching the train sometimes if I was lucky enough to get a seat. Being able to listen to a podcast or read a book was great. But when you’re spending an hour on a train one way, the lost time over a year is enormous.Want to listen to a podcast or read a book? You can still use that time you would have commuted to unwind, except you can do it on the couch in your own home.I calculated when I was going into the office for one of my previous jobs. I was losing a minimum of 40 hours per month in commute time. And, realistically, it was more than that if the traffic was terrible or the bus didn’t show up, a medical emergency at a train platform/train, or a bad storm causing damage/flooding to the roads and train tracks.In total, going into the office full time is costing you around 500 hours per year in commute time (if it takes you one hour to go one way to the office).ConclusionIf I didn’t make it obvious enough, I don’t miss the office at all. And really, what purpose do they serve? What can you do in an office that you can’t do at home or in a cafe? You can try citing cultural reasons for wanting to keep an office, but you can have good company culture without having people in a physical place.Companies trying their hardest to force people back into the office are waving red flags by fighting the inevitable. As always, companies that refuse to embrace the future of work are doing so because it means giving up control. If you can’t trust your employees enough to let them work remotely, either fire them and hire people you do trust or take the leap and see what happens.Ironically, many of the companies fighting remote work during the pandemic (like almost every company that could) allowed their employees to work from home full time anyway. If your company survived the pandemic by letting your employees work from home, isn’t that proof enough? If your company saw its revenue increase letting employees work remotely, isn’t that further proof?Can anyone point me to a list of companies that went out of business because they switched to a remote workforce during the pandemic? Because I haven’t heard of any companies going out of business by offering it.And think about it some more. Weren’t you already working remotely anyway? If you own a pair of noise-cancelling headphones or work with others that do, and your company uses Slack or Microsoft Teams, people were already socially distancing and in their tiny bubbles in the office anyway.When I worked in an office full-time, I would say about 80% of work-related conversations would occur over Slack, email, or inside a ticket in Jira or Trello. Why does it matter if I do it in the office or at the home? It’s the same either way.",2884
"Facebook Changes Its Name to Meta; Same Shit, Different Smell","After weeks of speculation in what has been the worse kept secret, Facebook has announced that it is changing its name to Meta. When they say name change, they, of course, are referring to the corporate umbrella that is the company, not the facebook.com social network.The move is akin to Google rebranding to Alphabet and other companies like BP renaming to Beyond Petroleum. Except, in the case of Google, they didn’t rebrand to escape controversy like Facebook, BP and Phillip Morris have done.“Hey everyone, don’t worry about that racism stuff, check out our cool new name”The name change signifies Facebook is moving in a new direction, away from its social media platform roots. They’ve been talking about a fantasy world called the Metaverse, in which people will interact with others virtually, visit virtual buildings and other stuff. It sounds incredible if not a tad Black Mirror creepy.Despite the rebrand, the same old problems that plague Facebook will remain. And no, I am not just talking about the alarming number of baby boomers and anti-vaxxers that flock to the platform.If Facebook can’t even get its current platform in order, can you imagine what this Metaverse will be like? A cesspool of hatred, anti-vaccine propaganda and boomers typing in all capital letters with ten exclamation marks at the end of each sentence. What will the tagline be for Metaverse? Racism and hatred: now in 3D form.It might sound contradictory, but it could be exciting if they can pull off this Metaverse concept and not have it devolve into a toxic wasteland. I am picturing Habbo Hotel-esque avatars and 12-year-old kids shouting profanities when I think of the idea, but I guess technology has evolved a little since the early 2000s, right?Still, despite my excitement of VR finally becoming a thing after years of false starts, the rebrand right now is nothing more than lipstick on a pig and case of the same shit, different smell. Facebook has a lot of work to do.",493
Developers Don’t Care About Web Standards Anymore,"When I started as a developer, the term front-end developer was almost nonexistent. Let me pull up my old man socks while I regale you with stories of a simple time in web development when Node.js wasn’t even in the womb yet, and Microsoft was not the open-source friendly company they are today.It used to be a badge of honour to have a W3C validation badge on your website. Developers used to spend ridiculous amounts of time getting their sites compliant with XHTML/HTML as per the spec. I am talking about alt tags, proper semantic use of HTML elements, putting widths on images, everything.But is it just me, or are developers more concerned about front-end libraries and tooling? Have developers become more obsessed over their bundle sizes than they have writing spec-compliant HTML? Thanks to React and other options, is it just way too easy to write non-spec compliant HTML because everything is abstracted and jumbled up now?The answer is yes.It turns out the W3C validator still exists here. I would shudder to think what it would come back with for a lot of popular sites. Take Reddit, for example; 210 items came back. I am sure some of this is picky stuff, maybe related to third-party scripts, but the point is nobody seems to care about things like this anymore.Most of those things us old timer web developers used to fret over were accessibility things, not standards as in browser features. Not all disabilities are equal. Some people are colour blind. Some are blind, others limited movement of their limbs. Not everyone visiting your site is using a keyboard and mouse or the latest tablet/phone.Let me share a cool story about people fighting for accessibility. In 1999, someone filed a lawsuit and won against the 2000 Sydney Olympic Games website for being inaccessible. There are accessibility guidelines that developers should abide by, but half of the websites you visit in 2021 don’t even work properly if you disable Javascript.And look, I would be a hypocrite if I told you that I always think about accessibility and standards. I bet this site isn’t even accessible (although I did modify an existing theme). I am not finger-pointing here, more raising a question: what happened to web development? At what point did we lose sight of what was important and start caring more about hype.js libraries and tooling?here210 items came backfiled a lawsuit and wonNow, if you’ll excuse me, I have to yell at some kids to get off my lawn while I wave a newspaper around (do you remember newspapers?).",631
The 10x Engineer: Born or Created?,"In the world of software and web development, you might have heard of the term 10x engineer. It’s a term that refers to a person who can increase productivity and get work done faster on a team than other developers. It’s a term people often misuse to describe a team member who can do the work of ten people or work ten times faster. In other words, someone with a rare set of skills and talents makes them more productive and makes them far more valuable to their employer.The mere mention of the term conjures images of a lone wolf developer in a hoodie, working on complex problems by themselves at a stand-up desk. Left alone for days and weeks wallowing in their genius, to emerge from their darkened Apple store aesthetic office with a bug-free solution, appeases management and fills others with envy.How do they do it? They’re so intelligent. They know everything.Everyone has an opinion on this topic. Some believe 10x engineers exist. Others put them into the same category as ghosts and Big Foot.I believe 10x engineers do exist, but not in the way you might think. I don’t believe some engineers can move between companies and instantly produce higher output than anyone else in the organisation. I also do not believe that a single developer can have ten times the output of other developers.A 10x engineer is a combination of domain experience and business knowledge. Someone with 12 years of experience at the company for four years will be more efficient than someone with three years of experience who might have only been with the company for a few months or a year. That’s just a fact.Let’s run with a fictional example to illustrate the point:Let’s run with a fictional example to illustrate the point:John works for Insurance Co and has been there for seven years as a developer. At Insurance Co, they build consumer insurance products. Think of a website where you can manage your policy, add additional items to your insurance policy and handle payment.In that time, John has worked on a plethora of features. He knows the terms that are specific to insurance. He also is quite familiar with the codebases, able to find where particular features live quickly. If John were tasked with creating a new feature, he would know where it should go and what existing API’s and code already exist to reduce the amount of work.Now, Tim joins the company. Tim is an experienced engineer who graduated at the top of his class and has worked for some prestigious tech companies. He has just as much experience as John. Still, he’s new and Tim has never worked in insurance before. Before Tim can be an efficient member of the team, he needs to understand the codebase and the business knowledge that has been instilled in others who have been at the company a lot longer.The thing to note in this example is that his intelligence or skillset will not limit Tim. Tim is going to be hindered by his lack of business knowledge and familiarity with the codebase. By comparison, John and any other developer at Insurance Co will be more productive than Tim is, at least until he gets up to speed.We have all worked with 10x engineers. They just might not fit the exact description of what you’ve been told they are. Not all 10x engineers are arrogant, self-entitled lone wolves that graduated top of their class, have worked for Google and write compilers for fun.Every company usually has a 10x engineer. It might even be you. You know that one person (it’s commonly one person, in my experience) that everyone else in the company goes to when they get stuck on a problem or want to know where something is? That one person that the boss and management pull into a planning meeting when they want to build a new feature.Except, I don’t call them 10x engineers. I call them office encyclopedias. Because that’s what they are. This person’s perceived intelligence and efficiency is because they have been with the company most likely for a long time. They know where everything lives.This is not to say that a 10x engineer isn’t also intelligent. Knowing where things are and how to work with the codebase is one advantage, but you need exceptional problem-solving skills to be worthy of the title. These engineers are usually founding members (or early hires). They probably built a lot of the foundational code other developers are working with. Anyone who has ever worked for a small company or had to wear many hats will relate.It is for this reason that recruiters and companies struggle whenever they try hiring 10x engineers. They are not born. They are created. All developers can become 10x engineers. Having fancy degrees or working for big tech companies is not a requirement, nor will they make you become one faster.",1184
Why Is PayPal Interested in Buying Pinterest?,"If you haven’t heard the news, PayPal (aka the devil’s payment processing company) is allegedly in late-stage talks to acquire Pinterest. But, the question I’m sure many are asking is: why?late-stage talks to acquire PinterestWhy does PayPal want to buy Pinterest? Why would Pinterest be entertaining such a low offer at $70 per share which would value the deal at approximately $45b when Microsoft offered a $51b deal in early 2021? It seems a little low unless PayPal is going to offer stock as part of the deal.It’s no secret that Pinterest is a valuable part of the sales funnel. They also have a very valuable userbase comprised mostly of women. PayPal has also seen increased competition from buy-now-pay-later companies like AfterPay, competition from Shopify and Instagram taking an e-commerce path. Really, PayPal is part of the sales process but has no platform of its own.If PayPal did acquire Pinterest, undoubtedly it would be a boon for PayPal. It’s a valuable platform, despite reporting a decline in users in July 2021.a decline in usersThe most exciting thing about this deal I want to know is, how is PayPal going to afford this? The contract terms will be interesting if the deal goes through and doesn’t fall apart as the Microsoft one did.Even more left field, what happens if Microsoft comes back to the table and offers even more money? All I know is, PayPal are going to have to pony up more than $70 per share for this deal to go through.",366
The Bespoke Chip Arms Race Is Upon Us,"Semiconductor shortages aside, a chip arms race has been brewing in the background of the pandemic over the last few years. Even before the pandemic, large companies were already exploring their own custom fabrications to reduce costs and dependence on dominant chip suppliers.For years, just a handful of chip companies have dominated most facets of the chip market, from the chips inside of your modems or wireless devices to graphics cards, mobile phones, gaming consoles, and any other modern gadget with a chip inside of it.But, times are changing.Apple, Amazon, Facebook, Google, Microsoft, Tesla, and a few other companies have begun producing their custom chips.In November 2020, Apple announced its custom chip, the M1. After using Intel chips for years, this move was quite monumental. Google just announced new Pixel phones with a new custom-designed chip called Google Tensor. Apple announced its custom chipGoogle TensorTSMC and Samsung are two of the biggest beneficiaries of these custom chipsets — with TSMC being one of the biggest chip foundries in the world. It takes billions of dollars and upwards of a decade to create a foundry as TSMC has in Taiwan. An undertaking that not many companies can handle themselves.TSMC and SamsungThe benefits of bespoke chips are they can be designed and tailored to the specific use-cases of the companies commissioning them, reduced power usage and a tailored set of features they can handle. Google has been focusing on artificial intelligence applications these past few years (this AI chip work resulted in Google Tensor)The downside of companies like Apple and Google producing custom chipsets for their electronic devices is the ongoing semiconductor shortage. TSMC was rumoured to have prioritised Apple’s chip production (for obvious reasons), probably contributing to the chip shortages in other industries that rely on chips (like the automotive industry). TSMC accounts for around 50% of semiconductor manufacturing.Although companies like Apple and Google are designing their own chips, they are not manufacturing them. The equipment required is expensive, the facilities required to house the equipment are expensive. I’ve seen estimates in the tens of billions to set up a foundry as TSMC has.As a result of the pandemic, we will probably inevitably see Apple and Google invest in chip manufacturing facilities, which is not a bad thing. The worrying thing about TSMC which currently produces chips exclusively in Taiwan is the geopolitical crisis with its neighbour China. Any conflict would have disastrous consequences that would make the pandemic pale in comparison.",660
Exploring Strapi as an Alternative to WordPress,"I love WordPress. To me, WordPress is comfortable. I am familiar with it and have never encountered a situation where I couldn’t make it accommodate my use case or needs. In the years that have passed since WordPress became the dominant CMS, a few other CMS’s and frameworks have come and gone, threatening to take WordPress’ mantle (it still hasn’t happened).Some elitists out there believe that WordPress is underpinned by spaghetti legacy PHP code that makes it a non-option for some. Most of those against WordPress are more opposed to PHP itself than the CMS.Yes, WordPress has some skeletons in its closet. But, at the same time, Automattic and contributors to WordPress have done an exceptional job of evolving WordPress and keeping it fresh. Gutenberg is much better now they’ve sorted out the issues that plagued it for the first two years of its existence. The WordPress REST API is exceptional.Despite what some will try arguing, WordPress itself is not inherently bad.You get custom post types, a powerful taxonomy system, REST API, an ecosystem of plugins and themes, and most importantly: a well designed and fully featured administration panel. Although I am pretty comfortable with WordPress, I also acknowledge some needs. It might be a bit heavy-handed.This is where Strapi comes onto the scene.This is where Strapi comes onto the scene.On paper, Strapi looks like a great Node.js based alternative for those who use WordPress more as an application framework via the headless REST API. It has support for a multitude of databases, it has a small but passionate community and it’s easy to get started.However, there is one downside to Strapi that doesn’t replace a need being met by WordPress.WordPress Multisite aka Multi-tenancyWordPress Multisite aka Multi-tenancyI know many people don’t need WordPress Multisite, but I do on a project I’ve been working on for over a year. Sadly, Strapi has no official support for multi-tenancy. It has been on the roadmap since 2018 but doesn’t look like it’ll be coming any time soon.It has been on the roadmap since 2018If you were building a system with a leading site followed by many child sites (blogs, stores, user profiles), you need some form of multi-tenancy. Deploying an installation of Strapi per use would require a lot of orchestration to achieve. The only solutions for Strapi at present require you to implement hacks to get faux multi-tenancy, but not the same level of individual site management on a single codebase type multi-tenancy.I think Strapi is a great option where you need something tailored and customised to your needs closer to the metal (so to speak). However, if you need blogging capabilities or multi-tenancy, WordPress is still the undisputed open-source option.",690
How to Sort an Array of Object Values Alphabetically and Numerically,"While sorting an array of objects recently, I needed to sort by an identifier prefixed with two letters and ended with numbers. An example is “AQ110” — you might be tempted to reach for a library, but you can achieve this in Javascript with very little code.Say hello to localCompare.Say hello to localCompare.I had a list of values that looked like this:AQ2AQ110AQ19AQ190AQ64AQ5AQ2AQ110AQ19AQ190AQ64AQ5The lettering didn’t matter so much, but the numbers did. By default, just using a plain old .sort() will not be enough. If you were to use .sort() by itself, it would get tripped up on high numbers starting with the same value (eg. 10, 110, 11 and so on).sites.sort((a, b) => {
  return a.meta.siteNumber.localeCompare(b.meta.siteNumber, 'en', {numeric: true, sensitivity: 'base'});
});sites.sort((a, b) => {
  return a.meta.siteNumber.localeCompare(b.meta.siteNumber, 'en', {numeric: true, sensitivity: 'base'});
});The first argument of the localeCompare function is the value being compared against. The second is the locale, and the last argument is the configuration options. The important option here is numeric: true which tells localCompare that we want to parse the numbers in our value. We also use sensitivity: 'base' to ensure our comparison is case-insensitive.numeric: truelocalComparesensitivity: 'base'As you can also see, my array contains objects and I am sorting on a specific property called siteNumber which is a meta value for the item I am iterating.siteNumberThat’s all there is to it. There are many more uses of localCompare, but comparing numerical and string values strings is one of its most substantial capabilities.",413
How to Query WordPress Multisite by ACF Option Values,"Did you know WordPress Multisite got some powerful new functionality in WordPress 5.1, allowing you to query sites by meta values?allowing you to query sites by meta values?This functionality allows you to query sites in a WordPress Multisite network by meta values and other query-based syntax to get back one or more locations.Like many who develop WordPress websites, I use the awesome Advanced Custom Fields plugin. What ACF Pro gives you is powerful field functionality that really should be core functionality in WordPress itself.I need to have an options page that allows me to store unique options against a site and then use WP_Site_Query or get_sites() to query for sites with those specific values. One such use case is categorising sites by country.Surprisingly, there are not a lot of blog posts or even information on using WP_Site_Query or its related functions in WordPress. In my opinion, it’s some of the most useful functionality introduced in WordPress in a very long time. It opens up a lot of possibilities that required hacking together switch_to_blog calls to achieve prior.Mirror ACF settings to sitemetaIf you have a custom ACF options page registered using acf_add_options_page, it might look something like this:acf_add_options_page(array(
  'page_title' 	=> 'Site Settings',
  'menu_title'	=> 'Site Settings',
  'menu_slug' 	=> 'site-settings',
  'capability'	=> 'edit_posts',
  'redirect'		=> false
));acf_add_options_page(array(
  'page_title' 	=> 'Site Settings',
  'menu_title'	=> 'Site Settings',
  'menu_slug' 	=> 'site-settings',
  'capability'	=> 'edit_posts',
  'redirect'		=> false
));This code will create a custom options page that allows us to add in ACF fields as we would to a custom post type or other screens in WordPress.Behind the scenes, ACF will use the WordPress options API and numerous methods such as update_option to save the values. Sadly, you cannot query WordPress WP_Site_Query to query for sites by options. However, we can mirror our options to sitemeta that can be queried.Inside of your functions.php file, add in the following code:add_action('acf/save_post', 'mirror_options_to_sitemeta');
function mirror_options_to_sitemeta($post_id) {
    $screen = get_current_screen();

    if (strpos($screen->id, 'site-settings') == true) {
        //print_r(get_site_meta( get_current_blog_id()));
        $values = get_fields( $post_id );

        if ($values) {
            foreach ($values as $name => $value) {
                update_site_meta(get_current_blog_id(), $name, $value);
            }
        }
    }

    return true;
}add_action('acf/save_post', 'mirror_options_to_sitemeta');
function mirror_options_to_sitemeta($post_id) {
    $screen = get_current_screen();

    if (strpos($screen->id, 'site-settings') == true) {
        //print_r(get_site_meta( get_current_blog_id()));
        $values = get_fields( $post_id );

        if ($values) {
            foreach ($values as $name => $value) {
                update_site_meta(get_current_blog_id(), $name, $value);
            }
        }
    }

    return true;
}Using the get_current_screen() method, we determine if the screen is the same as our menu slug from the code above. The acf/save_post action will get fired every time options on our ACF options screen are saved. We do this, so it doesn’t fire for other ACF save calls (acf/save_post is a generic hook called for any ACF save).Querying WordPress Multisite by meta valuesThis is the fun part. I have a function called using the WordPress REST API that allows you to query for sites that belong to a specific country.$site = new WP_Site_Query();

sites = $site->query([
  'number' => 99999,
  'meta_query' => [
    [
      'key'   => 'country',
      'value' => urldecode($request->get_param('country'))
    ]
  ]
]);$site = new WP_Site_Query();

sites = $site->query([
  'number' => 99999,
  'meta_query' => [
    [
      'key'   => 'country',
      'value' => urldecode($request->get_param('country'))
    ]
  ]
]);I am setting a stupidly high number because there could be hundreds or thousands of sites that come back. The secret sauce of this query is the meta_query which might look familiar if you have ever written meta queries when querying using WP_Post.A more contrived example if you were to manually query by meta value would be:$site = new WP_Site_Query();

sites = $site->query([
  'number' => 99999,
  'meta_query' => [
    [
      'key'   => 'country',
      'value' => 'australia'
    ]
  ]
]);$site = new WP_Site_Query();

sites = $site->query([
  'number' => 99999,
  'meta_query' => [
    [
      'key'   => 'country',
      'value' => 'australia'
    ]
  ]
]);This will return an array of WP_Site objects. However, the downside is that the WP_Site object returned might not have helpful information, like the site’s name. This is where you’ll need to use the methods get_blog_details and possibly get_site_meta to get the additional data.WP_Siteget_blog_detailsget_site_meta",1245
How to Remove the My Sites Menu From the WordPress Admin Bar,"I’ve been working on a WordPress Multisite installation that is going to eventually have upwards of 20,000 sites. Somewhere around the 50 site mark, I noticed the WordPress admin panel was beginning to slow down. And, after some investigation, I discovered the “My Sites” menu in the admin bar was part of the problem.Using the admin_bar_menu action hook, we can bind to this admin bar (like most things in WordPress), and remove items we don’t want to show.add_action('admin_bar_menu', 'remove_admin_toolbar_items', 999);

function remove_admin_toolbar_items( $bar ) {
	$bar->remove_node( 'wp-logo' );
    $bar->remove_node( 'my-sites' );
    $bar->remove_node( 'comments' );
    $bar->remove_node( 'new-content' );
    $bar->remove_node( 'wp-mail-smtp-menu' );
}add_action('admin_bar_menu', 'remove_admin_toolbar_items', 999);

function remove_admin_toolbar_items( $bar ) {
	$bar->remove_node( 'wp-logo' );
    $bar->remove_node( 'my-sites' );
    $bar->remove_node( 'comments' );
    $bar->remove_node( 'new-content' );
    $bar->remove_node( 'wp-mail-smtp-menu' );
}In my case, I am removing the logo, my sites, comments, new content and a menu added by an SMTP plugin for mail. Just by removing the My Sites menu from the admin bar, my load times went from sub-10s to a couple of seconds. I more than halved the load time of the WordPress admin panel.Now, to determine the names of the items in your admin bar (if you want to remove other things) you can inspect the page by hitting F12 and bringing up your developer tools.All you have to do is remove wp-admin-bar- from the ID to get the node slug we then pass to remove_node. If you end up removing a heap of things from the admin bar, it might even be worth just removing it entirely.wp-admin-bar-remove_node",442
Shared Uploads Directory in WordPress Multisite,"If you’re working with WordPress Multisite, you might encounter a scenario where you want all uploaded media to share the same folder. In my case, I needed all sites to use the same uploads directory as the parent site.As with most things in WordPress, there is some hook or filter you can use to change core WordPress functionality. For the upload directory, there is a filter called upload_dir which is called when the upload directory is configured.upload_diradd_filter(
  'upload_dir',
  function ($dirs) {
    if ( is_multisite() ) {
      $dirs['baseurl'] = network_site_url('/wp-content/uploads');
      $dirs['basedir'] = ABSPATH . 'wp-content/uploads';
      $dirs['path'] = $dirs['basedir'] . $dirs['subdir'];
      $dirs['url'] = $dirs['baseurl'] . $dirs['subdir'];
    }

    return $dirs;
  },
  999
);add_filter(
  'upload_dir',
  function ($dirs) {
    if ( is_multisite() ) {
      $dirs['baseurl'] = network_site_url('/wp-content/uploads');
      $dirs['basedir'] = ABSPATH . 'wp-content/uploads';
      $dirs['path'] = $dirs['basedir'] . $dirs['subdir'];
      $dirs['url'] = $dirs['baseurl'] . $dirs['subdir'];
    }

    return $dirs;
  },
  999
);WARNING: While this will give you a shared uploads directory, it will have a severe side effect. If you delete one of your WordPress Multisites, it will delete all of the media in your custom upload path as well. This is because WordPress will clean up after itself and see your upload path as a custom one.WARNING: ",371
WordPress Multisite Domain Issues? You Might Be Forgetting to Call restore_current_blog,"Recently, I created a WordPress Multisite installation on Amazon AWS Lightsail. I had this bizarre issue where my child sites were working, but whenever I would try creating posts or pages, the new button would take me to the root domain. Chalk this one up to stupidity (on my part).It was a perplexing issue because it was acting like I was in the admin panel for the root domain, not the child site. It was even showing my post counts for the root domain (but not posts). Going to the permalinks screen was showing the root domina, everything was a disaster.I could not work out what was going on. I debugged my .htaccess, and to an extent, my wp-config.php file. Nothing seemed to fix the issue. The one thing I could see was disabling my theme fixed the problem.I realise this is quite specific, but I was calling switch_to_blog(1) inside a function call. The rookie error I made was forgetting to call restore_current_blog at the end of the function. This straightforward mistake caused an issue so significant. I pulled apart half of my site, trying to fix it.So, the moral of the story is, if you’re having Multisite domain issues and you’ve checked your .htaccess and wp-config.php are not the culprit, this might be an avenue to explore.",312
How to Remove Duplicate Items From an Array in JavaScript (the Easy Way),"Languages such as PHP have methods for de-duplicating arrays, but Javascript strangely does not. In PHP, you can write. array_uniqueAnd this function will remove duplicates.array_uniqueThere are so many solutions online you will find for this problem. They mostly centre around using filter and index checks, others recommend libraries like Lodash.filterForget installing libraries or writing convoluted functional programming inspired solutions. You can do this:const arr = [1, 2, 3, 9, 1, 6, 4, 3, 9, 2, 4, 6];
const unique = [...new Set(arr)];const arr = [1, 2, 3, 9, 1, 6, 4, 3, 9, 2, 4, 6];
const unique = [...new Set(arr)];If you were to console log the contents of unique using the spread operator, we convert it back to a unique array. You would get an array containing: [1, 2, 3, 9, 6, 4] — the Set object is creating for storing unique collections, not only arrays but other types of data too.unique[1, 2, 3, 9, 6, 4]Set ",233
Creating a Custom WordPress REST API Endpoint That Can Accept Spaces,"The WordPress REST API is one of my favourite features in WordPress. The ability to create custom endpoints for getting data and other facets of building in WordPress makes life so much easier.Recently, while building a custom endpoint that allowed for some custom post type content to be searched, I encountered a snag. register_rest_route( 'raw-queue/v1', '/queue/(?P<name>[a-zA-Z0-9-]+)/(?P<results>[\d]+)/(?P<page>[\d]+)/(?P<source>[a-zA-Z0-9-]+)/(?P<term>([a-zA-Z])+)', array(
  'methods' => 'GET',
  'callback' => 'rest_get_news_stories_from_category',
  'permission_callback' => '__return_true'
) );register_rest_route( 'raw-queue/v1', '/queue/(?P<name>[a-zA-Z0-9-]+)/(?P<results>[\d]+)/(?P<page>[\d]+)/(?P<source>[a-zA-Z0-9-]+)/(?P<term>([a-zA-Z])+)', array(
  'methods' => 'GET',
  'callback' => 'rest_get_news_stories_from_category',
  'permission_callback' => '__return_true'
) );For singular search terms, it worked well. However, once I searched for words like “basketball player”, I would get the dreaded 404 error.It turns out you need to tell WordPress in your regular expression path to allow the %20 character (which is a space). By default, spaces are ignored (as are other characters).%20register_rest_route( 'raw-queue/v1', '/queue/(?P<name>[a-zA-Z0-9-]+)/(?P<results>[\d]+)/(?P<page>[\d]+)/(?P<source>[a-zA-Z0-9-]+)/(?P<term>([a-zA-Z]|%20)+)', array(
  'methods' => 'GET',
  'callback' => 'rest_get_news_stories_from_category',
  'permission_callback' => '__return_true'
) );register_rest_route( 'raw-queue/v1', '/queue/(?P<name>[a-zA-Z0-9-]+)/(?P<results>[\d]+)/(?P<page>[\d]+)/(?P<source>[a-zA-Z0-9-]+)/(?P<term>([a-zA-Z]|%20)+)', array(
  'methods' => 'GET',
  'callback' => 'rest_get_news_stories_from_category',
  'permission_callback' => '__return_true'
) );Notice the |%20 part in the route pattern? That tells WordPress we are okay with spaces in our route, and it will no longer break. Now, search terms like basketball%20player will work with our route. You can add in additional URL characters as you need them.|%20basketball%20playerNow, last and most importantly, inside of your route callback, you need to make sure you use urldecode to remove the characters from the URL.last and most importantlyurldecodeurldecode($request->get_param('term'));urldecode($request->get_param('term'));",580
Don’t Back Projects on Kickstarter Until Supply Chains Are Fixed,"The world might be attempting to get back to normal under the “new normal” label, but supply chains are still absolutely busted. As we head towards Christmas 2021, things show no sign of going back to normal in the world of logistics and supply chains.Postal service providers like USPS and Australia post are struggling to deal with the large volumes of mail. USPS temporarily suspended postage to 21 countries, including Australia.  Australia posttemporarily suspended postage to 21 countriesChina is amid a power shortage. The cost of shipping has skyrocketed as much as 500% and still climbing. There is a global chip shortage. There is a raw materials shortage on plastics and other materials. The basic building blocks of modern civilisation have been fractured.a power shortagehas skyrocketed as much as 500% and still climbingglobal chip shortageraw materials shortageRight now, you can’t even go into a car dealership and buy a car right off the floor anymore. Many retail stores have empty shelves, struggling to refill them to meet consumer demand. Not even Ikea is immune to the shortages.Not even Ikea is immune to the shortagesDespite the stresses on these supply chains, creators are still creating Kickstarter campaigns. Most of these campaigns are physical products such as board games, miniatures and electronic gadgets.The reality is that most of the projects on Kickstarter right now will not meet their projected delivery time frames. Despite what creators tell you, they will not meet their project timeframes (not that many projects ever do).I’ve been waiting on some projects I backed at the start of the year. Projects that were already in the final design stages and mostly just needed to be manufactured. Some are shipping now, but only in limited batches.Take the estimated delivery date of most projects on Kickstarter, and add at least six months. With Christmas coming up, already stressed supply chains are going to be pushed to the brink. Allegedly this Kickstarter project for a 4K dashcam will mass-produce units in October and then start shipping in November, which sounds like a complete fantasy.this Kickstarter project for a 4K dashcamIf you are willing to wait, you will avoid disappointment. As much as I want to support creators in these trying times, the unprecedented global conditions mean I will be postponing any investment into Kickstarter projects until 2022.",602
Querying WordPress Multisite Sites With Meta Queries,"WordPress documentation is usually robust. However, recently I had a use case where I needed to query sites by custom meta values.In WordPress 5.1, the WP_Site_Query was introduced to allow a faster way to query sites in a WordPress Multisite setup without using the old hack of needing to get all ID values using get_sites and then inside of a loop using switch_to_blog to query the sites and get values.get_sitesswitch_to_blogYou can find documentation for individual meta query functions like get_site_meta but very little documentation on the WP_Site_Query class. At best, you’ll come across this Make post from 2019.get_site_metathis Make post from 2019Meta values on sites allow you to categorise them and have configuration values unique to that site. In my use case, I need to be able to group sites by country. If I wanted to say, “Query all sites that are for Australia” I need a list of sites that match the meta_key country value of Aurelia.We already know you can query posts and taxonomies using meta_query and tax_query — but you can also query sites using meta queries for sites as well. It turns out, if you dig into the source code for the WP_Site_Query class, it supports a lot more than the light documentation suggests.meta_querytax_querydig into the source code for the WP_Site_Query class        // WP_Site_Query arguments
        $args = array(
            'meta_query' => array(
                array(
                    'key'   => 'country',
                    'value' => 'australia'
                )
            )
        );

        // The Site Query
        $site_query = new WP_Site_Query( $args );        // WP_Site_Query arguments
        $args = array(
            'meta_query' => array(
                array(
                    'key'   => 'country',
                    'value' => 'australia'
                )
            )
        );

        // The Site Query
        $site_query = new WP_Site_Query( $args );Now, assuming that our site has a meta key of country and we find one or more sites with Australia as the value, we’ll get back a list of sites.Like you can with WP_Query, you can also have multiple meta queries, relationships and other useful query vars to query your sites. The only limitation is the metadata that you have.",569
Get Advanced Custom Field Options by Site ID in WordPress Multisite,"Out of the box, Advanced Custom Fields offers immense power but provides no functions to work with WordPress Multisite sites. Fortunately, WordPress makes it easy to work with multisite sites.Throw the following function into your theme functions.php file and call it at will. Just provide the field name and site ID, the function will get your value from that site (if there is a value and field exists).functions.phpfunction get_acf_site_option($field_name, $site_id) {
    switch_to_blog($site_id);
  
    $value = get_field($field_name, 'option');
  
    restore_current_blog();
  
    return $value ? $value : null;
}function get_acf_site_option($field_name, $site_id) {
    switch_to_blog($site_id);
  
    $value = get_field($field_name, 'option');
  
    restore_current_blog();
  
    return $value ? $value : null;
}",207
The Most Efficient Way to Bulk Delete Things in WordPress,"What is the most efficient way to delete thousands of posts, custom post types, media attachments and other things in WordPress? The answer is: not through the admin panel.If you have ever tried deleting a couple of hundred (or so) items from the WordPress admin panel before, then you would know if the request doesn’t timeout, you’ve won the WordPress lottery. And, if it does timeout, you have to keep spamming the refresh button until it works.There is a better way. The WP CLI is often overlooked, but it’s invaluable for managing large amounts of data in a WordPress site. In my case, I had about 15,000 broken image attachment references and 6000 custom post type posts to remove.By calling the wp delete command and passing in a sub-call to post list, we can get the ID’s of every post of any type and then delete. We use the --force flag to tell WordPress to permanently delete them and not just send them to the trash.wp delete--forcewp post delete $(wp post list --post_type='my_post_type' --format=ids) --forcewp post delete $(wp post list --post_type='my_post_type' --format=ids) --forceThe most remarkable thing of all? WordPress media items are just posts of type attachment so, if you want to remove all attachments, you can do this:wp post delete $(wp post list --post_type='attachment' --format=ids) --forcewp post delete $(wp post list --post_type='attachment' --format=ids) --forceThe WP CLI is invaluable to level up your WordPress workflow and development. I highly recommend you leverage it, especially for heavy tasks where plugins and the UI can’t handle it.WP CLI",397
My Experience Writing a Long-Running PHP Script to Parse News Content From the Associated Press News API,"Filed under: super-specific use case with hints of generality for those wanting to write long-running PHP scripts.Filed under: super-specific use case with hints of generality for those wanting to write long-running PHP scripts.For a little while now, I have been building a site with WordPress that consumes news content from the Associated Press news API and then stores the news content in WordPress.My first iteration of the ingest engine with PHP worked quite well, but I encountered dreaded NGINX server timeouts and other issues.You see, with the Associated Press API, it works for the feed request a little like this.Make request to feed endpointSome items might be returned in the first requestParse items (if any) which requires making a separate request for an NITF fileIf there is a next_page link property, make a request to itMake request to feed endpointSome items might be returned in the first requestParse items (if any) which requires making a separate request for an NITF fileIf there is a next_page link property, make a request to itHow the AP API is designed is that requests are long-running. A request remains open using a long polling feature for about 15 seconds before requiring you to follow the following page link (if one exists). The idea is that you perpetually connect to the API (especially if there is breaking news).The nature of PHP and web servers is you won’t be able just to run a script and expect it always to run reliably. PHP isn’t a language that lends itself to long-running processes, nor are servers like Apache or Nginx, but it can work with some patience. A lot of the solutions you will find for this problem go back as far as 2010.My solution involves creating four shell scripts for ultimate control and using native cron functionality.A script to run our PHP application and create a process for itA script to check on our PHP application to ensure it is running (start it if it’s not)A script to reset our PHP applicationA script to stop our PHP applicationA script to run our PHP application and create a process for itA script to check on our PHP application to ensure it is running (start it if it’s not)A script to reset our PHP applicationA script to stop our PHP applicationIt is not a requirement you use shell scripts. You could very much call commands directly via your crons, but shell scripts will allow you to cleanly maintain your functionality and do status checks and so on.The only package we are going to require to achieve our constant long-running PHP scripts is nohup. I am using Debian, and by default, nohup comes installed with the operating system. If you don’t have nohup, use your respective package manager to install it.The first shell script we create is the one that starts everything.The first shell script we create is the one that starts everything.#!/bin/bash
nohup /opt/bitnami/php/bin/php -q /opt/bitnami/wordpress/fetch-news.php >/dev/null 2>&1 &#!/bin/bash
nohup /opt/bitnami/php/bin/php -q /opt/bitnami/wordpress/fetch-news.php >/dev/null 2>&1 &We are running our PHP script using nohup and forcing it to be in the background. We also suppress PHP errors and output using a combination of -q as well as /dev/null 2>&amp;1 -q/dev/null 2>&amp;1Now, let’s create a shell script that allows us to stop the process.Now, let’s create a shell script that allows us to stop the process.#!/bin/bash
PID=`ps -eaf | grep '/opt/bitnami/wordpress/fetch-news.php' | grep -v grep | awk '{print $2}'`
if [ """" !=  ""$PID"" ]
then
    echo ""killing $PID""
    kill -9 $PID
else
    echo ""not running""
fi#!/bin/bash
PID=`ps -eaf | grep '/opt/bitnami/wordpress/fetch-news.php' | grep -v grep | awk '{print $2}'`
if [ """" !=  ""$PID"" ]
then
    echo ""killing $PID""
    kill -9 $PID
else
    echo ""not running""
fiWe are checking if our PHP script can be found in the running processes or not. It then tries to determine if it is, then kills the process using kill and the process ID.killAnother script for checking if our script is running or not.Another script for checking if our script is running or not.#!/bin/bash
PID=`ps -eaf | grep '/opt/bitnami/wordpress/fetch-news.php' | grep -v grep | awk '{print $2}'`
if [ """" !=  ""$PID"" ] 
then
    echo ""Parser running on $PID""
else
    echo ""Not running, going to start it""
    cd /opt/bitnami/wordpress/
    ./run-news-parser.sh
fi#!/bin/bash
PID=`ps -eaf | grep '/opt/bitnami/wordpress/fetch-news.php' | grep -v grep | awk '{print $2}'`
if [ """" !=  ""$PID"" ] 
then
    echo ""Parser running on $PID""
else
    echo ""Not running, going to start it""
    cd /opt/bitnami/wordpress/
    ./run-news-parser.sh
fiWhen I run my cron, this is the script that I call. It will check if my parser is running or not. If no process ID can be found, it’s not running and, therefore, needs to be started using our run-news-parser.sh script.run-news-parser.shSome improvements here could be having a maximum number of attempts to start before altogether bailing and maybe sending you a notification something went wrong (remote API went down, credentials expired or revoked, etc.).And finally, a shell script that can restart our script.And finally, a shell script that can restart our script.#!/bin/bash
PID=`ps -eaf | grep '/opt/bitnami/wordpress/fetch-news.php' | grep -v grep | awk '{print $2}'`
if [ """" !=  ""$PID"" ]
then
    echo ""killing $PID""
    kill -9 $PID
else
    echo ""not running""
fi

echo ""Starting again""
/bitnami/wordpress/run-news-parser.sh#!/bin/bash
PID=`ps -eaf | grep '/opt/bitnami/wordpress/fetch-news.php' | grep -v grep | awk '{print $2}'`
if [ """" !=  ""$PID"" ]
then
    echo ""killing $PID""
    kill -9 $PID
else
    echo ""not running""
fi

echo ""Starting again""
/bitnami/wordpress/run-news-parser.shThis script looks similar to that of the ones that came before it. It’s a combination of the start and stop scripts. If it finds a process, it will kill it and then restart.This approach might not be pure and some might laugh at how simple it is, but it works and will continue to work well into the future. I am probably going to rewrite these scripts to use Node.js, but for now, it’s something I will keep using because it works (even if a little hacky).",1545
How to Work With DATETIME Values in JavaScript,"Dates in Javascript have always been a PITA. It is one of the reasons that libraries like MomentJS reigned supreme for so long. Things are complicated, especially if you’re dealing with different timezones. Fortunately, working with simple DATETIME values has never really been a problem.What prompted this post was that I am working with WordPress a lot at the moment, and all dates in WordPress are MySQL DATETIME formatted dates. Say you have a DATETIME value that looks like this: 2021-10-05 00:00:00 — you can use the Javascript Date object on this because it’s a valid date value.In your developer tools in the browser, you can pass this value into Date and it will spit out something that looks like the below:DateFor brevity, here is the code itself:For brevity, here is the code itself:new Date('2021-10-05 00:00:00');new Date('2021-10-05 00:00:00');Get the UNIX timestampSo, you have your date object. You can now easily convert the value to a UNIX timestamp using getTime(). This allows you to do things like sort elements by integers instead of date objects. You can do safe comparative checks and use the .sort method.getTime().sortHere is the code:Here is the code:Math.floor(new Date('2021-10-05 00:00:00').getTime() / 1000);Math.floor(new Date('2021-10-05 00:00:00').getTime() / 1000);We have to divide this value by 1000 because the Javascript getTime() will return milliseconds since the UNIX epoch. Unix time is in seconds, so dividing the milliseconds gives us the seconds representation.getTime()The reason we use Math.floor here is quite important too. Some examples online will use Math.round which is not accurate.  We want the actual elapsed seconds and using floor will not give us that. Because dividing by 1000 can sometimes result in fractional numbers, we use this to get a nice clean value.Math.floorMath.roundChances are, if you’re reading this, you’re working with dates that are coming from a database like MySQL. Luckily, Javascript makes working with these dates relatively painless. Just don’t mention timezones. That conversation is for another post entirely.",524
The Whole of Twitch (and a bunch of other affiliated sites) Has Been Leaked,"If you thought the CD Projekt Red leak of Cyberpunk 2077 and other source code was bad, get a load of this latest leak.Twitch just got hacked entirely, and the entirety of its source code, internal repositories, financial payout information and absolutely everything you can think of has been taken and put online in a 128GB torrent over on 4chan.put online in a 128GB torrent over on 4chan.The anonymous individual or group says, “we have completely pwned them,” and let’s be honest, they’ve well and truly pwned Twitch here. What a massive leak. Some heads are going to roll over in the IT department over this. I am curious how this even happened.I speculate that they probably use something like GitHub Enterprise and someone found their way into their organisation (maybe someone with high permissions got their details swiped). Either way, this is hands down one of the largest leaks in recent memory.The anonymous poster writes:Their community is also a disgusting toxic cesspool, so to foster more disruption and competition in the online video streaming spaceTheir community is also a disgusting toxic cesspool, so to foster more disruption and competition in the online video streaming spaceI did have a good laugh at the endnote: Jeff Bezos paid $970 million for this, we’re giving it away FOR FREE.Most of the people who download this will do it out of curiosity. Not many people out there would take this and make it work for their own needs. So, I doubt we’ll see a bunch of Twitch clones popping up as a result.What makes this leak even crazier is that the leak isn’t just Twitch source code. It’s any Twitch corresponding application and tool. There is source code for the sites IGDB and CurseForge in there as well.This is a treasure trove of information and code. Perhaps, one of the most interesting tidbits is the mention of an unreleased Steam competing service from Amazon Game Studios.And when you thought this leak couldn’t get more disastrous for Amazon, this is only part one. There are other parts to this. If this is only part one, holy moly, what is in part two and possibly other parts beyond that?If you’re a Twitch user, you might want to change your password and enable two-factor authentication, like right now.",561
A Quick Review of Windows 11: Should You Upgrade?,"If Windows 11 were a product with a slogan, it would be “same Windows, now 50% less ugly.” With Windows 11 finally, upon us, the question on the lips of many is: is it worth the upgrade?Instead of starting from scratch, Microsoft took Windows 10 (which was already quite good), pulled some levers, put some finesse on the UI and modernised a few parts of the 35-year-old operating system (which was starting to show its age).In the short time I have been using Windows 11, I mostly like it. Is it a groundbreaking operating system? Not really. They could have released this as a significant update for Windows 10, but I can understand the new version number given the UI is different.The first thing you will notice with Windows 11 (especially if you’re upgrading from Windows 10) is the UI isn’t flat like Windows 10. Application windows and buttons have rounded corners. The taskbar now resembles that of macOS with its centred icons.That’s the thing that takes the most getting used to. I am a Mac user, but I am primarily Windows in my day to day development activities. One day in, and I am still instinctively trying to click the far left of the bottom bar to get to the start menu. The muscle memory I’ve formed with Windows 10 shows when I interact with the taskbar.The new taskbar works best on widescreen monitors (mine is 34″)While Microsoft has added the ability to move the icons back to the left, I will ride things out a little longer and see if I get used to the new positioning of the icons. I have always been a fan of how macOS has the taskbar down the bottom, so eventually, things will be fine once I unlearn Windows 10.One addition that will be useful for those who forgot it existed is hovering over the maximise window button of an application will now present screen snapping options.This feature existed in Windows 10, but apparently, not everyone knew it existed. It’s user-friendly additions like these that make Windows 11 feel fresher than 10.There’s a sleek new File Explorer.There’s a sleek new File Explorer.This is one of my favourite changes in Windows 11: the redesigned File Explorer. The File Explorer in Windows 10 was the same old File Explorer from prior versions of Windows, and the icons which look like they were lifted out of Windows 95 are now nowhere to be seen.Gone is that terrible ribbon toolbar, replaced by simple buttons and icons, making the File Explorer less bloated and cleaner. The File Explorer has been long overdue for an overhaul, and I would love to see Microsoft eventually take this even further.ConclusionConclusionPoke around the operating system long enough, and you’ll discover Microsoft has tweaked quite a few parts of the operating system. While on the surface, it might appear to be Windows 10 with some nice new window dressing, it’s a step in a new direction. Design matters and I am glad Microsoft finally sees that.The subtle animations when you open applications, the noticeable speed improvements in Microsoft Edge and revised privacy settings make it a worthy upgrade. Did I mention it’s free? If you like Windows 10, you will like Windows 11.",781
How to Add Infinite Scrolling to a HTML Element With Vanilla JavaScript,"There is a Javascript library for almost anything. But sometimes, it’s good to code your solutions to specific problems where the resulting code will be lighter, and you will feel more rewarded as a result.When it comes to infinite scrolling in Javascript, many examples, you will find online relate to scrolling the main window (which is a common thing). But what if you have an overflowing DIV that you want to add infinite scrolling on? The logic is the same.We determine the maximum scroll height by subtracting the clientHeight from the elements scrollHeight. We then obtain the current scrollTop value (distance scrolled from top of the container). Finally, we add the scroll distance and our buffer and determine if it equals the maximum scroll (end of the container) or exceeds it.const wrapper = document.querySelector('#items');
let scheduledAnimationFrame = false;
let bufferValue = 80;

function checkScrollPosition() {
    const maxScroll = wrapper.scrollHeight - wrapper.clientHeight;
    const currentScrollValue = wrapper.scrollTop;
    const isElementScrolledToTheBottom = (currentScrollValue + bufferValue) >= maxScroll);

    if (isElementScrolledToTheBottom) {
     // Call our data loading function in here
    }
}

function onScroll() {
    if (scheduledAnimationFrame) {
        return;
    }
  
    scheduledAnimationFrame = true;
  
    requestAnimationFrame(() => {
        checkScrollPosition();
        scheduledAnimationFrame = false;
    });
}

wrapper.addEventListener('scroll', onScroll);const wrapper = document.querySelector('#items');
let scheduledAnimationFrame = false;
let bufferValue = 80;

function checkScrollPosition() {
    const maxScroll = wrapper.scrollHeight - wrapper.clientHeight;
    const currentScrollValue = wrapper.scrollTop;
    const isElementScrolledToTheBottom = (currentScrollValue + bufferValue) >= maxScroll);

    if (isElementScrolledToTheBottom) {
     // Call our data loading function in here
    }
}

function onScroll() {
    if (scheduledAnimationFrame) {
        return;
    }
  
    scheduledAnimationFrame = true;
  
    requestAnimationFrame(() => {
        checkScrollPosition();
        scheduledAnimationFrame = false;
    });
}

wrapper.addEventListener('scroll', onScroll);Inside of checkScrollPosition is where you would call any code, such as making requests to a server to load some data. It is also worth noting that you might not even need to use requestAnimationFrame I am using it because of a force of habit, but these days you can implement scrolling solutions like this without using it.checkScrollPosition",649
How to Upgrade to Windows 11 if You Don’t Want to Wait for the Upgrade Prompt,"Windows 11 is finally here, and like Windows 10, it will be a free upgrade for Windows 10 users. However, the rollout will be slow and staggered despite Windows 11 releasing on October 5, 2021.This means for many Windows 10 users. It could take months and upwards of mid-2022 before you get the upgrade popup (if your machine is compatible).If you are like me and live on the edge, you are most likely looking to upgrade now. The good news, you don’t have to wait for Microsoft to tell you that you can get Windows 11. There is a trick that currently allows you to upgrade without having to wait.Head over to the Windows 11 download section on the Microsoft website and look for the Windows 11 Installation Assistant (the first item for me on this page).over to the Windows 11 download sOnce you’ve downloaded the assistant, it will guide you through the download and setup phase for Windows 11.That’s it. No waiting to be invited to upgrade, you can upgrade now. Fair warning, given Windows 11 is new, some software might be incompatible and there could be driver issues with certain hardware. Modern graphics cards and devices which have certified Windows 11 drivers are fine.",295
The Only Long-Term Casualties of the Pandemic Are Companies Who Push for Workers to Return to the Office,"Some of you already know my stance on remote work vs the office. As the pandemic rages on, the world is trying to return to a sense of normality.Name a company that went out of business because its employees had to work remotely. Productivity of employees can be measured in output, not bums in seats in an office. As you’ll soon learn, remote work is no longer a Silicon Valley startup perk to attract talent. It’s now expected by many.If you are reading this and you weren’t already being afforded the privilege of working from home, chances are, in 2020, the company you work for asked you to stay home (if your job allows you to work remotely, that is).As lockdowns and uncertainty around vaccines raged on, we did our part, and we stayed home. Not everyone liked working from home. It was a complicated situation (especially if you had kids and couldn’t go to school or kindergarten).I was fortunate to have already worked remotely three days per week, the other two spent in the office before the pandemic forcing us out of the office. These office days were for catching up.For many, working from home was new. Not many companies offered remote work (also known as flexible work) before COVID-19 ground the world to a halt.As countries begin to open up and restrictions are eased as we head into 2022, some companies are eager to get workers back into the office. Other companies have started to embrace this new remote work world we live in, acknowledging that COVID-19 wasn’t the first and certainly won’t be the last virus.One such company is Australian tech darling Atlassian, with a newly introduced policy to offer indefinite remote work (to those who want it). Other companies with similar “work from home forever” policies include Twitter, not forcing workers back into the office. Dropbox famously announced in 2020 they were going to be a virtual-first company.a newly introduced policy to offer indefinite remote workTwitterthey were going to be a virtual-first companyWhy do we need to be in an office?Think about it. We have already been working remotely long before COVID hit. If the company uses Slack or Microsoft Teams, you already work remotely.As a developer (and many other developers I have worked with), I like to listen to music through a pair of noise-cancelling headphones while working. For the last few years in my career, if anyone has needed anything from me, they most likely would “Slack me.”We’ve been working remotely in offices already. When I point this fact out to people, some are blown away when they realise we sit in our offices mostly in isolation, and the only time we interact is in meetings, lunch or coffee.Not everyone can work remoteI think it’s important to acknowledge that for some, remote work is not an option, not even hybrid remote work. Some people I know struggle to be productive at home and require the rigidity of an office to get results.The broader point being made here is that companies should give their employees the option to choose how they want to work. For some, they will prefer being in an office, for others, being at home. At the very least, companies should office a minimum of three days remote in a hybrid approach.Adapt or dieDespite the changing landscape of work, some companies are refusing to embrace remote work. For some, it’s a case of being unable to micromanage and surveil their employees when they are not in the office.The office is an outdated concept. I believe the future of work was already slowly changing before the pandemic. COVID-19 just came along and sped things up by a few years. But, ultimately, where we are right now is where we were always heading pandemic or not.Now that borders have been removed, some companies who have embraced remote work can hire beyond their local talent pool. Companies should be concerned. No longer does their geographical or timezone give them an advantage. Many of us can work anywhere now.Your competitors are everywhere. For every company that refuses to adapt and offer some form of remote work as an option, another company will and when it comes to hiring, this could be the one thing that causes you to lose out on good talent.",1044
Quora’s Slow Descent Into Nothingness,"A few short years ago, Quora was the darling of the tech world. A place where you could ask questions and get answers from people at the top in their chosen field. CEO’s, leaders, heavy hitters.The fun part of Quora was how accessible skilled experts were, who would otherwise not be easy to reach. You could ask pilots questions about flying. You could ask police officers about their job. I mean, even astronauts were answering questions on Quora.even astronauts were answering questions on QuoraQuora has been around since 2010 and was once poised to take over the world.Fast forward to late 2021, and Quora appears to be slowly fading into the ether, and nobody seems to notice (except maybe Quora themselves and their unfortunate investors).One of the first signs of trouble for Quora revealed itself in 2020 when they laid off a bunch of staff. Fair shake of the sauce bottle here, a lot of companies laid people off in 2020 because of the pandemic, but the problems Quora have predated the pandemic.in 2020 when they laid off a bunch of staffAnd, I am kind of sad about the sorry state of Quora. I have been a member since 2011, and in that time, I accumulated a lot of views on my answers.I have been a member since 2011As you can see, I’ve amassed over 2 million views on Quora. Rookie numbers compared to others, but considering my engagement has varied over the years, still interesting to see.But, what kind of pushed me away from Quora was how detached they became from what people wanted from the platform. The spaces feature, in particular, felt like a wrong move, and as you will see if you spend a few minutes on Quora, it is being used for spammy purposes.It became pretty clear that Quora was focusing on SEO optimisation over fostering content creation on the site. It is now fast becoming a Yahoo! Answers type platform where the quality has slowly degraded. Eventually, it’ll be the fodder of memes and niche subreddits if Quora does not do something to stop the haemorrhaging.I still check in to Quora from time to time, and some people still provide quality answers. Still, the missteps around monetisation features and incentives which promote people to spam have sent Quora into a downward spiral.",556
Is the Nintendo Switch OLED Worth It?,"Despite having had my Switch since 2017, I am still in love with the console. After a busy life and kids made my Playstation 4 obsolete because it was constantly downloading some massive patch, the instant-on nature of the Nintendo Switch appealed to me.Sure, the Switch is a little rough around the edges, and the screen isn’t the prettiest thing to look at, but it still looks good and performs well.Before Nintendo announcing the Nintendo Switch OLED edition, there was talk of a Nintendo Switch Pro. A beefed-up version of the Switch with a better screen, better battery and increased graphical performance. Well, the Switch Pro never came to be, and instead, we got the Switch OLED.I impulsively pre-ordered the Nintendo Switch OLED when it was announced. It was a $10 deposit, so no big deal. The other day (just a week out from release), I cancelled my pre-order. The time between ordering and shipping made me realise that I didn’t need the OLED, and it offered nothing, making it worthy of upgrading on a whim.If you own a Switch already, the differences between the new and shiny version and the v1 are not that great. The screen is 0.8 inches bigger and, most notably, it’s an OLED screen. For some, the OLED will be a warranted upgrade. Do you play your Switch in handheld mode that much?When travel was a thing (remember planes?), the Nintendo Switch was amazing on a plane. There’s nothing better than playing Mario Kart against others on a plane. But then COVID came along, and all of a sudden, travel was cancelled. I don’t play my Switch in handheld mode that much, and even when I do, the screen is good enough on the original console.I am disappointed that Nintendo didn’t create a revised version of the Switch, to be honest. We are four years into the Switch life, and while it has been a great console with some great games, some more graphical power and a few other improvements would have been appreciated.The answer is simple. If you don’t have a Switch already, the OLED seems like a no-brainer to buy. If you already have a Switch, unless you have money to burn or you’re a collector, there are very few reasons to upgrade. The internal hardware on both versions is the same (disappointingly).",555
How to Deploy a Site via SSH Using GitHub Actions,"I love GitHub Actions. They are so simple and powerful, allowing you to have your code deployment and source code in one location.I manage and deploy all of my sites using SSH (because it’s more secure), and over the years, I’ve adopted numerous deployment strategies. I adopted a Git strategy not too long ago where my server would pull down changes from Git, but it’s a flawed approach.Here is an actual GitHub Actions build file I use for a project. It’s a mixture of Node.js and WordPress. If your needs are not as complicated, your file will resemble a fraction of this.The first few parts, I think, are reasonably self-explanatory. Give our action file a name. Specify what triggers our build (in our case, a push to the master branch).masterInside of the jobs section, we get into the filling of our delicious GitHub Actions pie. We specify we want an Ubuntu Linux instance. I am managing my WordPress dependencies using Composer, so I am using the php-actions/composer action. I also defined I want Node.js version 14 installed (you can change this to suit your version needs).php-actions/composerYou probably don’t need libnotify-bin I needed it for some scripts I was running which write console output.libnotify-binBecause I have Node dependencies inside of my WordPress theme, I cd into the directory and then run npm install — I have a couple of Aurelia applications inside of this theme which I then install the dependencies for and subsequently build (for deployment).npm installname: Deploy Prod Apps

on:
  push:
    branches:
      - master

jobs:
  deploy:
    name: Deploy apps to prod
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2
      - uses: php-actions/composer@v6

      - name: Install Node.js
        uses: actions/setup-node@v1
        with:
          node-version: '14.x'

      - name: Install libnotify
        run: sudo apt-get install libnotify-bin

      - name: Install npm dependencies
        run: cd wp-content/themes/news-theme && npm install

      - name: Pinning Queue
        run: cd wp-content/themes/news-theme/platform-apps/pinning-queue && npm install && npm run build

      - name: Locality Selector
        run: cd wp-content/themes/news-theme/platform-apps/locality-selector && npm install && npm run build

      - name: Deploy
        uses: easingthemes/ssh-deploy@main
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY_PROD }}
          ARGS: >-  
            -rltgoDzvO
            --exclude='.git'
            --exclude='node_modules/'
          REMOTE_HOST: ${{ secrets.REMOTE_HOST_PROD }}
          REMOTE_USER: ${{ secrets.REMOTE_USER_PROD }}
          TARGET: /opt/bitnami/wordpress/name: Deploy Prod Apps

on:
  push:
    branches:
      - master

jobs:
  deploy:
    name: Deploy apps to prod
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v2
      - uses: php-actions/composer@v6

      - name: Install Node.js
        uses: actions/setup-node@v1
        with:
          node-version: '14.x'

      - name: Install libnotify
        run: sudo apt-get install libnotify-bin

      - name: Install npm dependencies
        run: cd wp-content/themes/news-theme && npm install

      - name: Pinning Queue
        run: cd wp-content/themes/news-theme/platform-apps/pinning-queue && npm install && npm run build

      - name: Locality Selector
        run: cd wp-content/themes/news-theme/platform-apps/locality-selector && npm install && npm run build

      - name: Deploy
        uses: easingthemes/ssh-deploy@main
        env:
          SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY_PROD }}
          ARGS: >-  
            -rltgoDzvO
            --exclude='.git'
            --exclude='node_modules/'
          REMOTE_HOST: ${{ secrets.REMOTE_HOST_PROD }}
          REMOTE_USER: ${{ secrets.REMOTE_USER_PROD }}
          TARGET: /opt/bitnami/wordpress/The SSH deployment stepThis is the part you probably want to know about (not the other life story nonsense). We create a new build step called “Deploy” and use the easingthemes/ssh-deploy action. We then pass in some environment variables to this.easingthemes/ssh-deploy- name: Deploy
  uses: easingthemes/ssh-deploy@main
  env:
    SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY_PROD }}
    ARGS: >-  
      -rltgoDzvO
      --exclude='.git'
      --exclude='node_modules/'
    REMOTE_HOST: ${{ secrets.REMOTE_HOST_PROD }}
    REMOTE_USER: ${{ secrets.REMOTE_USER_PROD }}
    TARGET: /opt/bitnami/wordpress/- name: Deploy
  uses: easingthemes/ssh-deploy@main
  env:
    SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY_PROD }}
    ARGS: >-  
      -rltgoDzvO
      --exclude='.git'
      --exclude='node_modules/'
    REMOTE_HOST: ${{ secrets.REMOTE_HOST_PROD }}
    REMOTE_USER: ${{ secrets.REMOTE_USER_PROD }}
    TARGET: /opt/bitnami/wordpress/The most important value here is SSH_PRIVATE_KEY this is your private key value, and you mustn’t publicly specify this in your file whatsoever. For actions, it is best practice to use secret values to pass into your GitHub Actions. I will assume you know how to generate SSH keys, register them and then copy the key. This section on the repo will point you in the right direction.SSH_PRIVATE_KEYThis sectionIf you get stuck on this step, make sure you’re using your private key and not your public key. Most of the issues you’ll face are related to not providing the correct key value.Secondly, you must exclude directories that you do not want to be copied. You don’t want to copy your .git directory, and you especially never want to copy a node_modules directory (have you ever seen how big this folder gets?).node_modulesThe REMOTE_HOST value is usually the IP address of your site. The REMOTE_USER is the username you would use when tunnelling into our server via SSH. The TARGET is where your files will be copied. You can see I am using a Bitnami WordPress image, so I make sure my files are deployed into the WordPress directory.REMOTE_HOSTREMOTE_USER is the username you would use when tunnellingTARGETBonus points: Post deploy commandsSadly, the ssh-deploy action does not support post deploy commands (the ability to run commands after a deploy is done). I have had to do this on a couple of occasions to fix file and folder permissions.ssh-deployFor post deploy commands, you can use the appleboy/ssh-action action to do this. It kind of looks similar to that of the above ssh-deploy.appleboy/ssh-actionssh-deploy      - name: Post Deploy
        uses: appleboy/ssh-action@master
        with:
            host: ${{ secrets.REMOTE_HOST_PROD }}
            username: ${{ secrets.REMOTE_USER_PROD }}
            key: ${{ secrets.SSH_PRIVATE_KEY_PROD }}
            port: 22
            script: | 
              sudo find /opt/bitnami/wordpress/wp-content/mu-plugins -type d -exec chmod 775 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/plugins -type d -exec chmod 775 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/themes -type d -exec chmod 775 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/mu-plugins -type f -exec chmod 664 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/plugins -type f -exec chmod 664 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/themes -type f -exec chmod 664 {} \; 
              sudo chmod 664 /opt/bitnami/wordpress/wp-config.php
              sudo chmod 775 /opt/bitnami/wordpress/wp-content/w3tc-config 
              sudo chmod 775 /opt/bitnami/wordpress/wp-content/cache      - name: Post Deploy
        uses: appleboy/ssh-action@master
        with:
            host: ${{ secrets.REMOTE_HOST_PROD }}
            username: ${{ secrets.REMOTE_USER_PROD }}
            key: ${{ secrets.SSH_PRIVATE_KEY_PROD }}
            port: 22
            script: | 
              sudo find /opt/bitnami/wordpress/wp-content/mu-plugins -type d -exec chmod 775 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/plugins -type d -exec chmod 775 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/themes -type d -exec chmod 775 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/mu-plugins -type f -exec chmod 664 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/plugins -type f -exec chmod 664 {} \; 
              sudo find /opt/bitnami/wordpress/wp-content/themes -type f -exec chmod 664 {} \; 
              sudo chmod 664 /opt/bitnami/wordpress/wp-config.php
              sudo chmod 775 /opt/bitnami/wordpress/wp-content/w3tc-config 
              sudo chmod 775 /opt/bitnami/wordpress/wp-content/cacheHere is a post-deploy build step that ensures all wp-content folders have 775 permission and the files inside are 664. The pipe on the first line allows us to run multiple script commands. If you have one, put it on a single line and don’t use a pipe.",2215
How to Parse Dates With Different Timezones in PHP (and convert them),"Recently I was tasked with processing some content from an API, the published dates and times were coming through with timezone values in the string. My dates looked like this: 2021-09-29T04:24:39Z2021-09-29T04:24:39ZIf you parse these using strtotime like I was and importing them into WordPress, even if your server timezone is configured correctly, the timezone will be wrong. In my situation, the dates and times were showing all hours of the morning.strtotimeThis is where the DateTime object comes in very handy. We create a new DateTime object using the provided value that we want to convert into our timezone.DateTimeDateTimeI then call the setTimeZone function on the DateTime object to convert my date object into the timezone for Sydney, Australia.setTimeZoneDateTimeFinally, we call the format function to get the date in our desired format.format$post_date = new DateTime($published);
$post_date->setTimeZone(new DateTimeZone('Australia/Sydney'));
$post_date = $post_date->format(""Y-m-d H:i:s"");$post_date = new DateTime($published);
$post_date->setTimeZone(new DateTimeZone('Australia/Sydney'));
$post_date = $post_date->format(""Y-m-d H:i:s"");It’s a remarkably simple and powerful way to convert date values between timezones. It took years for browsers to get this kind of power with dates and times (still, not perfect in Javascript land).",339
How to Get Authentication Working Using the Node WPApi Package,"If you are working with WordPress version 5 and up, you might be using the REST API. I love the in-built REST API WordPress provides, especially for creating applications on top of WordPress.The Node WPApi package makes this a breeze, especially when it comes to authentication based actions.Node WPApi packageMy first test with this package was creating a new post, and I got this error message:Sorry, you are not allowed to create posts as this user.I was confused at first because I entered the correct username and password for my WordPress installation. Well, as you will discover, WordPress won’t just allow you to perform authentication-based requests using your standard credentials.Once you are logged in, you need to go down to the application passwords section and generate a new password in your user profile section.Once you enter a descriptive label, you will be presented with a randomly generated password. You will not be able to retrieve this, so copy and paste it somewhere. You will then use your currently logged in username as your username value and your newly generated application password as your password (instead of the password you use when you log in).import WPAPI from 'wpapi';

const wp = new WPAPI({
    endpoint: 'https://mycoolwebsite.com/wp-json',
    username: 'user',
    password: 'lcSd JKgZG w49DE TbqZ A95eM 64HJ3a'
});import WPAPI from 'wpapi';

const wp = new WPAPI({
    endpoint: 'https://mycoolwebsite.com/wp-json',
    username: 'user',
    password: 'lcSd JKgZG w49DE TbqZ A95eM 64HJ3a'
});This is what your code would look like. I am using my generated application password but my standard username, and it works.",416
How to Remove WordPress Menu Items (Including Those Created by Plugins),"WordPress comes with a lot of stuff out of the box. Throw in some plugins, and it gets even noisier. Next minute, your menu sidebar in admin looks like that drawer in the kitchen you shove everything into (known in Australia as a crap draw).Now, you’ll want to hide some menu items for particular roles or even users (whatever floats your boat). The first hurdle can be knowing what each item is registered as (the slug matters). The core menu items like plugins and tools are easy enough, but custom menu items like “Custom Fields” and “Elite Video Player” — you need to know the slugs these were registered using.For this, we’ll “pretty print” the menu array (where all menu items live) to discover the slugs. Using this handy little snippet (because I can never remember how to do it using PHP), you’ll get a nice extensive list of menu items.this handy little snippetadd_action( 'admin_init', 'get_main_menu_items' );

function get_main_menu_items() {
    echo '<pre>' . print_r( $GLOBALS[ 'menu' ], TRUE) . '</pre>';
}add_action( 'admin_init', 'get_main_menu_items' );

function get_main_menu_items() {
    echo '<pre>' . print_r( $GLOBALS[ 'menu' ], TRUE) . '</pre>';
}This will add an enormous ugly printed array inside of your admin panel. What we are looking for is index 2 of the displayed values. This is the slug we need to provide to remove_menu_pageremove_menu_page<pre>Array
(
    [2] => Array
        (
            [0] => Dashboard
            [1] => read
            [2] => index.php
            [3] => 
            [4] => menu-top menu-top-first menu-icon-dashboard menu-top-last
            [5] => menu-dashboard
            [6] => dashicons-dashboard
        )

    [4] => Array
        (
            [0] => 
            [1] => read
            [2] => separator1
            [3] => 
            [4] => wp-menu-separator
        )

    [5] => Array
        (
            [0] => Posts
            [1] => edit_posts
            [2] => edit.php
            [3] => 
            [4] => menu-top menu-icon-post open-if-no-js menu-top-first
            [5] => menu-posts
            [6] => dashicons-admin-post
        )

    [5.46068] => Array
        (
            [0] => Raw Queue
            [1] => manage_options
            [2] => raw-queue
            [3] => Raw Queue
            [4] => menu-top menu-icon-generic toplevel_page_raw-queue
            [5] => toplevel_page_raw-queue
            [6] => dashicons-admin-generic
        )

    [6] => Array
        (
            [0] => Pinning Queue
            [1] => manage_options
            [2] => pinning-queue
            [3] => Pinning Queue
            [4] => menu-top menu-icon-generic toplevel_page_pinning-queue
            [5] => toplevel_page_pinning-queue
            [6] => dashicons-admin-generic
        )

    [10] => Array
        (
            [0] => Media
            [1] => upload_files
            [2] => upload.php
            [3] => 
            [4] => menu-top menu-icon-media
            [5] => menu-media
            [6] => dashicons-admin-media
        )<pre>Array
(
    [2] => Array
        (
            [0] => Dashboard
            [1] => read
            [2] => index.php
            [3] => 
            [4] => menu-top menu-top-first menu-icon-dashboard menu-top-last
            [5] => menu-dashboard
            [6] => dashicons-dashboard
        )

    [4] => Array
        (
            [0] => 
            [1] => read
            [2] => separator1
            [3] => 
            [4] => wp-menu-separator
        )

    [5] => Array
        (
            [0] => Posts
            [1] => edit_posts
            [2] => edit.php
            [3] => 
            [4] => menu-top menu-icon-post open-if-no-js menu-top-first
            [5] => menu-posts
            [6] => dashicons-admin-post
        )

    [5.46068] => Array
        (
            [0] => Raw Queue
            [1] => manage_options
            [2] => raw-queue
            [3] => Raw Queue
            [4] => menu-top menu-icon-generic toplevel_page_raw-queue
            [5] => toplevel_page_raw-queue
            [6] => dashicons-admin-generic
        )

    [6] => Array
        (
            [0] => Pinning Queue
            [1] => manage_options
            [2] => pinning-queue
            [3] => Pinning Queue
            [4] => menu-top menu-icon-generic toplevel_page_pinning-queue
            [5] => toplevel_page_pinning-queue
            [6] => dashicons-admin-generic
        )

    [10] => Array
        (
            [0] => Media
            [1] => upload_files
            [2] => upload.php
            [3] => 
            [4] => menu-top menu-icon-media
            [5] => menu-media
            [6] => dashicons-admin-media
        )Now, in the case of our custom plugin menu item for Elite Video Player, we see this:    [100] => Array
        (
            [0] => Elite Video Player
            [1] => manage_options
            [2] => elite_player_admin
            [3] => Elite Video Player Admin
            [4] => menu-top toplevel_page_elite_player_admin menu-top-first menu-top-last
            [5] => toplevel_page_elite_player_admin
            [6] => dashicons-video-alt3
        )    [100] => Array
        (
            [0] => Elite Video Player
            [1] => manage_options
            [2] => elite_player_admin
            [3] => Elite Video Player Admin
            [4] => menu-top toplevel_page_elite_player_admin menu-top-first menu-top-last
            [5] => toplevel_page_elite_player_admin
            [6] => dashicons-video-alt3
        )We can see index 2 has a value of “elite_player_admin” this is what we’ll pass to the remove_menu_page function.remove_menu_pagePutting it all togetherfunction theme_hide_menu_items() {
    remove_menu_page( 'options-general.php' ); // Remove WordPress settings menu
    remove_menu_page( 'elite_player_admin' );
}

add_action('admin_init', 'theme_hide_menu_items'), 999);
add_action('admin_menu', 'theme_hide_menu_items', 999);function theme_hide_menu_items() {
    remove_menu_page( 'options-general.php' ); // Remove WordPress settings menu
    remove_menu_page( 'elite_player_admin' );
}

add_action('admin_init', 'theme_hide_menu_items'), 999);
add_action('admin_menu', 'theme_hide_menu_items', 999);Now, here is where things get interesting. Most examples you will encounter only use the admin_init hook, but I also had to use the admin_menu hook to get items to remove. Even though I had a massive priority of 999, some items were stubborn and adding that second action hook removed them. Try just using one or the other first, my problem was probably something else and I used a machine gun to hammer in some nails.admin_initadmin_menuRemoving menu items based on userThis is a super-specific use case, but I had a site where only a particular user could see specific items. I could have used custom permissions/roles, but as you might be aware, that is a lot of messing around. I just wanted to remove menu items if the user wasn’t admin-user in your case, it might be one or more users.admin-userYou could rewrite this to check permissions/roles if you prefer, but this worked for my use case, even if it isn’t the right way to go about it.function theme_hide_menu_items() {
  	$user = wp_get_current_user();
  
    if ($user->user_login != 'admin-user') {
    	remove_menu_page( 'options-general.php' ); // Remove WordPress settings menu
    	remove_menu_page( 'elite_player_admin' );
    }
}

add_action('admin_init', 'theme_hide_menu_items'), 999);
add_action('admin_menu', 'theme_hide_menu_items', 999);function theme_hide_menu_items() {
  	$user = wp_get_current_user();
  
    if ($user->user_login != 'admin-user') {
    	remove_menu_page( 'options-general.php' ); // Remove WordPress settings menu
    	remove_menu_page( 'elite_player_admin' );
    }
}

add_action('admin_init', 'theme_hide_menu_items'), 999);
add_action('admin_menu', 'theme_hide_menu_items', 999);Hopefully, this helped and inspired you.",1994
A List of WordPress Gutenberg Core Blocks,"Believe it or not, finding this information in the official WordPress documentation was a nightmare. I had to go through the WordPress codebase itself to find these values.In a site I am working on, I wanted only to enable specific WordPress Gutenberg blocks. By default, my client will never use a lot of junk, like buttons and page separators that I wanted to disable.I am using the allowed_block_types hook to create an inclusion list where I specify what blocks I want to enable. You can also add your custom blocks to this list.allowed_block_typesadd_filter( 'allowed_block_types', 'theme_allowed_block_types', 10, 2 );

/**
    * Set allowed block types on pages/posts/CPTs
    */
function theme_allowed_block_types( $allowed_blocks, $post ) {
    if ( ! ( $post instanceof WP_Block_Editor_Context ) ) {
        return $allowed_blocks;
    }

    $allowed_blocks = array(
        'core/freeform',
        'core/video',
        'core/heading',
        'core/image',
        'core/gallery',
        'core/embed',
        'core/table',
        'core/list',
        'core/paragraph',
        'core/pullquote',
        'core/html'
    );

    return $allowed_blocks;
}add_filter( 'allowed_block_types', 'theme_allowed_block_types', 10, 2 );

/**
    * Set allowed block types on pages/posts/CPTs
    */
function theme_allowed_block_types( $allowed_blocks, $post ) {
    if ( ! ( $post instanceof WP_Block_Editor_Context ) ) {
        return $allowed_blocks;
    }

    $allowed_blocks = array(
        'core/freeform',
        'core/video',
        'core/heading',
        'core/image',
        'core/gallery',
        'core/embed',
        'core/table',
        'core/list',
        'core/paragraph',
        'core/pullquote',
        'core/html'
    );

    return $allowed_blocks;
}A fair warning. If you’re not using WordPress 5.8 or above, the above might cause issues for you.Here is a list of core Gutenberg block types that can be specified:Here is a list of core Gutenberg block types that can be specified:core/paragraphcore/imagecore/headingcore/gallerycore/listcore/quotecore/shortcodecore/archivescore/audiocore/buttoncore/buttonscore/calendarcore/categoriescore/codecore/columnscore/columncore/covercore/embedcore/filecore/groupcore/freeformcore/htmlcore/media-textcore/latest-commentscore/latest-postscore/missingcore/morecore/nextpagecore/preformattedcore/pullquotecore/rsscore/searchcore/separatorcore/blockcore/social-linkscore/social-linkcore/spacercore/subheadcore/tablecore/tag-cloudcore/text-columnscore/versecore/videocore/paragraphcore/imagecore/headingcore/gallerycore/listcore/quotecore/shortcodecore/archivescore/audiocore/buttoncore/buttonscore/calendarcore/categoriescore/codecore/columnscore/columncore/covercore/embedcore/filecore/groupcore/freeformcore/htmlcore/media-textcore/latest-commentscore/latest-postscore/missingcore/morecore/nextpagecore/preformattedcore/pullquotecore/rsscore/searchcore/separatorcore/blockcore/social-linkscore/social-linkcore/spacercore/subheadcore/tablecore/tag-cloudcore/text-columnscore/versecore/video",767
How to Copy Files Using the Copy Webpack Plugin (without copying the entire folder structure),"Despite the fact that I’ve been doing this whole front-end development thing for over a decade now, I still get caught up on silly things. Mostly build-related things trip me up.In a project using Webpack for the bundler, I needed to copy a folder from a node_modules directory and include the files in my bundle (don’t ask).node_modulesThe first thing I did was this:The first thing I did was this:new CopyWebpackPlugin({
  patterns: [
    { from: 'node_modules/@ia/qce/dist', to: 'content/qce' },
  ]}),new CopyWebpackPlugin({
  patterns: [
    { from: 'node_modules/@ia/qce/dist', to: 'content/qce' },
  ]}),Now, the problem here is that the copy-webpack-plugin will copy the entire path to the file. So, inside of my content/qce directory I had node_modules/@ia/qce/dist folders (the entire path). It will recreate the entire folder structure from the from value instead of just taking the contents (like we want).content/qcenode_modules/@ia/qce/distfromThis is why it always pays to read the docs. There is a context configuration property that will prevent this behaviour from occurring.it always pays to read the docscontext    new CopyPlugin({
      patterns: [
        {
          from: ""dist/"",
          to: ""content/qce"",
          context: ""node_modules/@ia/qce/"",
        },
      ],
    }),    new CopyPlugin({
      patterns: [
        {
          from: ""dist/"",
          to: ""content/qce"",
          context: ""node_modules/@ia/qce/"",
        },
      ],
    }),The bottom line here is to specify the context.",382
What happened to Deno?,"The ever-changing landscape of web development can be both cruel and kind. In May 2009, Ryan Dahl introduced Node.js to the world, and it didn’t take long before developers flocked to it like ants on a large pile of sugar. Ryan left the Node project in 2012.Node continued to increase in popularity and, front-end tooling started to build on Node.js, further propelling the popularity even further. To this day, most front-end tooling is built on Node, and Npm underpins the package ecosystem for both browser-based packages and Node.js packages.And, then in 2018, Ryan announced something new he had been working on called Deno. However, before Deno became available, Ryan famously did a JSConf EU talk called “10 Things I Regret About Node.js.”Deno

There is a good chance you have already seen this. If you haven’t, I highly recommend you watch it. In this talk, he raises several good points and downsides of Node.js that are systemic. Most notably, how heavy the node_modules directory becomes once you start installing packages.node_modulesYou’ve probably seen this highly accurate meme about Node.jsWhen Ryan did reveal Deno in 2018, the hype was inescapable. YouTube, Reddit, Medium, Twitter and even LinkedIn were all buzzing with fanfare and excitement. Voices were echoing the halls that Deno would kill Node.js. I was even excited when I heard what Deno was promising.One such feature that Deno was promising was out-of-the-box is native TypeScript support without a compile step (like you need with Node.js). But, its most anticipated and exciting feature was no package.json file and the ability to work with files (like you would include a script in a web page).package.jsonDespite the hype and promised features and Ryan (the original Node creator) behind it, Deno had its 15 minutes of fame, and you stopped hearing about it. Unless you have used Deno in the last year, you might have missed Deno shipping version 1.0 in May 2020.One of Deno’s most incredible features is running local scripts and being able to run remote scripts. In the getting started section, you can run the following: deno run https://deno.land/std/examples/welcome.tsWhen you run the above command, this is what it looks like run in a PowerShell window:Even though Deno is incredible and a step in the absolute right direction, I don’t know anyone using Deno in production, nor have I heard my developer circle talk about Deno (not in a long time). If you reframed the question from, “Are you using Deno” to “What do you think of Deno?” most developers who have worked with Node would tell you they think it’s incredible.Ryan, from the start, seems to have had particular goals in mind for Deno. But, Deno is not Node.js. There is no package system and no backwards compatibility with Node or Npm modules. What Deno offers is something quite vanilla that makes very few assumptions about what you want and need.Where Node.js has the upper hand is its ecosystem. There is an Npm package for everything ranging from web servers to encryption libraries and everything in between. If you use a framework or library with a server-side rendering (SSR) feature, the server will be Node.js based. Front-end tooling and bundlers (Webpack) all are built on top of Node too.To many developers who want to install a library and be done with it, this instant access to a treasure trove of packages is a HUGE advantage of Node.js and one that Deno cannot currently compete with right now. If Node and Deno started at the same time as one another, there is no doubt Deno would have won out.Right now, it seems most developers and companies are taking a wait and see approach to Deno. There is not much of an ecosystem. Most front-end tooling still uses Node.js and probably will continue to use Node for the foreseeable future.I believe that there will come a time when Deno will have the spotlight shone back on it. Right now, it is still early days in its development. With continual improvements and hopefully a package solution that appeals to developers who like the current npm install approach (maybe an independent effort like Npm originally was), Deno has so much potential for greatness.npm installThe consensus is that while Deno is fantastic and offers an incredible developer experience, it is not quite production-ready just yet.",1080
Working With the Children Decorator in Aurelia 2,"In Aurelia 1, we had the children decorator which allows you to query HTML elements inside of a component and then access them. In Aurelia 2, we have a children decorator, but it works differently.childrenchildrenWe are going to assume for this article that we have a bunch of custom elements being rendered called product-block which represent detail blocks for products in our custom element viewproduct-blockIn Aurelia 1@children('product-block') productBlocks;@children('product-block') productBlocks;Simple and effective.In Aurelia 2@children({ filter: (e: HTMLElement) => e.tagName === 'PRODUCT-BLOCK' }) productBlocks;@children({ filter: (e: HTMLElement) => e.tagName === 'PRODUCT-BLOCK' }) productBlocks;In Aurelia 2, we have to pass in a function with a filter property which we can then pass in a function that filters out the elements we need. This is akin to a Array.filter we loop through all elements inside of the component, check if the  tagName value equals that of what we want to query and happy days.filterArray.filterThe case is also important in this instance. Tag names are capitalised as per the spec, so when we query for our custom element, we have to write its tag name in capital letters.It is possible that in Aurelia 2, the simple use case like Aurelia 1 will be supported, but until then this is what you can do.",336
How to Add Typings to the Global TypeScript Window Object,"Does this error look familiar? Property ‘AddressFinder’ does not exist on type ‘Window & typeof globalThis’.ts(2339)Property ‘AddressFinder’ does not exist on type ‘Window & typeof globalThis’.ts(2339)There will come a time in TypeScript when you are not dealing with package-managed dependencies from Npm. These will come in the form of scripts added into the head or footer of your application. Furthermore, these global scripts might not have typings.You will also run into this error for numerous jQuery plugins and other libraries which work on the premise of modifying the global Window object.In my instance, I was using the AddressFinder API which uses a provided script you embed into your application. Adding in your own custom typings is a last resort. You should always check to see if there are officially provided typings for your package/script before proceeding to add in your own.Adding in your own custom typings is a last resortIn older versions of TypeScript you could just throw in an interface called Window and it would automatically add in your typing values to the Window object — times have changed and now the Window interface lives under the global scope instead.WindowWindowdeclare global {
    interface Window {
        AddressFinder: any;
    }
}By specifying declare global you’re saying whatever interfaces you specifying inside will be part of the global scope. Simply create a Window interface and then declare your properties inside. I am being naughty and using any to get myself out of a pickle, but will sort out proper typings later.declare globalWindowany",399
Xiaomi Roborock S5 Max Review,"In our household, we resisted buying a robot vacuum cleaner because, for such a long time, we held the belief that they didn’t do as good a job as a traditional vacuum and were more of a gimmick than a helping hand.Despite being the most technology literate person in my family, other family members (including my mother in law) purchased robot vacuums before we did. We kept resisting the need to buy something that we were sure would end up in a closet after a couple of weeks.We have two messy children, a six-year-old boy and a two-year-old girl. Collectively, they make a lot of mess. It has become so crazy with the lockdowns and school holidays (so many) that we decided enough was enough, and a robot vacuum might be a welcome helping hand.Even if it only picked up 50% of the mess the kids make, that’s 50% less mess my wife or me have to clean up.What this review is and isn’tUnlike more professional reviews, I will not talk about the vacuum’s kPa (suction pressure) or other meaningless technical metrics. We are a small family (two adults and two kids), currently in a single-story home comprised of tiled flooring in everyday and wet areas, carpet in the bedrooms.You will learn how good it cleans, the features we like, the things we don’t like, and other real-world things most people care about, like whether it works and the battery life offers.The research phaseWe immediately took the cheap no-name brands off the table. Our budget wasn’t going to be huge for this thing, but spending too little will result in a classic case of, “You get what you pay for” — this is the case with Kogan robot vacuum cleaners, even if you buy the more expensive Kogan models, they still seem to suck.Remember how I mentioned my mother in law buying a robot vacuum cleaner? It was a Kogan one, and it did such a poor job, she ended up sending it back (lol). We had a Kogan TV, and it barely lasted a year before it became a buggy messy, and we replaced it with a Samsung. “You get what you pay for”.Ultimately, we settled upon the Roborock S5 Max. Sure, it’s not the most affordable robot vacuum, but it’s also not the cheapest. Suppose you can get one on sale, even better. Feast your eyes on the glowing reviews for this thing on Product Review. A 4.6-star rating out of 487 reviews. — what’s the point in a cheap robot vacuum if you’ll end up having to manual clean anyway?Roborock S5 MaxFeast your eyes on the glowing reviews for this thing on Product Review. A 4.6-star rating out of 487 reviews.The S6 and even S7 were considered, although they are more expensive. The extra cost of these newer models seems to outweigh the benefits. You get better mopping functionality on the latest models, but that’s about it. The S5 Max has proven in tests that it’s the king of conventional vacuuming and the best value for money.Unboxing and setupSetting this thing up is easy. The large box comes with the robot, the dock and power lead, as well as the mopping attachment that clips on. From unboxing to plugging the dock in, it was about five minutes.The dock has some adhesive tape beneath, which will hold it in place. There is also a moisture mat (a piece of transparent plastic) that sticks to the floor with adhesive backing tape. Make sure these are correctly attached. The robot is quite strong and will shift it around if not affixed correctly.I downloaded the Roborock app from the Google Play store while it was charging and, once again, took a short period to get it installed and an account created.How noisy is the S5 Max?The “silent” vacuum setting is excellent for the night, but it can still make a bit of noise. The gentle setting (the lightest suction) is whisper quiet, but it doesn’t pick up much. I would avoid using the gentle setting unless you want to mop (it’ll just mostly push around dirt on this setting). If you have a sleeping baby or child that isn’t a heavy sleeper, the silent setting might still be too loud.The higher settings (turbo and Max) get a little noisier, but they are no louder than a conversation. My wife and I were able to sit at the dining table, which vacuumed in turbo mode around our feet as we drank our coffee. In a house at night with no T.V. or other noise, it might stand out, though.The mapping and navigation are amazingPlease resist the urge when you get your new robotic helper to start using it right away. Let it charge to 100% before you attempt to map your house, or it could prolong the process. Watching it roll around the house and seeing your home come to life in the app is both fantastic and addicting.There were some ants in the kitchen, so we dispatched Robi to get them for us.Maybe it’s because I am a nerd, but I loved seeing the S5 Max create a detailed map of our house. I equally had as much fun naming the rooms, creating zones and specifying boundaries. You can see the radar discovering door openings and room boundaries as it drives around. It’s pretty remarkable to watch.Before letting it do its first map creation, make sure that anything can be removed from the floor. I am talking chairs, clothes, toys and anything else that can be easily moved out of the way. Once the initial mapping is done, the robot will navigate around obstacles.As you might have also noticed in the screenshot above, the rooms are all named. Furthermore, once the robot has done its thing, you can edit the map. You can merge spaces. You can create new rooms by adding dividers in and so on. In the case of our dining table and living room, from a layout perspective, they’re one of the same, but our dining table area gets messy, so we made it its area.And the most incredible thing of all is that the map will continue to update with each cleaning run that it does. If the S5 Max goes into a room with new furniture or things that have been moved around, it will update the map itself, and this is handy if you mapped your house with chairs moved and then moved them back after the fact (which is what we did). Or, as was the case in my study, my computer chair wasn’t moved the first time, but subsequent runs when it was moved, it cleaned that area.How good does it clean?A lot better than you expect it to clean. We were pretty sceptical (my wife especially) that this robot vacuum cleaner was going to do a good enough job that we could get away with one big clean weekly, but on the first day of use, we were proven wrong. The S5 Max on the balanced setting cleans exceptionally well. When it detects carpeted areas, it’ll spin up the suction to give it that extra clean. Unless you’ve got super thick carpet, you don’t need to worry about using the battery draining turbo or max settings.The mopping functionality is also quite decent. It doesn’t remove anything too stubborn from the floor, but it will quickly remove spills, footprints, and dirt. It uses a microfibre mopping cloth that gets dragged along the floor. It seems to use a decent amount of pressure to effectively clean. It won’t replace the need to manually mop every once in a while but will make a noticeable difference.After your first mopping clean, check the cloth, and you’ll see how dirty it is. Our cleaner went under furniture (sofa, bedside tables) and seemed to pick up a lot of stuff throughout a conventional clean would get missed because of the inconvenience. Sadly, you don’t get an extra cloth in the box (or nothing extra, really), which is disappointing considering the cost.The timing was great when our S5 Max first arrived. We were due for a mop and let Robi do his thing as he mopped and vacuumed the house. Afterwards, a few spots needed to be manually cleaned (some of which was McDonald’s sweet and sour sauce dried on the floor), but the house looked noticeably cleaner.You need to be aware of the first time it does its mapping. It’s not going to clean that well. It’ll map out your house, and it will miss things. When it does a subsequent clean after mapping is completed, it’ll do a much more thorough cleaning job (especially the edges of a room).Don’t be surprised after its first thorough cleaning if it picks up more than you were expecting. We noticed our S5 Max went under sofas, bedside tables and other areas that are usually difficult to reach or forgotten about. We don’t have a pet, but it managed to find a lot of hair (it’s excellent at picking up hair). Using deterrgents and cleaning products for moppingXiaomi advises against using anything in the tank for mopping other than water. We are huge fans of using vinegar to clean in our household. It’s natural and effective. Many online have experimented with different products in the water tank.At first, we used just water in the S5 Max, but subsequent cleans experimented with mixing vinegar and water. The mopping feature works even better with a bit of vinegar, and while it’s not recommended, we’ve had no issues using it over prolonged periods. If anything, it’ll help prevent bacteria and other nasties from growing in your water tank.  If you add anything, do so at your own risk.Battery life is fantasticThe promised battery life on this is very accurate. How long this thing will clean for on the balanced setting will surprise you. According to the app, to do a full clean of the house, it’s 103 square metres, which takes 127 minutes.Even after the whole clean, it still had a decent amount of charge left in the battery. This thing could have easily gone for another 30 minutes (or more) if it needed to. Unless you live in a mansion, there will be plenty of juice left in the tank in your beloved robotic cleaning specialist.Customise cleaning settings on a per-room basisNot only can you break your home into named regions, but you can also specify specific cleaning settings for each room. We set the mopping to heavy for our dining table and the vacuum to the turbo setting (the second-highest). We did the same thing for our kitchen.We opted for a medium mop for the common area (the main hallway)and used the balanced setting for vacuuming. We made sure the water was turned off and used balanced for the vacuum setting.Customising per-room cleaning settings can help maximise battery life but also help you focus on areas that get dirtier than others (hallways, entrance doors, eating areas). for the bedroomsScheduled CleaningNo robotic vacuum would be complete without support scheduling cleans. You can schedule one-off or recurring cleans, specifying the types of days you would like to clean. We have a daily clean that does the kitchen and dining table area at 9 am (after breakfast is over and kids are no longer making a mess). After lunch, we have another clean cycle and, finally, a clean cycle for the kitchen and dining after dinner.If you’re vacuuming at night and you have kids, you probably want only to specify specific areas to be cleaned and using a quiet setting. For any night cleaning, we have it set to ignore the kid’s rooms (their doors are closed) and focus on the main areas of the house.Zone cleaningThis is a great feature if you have a specific area in your house you want to be cleaned. For example, I dropped some food from a plate while scraping it into the bin. It made no sense to clean the entire kitchen, it was an area near the bin (it was rice and hard to pick up myself). Using the zone mode, you can drag a box around an area and summon the S5 Max to take care of it.Interestingly, my wife uses zone cleaning over the room cleaning mode when she wants to clean a particular area of the house.Multi-level support (for those who need it)If you have a two-storey home, then you’re in luck. Because Xiaomi is still updating the S5 Max and adding new features, a recent addition can support multiple apps, which includes multiple floors. If the budget doesn’t extend to two of these, you can map both floors and carry it around.I haven’t tried the multi-level feature yet as the place we are currently in is a single-story. However, we are building a two-story house, and this will eventually come in handy for us.Home assistant support rocksOne feature I was curious about when we bought this was the home assistant support, notably Google Home. It works well. My wife and I can say, “Hey Google, clean the kitchen,” and out strolls the S5 Max to clean the kitchen for us. It’s simple and effective.If one of our children drops their dinner or a drink on the floor (a regular occurrence), we can ask Google to send the vacuum to the area where the mess is located, and it will take care of it for us. It’s more convenient than getting out a mop or vacuum to do it manually.Our son, who knows how to use Google Home, has started asking Google to clean up messes he sees. It’s the most proactive we’ve seen him when it comes to cleaning up. Kids love robots and voice assistant technology.ConclusionThe Roborock S5 Max will blow you away with how good it is. Unless you want better mopping functionality, the S5 Max offers the best value for money and really, you want a robotic vacuum for vacuuming hard to reach places and messes. Keep in mind the mopping functionality is better than expected.The only things we didn’t like about the S5 Max were the lack of extras. You get no extra filters, no extra mopping pads, no extra moisture mats or double-sided adhesive if you want to move the dock around. The robot cleaner itself is excellent, but it’s the lack of inclusions (considering the price) that may disappoint you.P.S. Robots deserve names. We called it Robi (my son named it). ",3368
One of the best and most affordable streaming microphones,"There are many microphones to choose from, and you will read conflicting opinions and reviews on all of them.Let me save you some time if you’re starting in streaming or YouTube and don’t have a $300-$500 budget to drop on a microphone you might not use in six months if you give up. Some opt for the Shure SM7B, but I believe this isn’t meant for streaming and is a waste of money.The Audio-Technica AT2020 microphone. A cardioid condenser microphone can go toe-to-toe with the more expensive microphones, but for a fraction of the cost. You can find it between $150-200 Australian Dollars. Sometimes a good deal on a used one will show up on eBay.Audio-Technica AT2020 microphoneUnlike some other more entry-level targeted microphones, this one is not USB. Audio-Technica does have a USB version of this microphone, but when it comes to audio, I always choose XLR over USB.Because it’s an XLR microphone, you’ll also need 48v phantom power. You can get this from an audio interface, and most usually come with one or more XLR/instrument inputs. I use the PreSonus Studio 24c audio interface, which has two inputs and individual gain control.PreSonus Studio 24c audio interfaceThe AT2020 has a great response with a decent dynamic range. While it sounds good out of the box, it’s the kind of microphone that will afford some room to EQ (if you want), but if you’re streaming you won’t need to touch any of that. It’s a flat sounding microphone which is perfect for talking. This microphone is quite sensitive, so a compressor is absolutely essential, otherwise, you’ll be playing with the gain control on your interface constantly.Unless you buy a streaming kit that comes with everything, you will need to purchase the following in addition to the microphone:An XLR cableAn audio interface of some kind to plug the microphone intoA foam windshield (not required, but highly recommended)An XLR cableAn audio interface of some kind to plug the microphone intoA foam windshield (not required, but highly recommended)Some people mistake buying microphones like these expecting everything you need to come with it, but it’s only the microphone. Just be aware of that. I saw some reviews of this where people were giving it one star because they didn’t know it was just the microphone and required additional items to be purchased.The good thing about this microphone is that it’s not only well-suited to streaming/podcasting and YouTube-ing. If you’re a musician, the AT2020 can be used for vocals, recording acoustic guitar and even as an additional room microphone. This is an impressive microphone for the price.Let me fill you in on a bit of a secret: microphone technology hardly ever changes. Buying this microphone is an investment that will last you for years, and it will hold its value quite nicely, too, if you look after it.You don’t need to spend a lot of money to get a good streaming/YouTube capable microphone. The AT2020 will be a step above a lot of the affordable USB microphones you’ll encounter, a professional sounding microphone for an amateur budget.",768
A review of the Elgato Key Light Air,"If you want to get into streaming, make yourself look better on Zoom or Microsoft Team calls or perhaps add some better lighting to your office, you need good lighting.I am no lighting or videography expert but ask any photographer/videographer how important lighting is, and you will get your answer: light is everything.I have been using lights for a while now, and before I purchased the Elgato Key Light Air, I was using these affordable Absestudio Softbox lights from Amazon. If you are on a budget, softbox lights like these can make a world of difference.these affordable Absestudio Softbox lights from AmazonThe issue I have always had with these softbox lights is my space is small, and because they’re lights mounted on stands, they require a bit of space to set up properly. I made do, but I hated how cramped they made the room feel.The Elgato Key Light Air retails currently on Amazon for 205 Australian Dollars. For some, $200 on a light might be a hard sell, and this definitely puts Elgato into a category slightly above affordable. If you’re a lowly streamer, who isn’t making money, investing this much might be a lot.retails currently on Amazon for 205 Australian DollarsWhere the Key Light Air justifies the steeper price is the control you have over it. If there is one thing that Elgato knows, it’s their customer base, and the Key Light Air connects via WiFi and can be controlled by an application on your computer or even via your smartphone. Cheaper lights are manual and don’t really offer customisation.You can adjust not only the brightness but also the colour temperature as well. If you are new to the world of lighting, specifically subject lighting, you want lights with adjustable colour temperature. Think of the Key Light Air as a Phillips Hue light, except that the colours can’t be changed.For me, the upside was I already used (and love) the Elgato Streamdeck and as to be expected, the Key Light Air works perfectly with the Streamdeck allowing you to assign functions light brightness and temperature adjustment to your buttons. This helps make adjustments on the fly during a stream, but control will be all you care about for most desktops and smartphones.The colour temperature is adjustable between 2,900K to 7,000K. The max brightness output is 1,400 Lumens (which is bright). It’s not as bright as some other LED lights for video production, but the Key Light Air is a specific product for illuminating a single subject sitting at a desk, not a room.ConclusionFor many, the question is going to be: is the Key Light Air worth the price? The answer is yes. I’ve used this for a while now. The small form factor, adjustable height, wireless controls, and even diffused light distribution make this the perfect light for single subject illumination. The price goes up with the inclusion of wireless, but you will soon realise how great the wireless controls are.You could roll the dice and buy a cheaper LED light off eBay or Amazon, but the quality will pale in comparison. I love the metallic stand that it sits on and the design of the light itself. It’s an upgrade from using bulky softbox lights and great for streams, Zoom calls and recordings.My only advice with this light is if you wear glasses. This is more of a general lighting issue, but if you wear glasses and don’t want glare, illuminate yourself from a 45-degree angle. When I followed the initial instructions of sitting it right in front of me, the glare was super obvious on my glasses.You can find the Elgato Key Light Air on Amazon here.here",889
Recording Reaction & Tutorial Videos With Streamlabs OBS,"There are numerous software applications you can use for reaction & tutorial videos and even streaming. Today, we will learn how you can record videos (and efficiently stream) using Streamlabs OBS.Streamlabs OBS is synonymous with streaming live video, but many forget that it is a very capable application that can record video. A lot of these tips will extend to better recordings and streams with Streamlabs OBS, not just specifically for recording videos.Audio sources onto separate tracksDuring the post-production, one thing you’ll find yourself doing is adjusting audio levels. By default, all audio is squished into one single audio track. While this makes sense for streaming because most streaming platforms don’t support multi-track audio, you want separate audio tracks for recording.In my case, I have a microphone, and I am also recording the desktop audio. When a video or song is playing, I want that audio on one track. When I talk over it, I want that on a separate track.To the right-hand side, where it says, “Mixer”, look for a cog icon in the top right that opens up advanced audio settings. Notice that I have 4 audio sources, but only two of them are assigned. Desktop audio is track one, and that is anything that plays on my computer. Mic/aux is my microphone, and that is assigned to track two.If you were to record using these settings and then open up the recorded video file in Adobe Premiere Pro, you would see it in the timeline.Notice how there are two audio tracks? A1 is the desktop audio, aka track one, and A2 is the microphone, aka track two. You can now easily adjust volume levels on individual tracks. This alone will level up your recording and editing capabilities. From an editing perspective, separate audio tracks are a gamechanger to your editing workflow.Use a compressorDepending on what type of microphone you have, you might run into issues where at times, your microphone is too loud and distorts. Sometimes that is the desired effect, but you want a compressor if you want professional sounding audio.In Streamlabs OBS, it comes with quite a few audio filters, and one of those is an effective compressor. Setting a few variables prevents your audio from going into the red if you yell or scream. It will prevent your audio from going into the red and sounding unpleasant.For this, the settings will be quite dependent on your microphone (USB or XLR) and any audio interface. Here are my settings.The ratio is set to 3. The threshold is set to -20. The attack is set to 1. We set the release to a super low value of 50.The one setting you will want to experiment with is the gain value. Compressing audio will lower the volume, and you’ll need to boost it slightly. You want on the monitoring level when you speak for the audio to sit in the yellow band, not right in the middle, but a little before it.Try speaking and yelling into your microphone to see where the level indicator goes and adjust accordingly. For me, I needed to turn the gain up to 6 for a good boosted sound that doesn’t put me into the red. You might need more or less.You might notice that we have a sidechain/ducking source at the bottom, and it’s set to Desktop Audio. The reason for this is when I speak, it will turn the volume down of the desktop audio source. It quietens it down a little bit. You might not want this, but it just means that you’re not fighting other audio when you talk.Only Have One Monitor? Use Browser SourceYou’ve probably wondered how YouTuber’s and Twitch streamers get a small little box that they use to show websites or YouTube videos. This can be achieved using a Browser Source. This is the perfect method for those of you who only have one monitor, where full-screening a video and monitoring your Streamlabs OBS settings is not possible.The downside is that if you have YouTube Premium, you’ll need to log in to it first if you’re showing YouTube videos and don’t want to be bothered by ads. You can interact with the browser source by right-clicking on the box and then selecting “Interact” — the URL goes into the source field, right-clicking and selecting “Properties” will provide the field to put in the URL.What you get is this beautiful browser source that can be resized and moved into whatever position you want it to be. This is what the result can look like.As a rule of thumb, if you are looking at your screen and to your left, put the box in the right-hand corner of Streamlabs OBS. Because everything is flipped like a mirror effect, it’s the opposite (so looking left records as looking right).Have Two (or more) Monitors? Use Display CaptureIf you have two or more monitors, display capture is the perfect way to capture either a whole screen or a specific application. Your left screen becomes the browser source that you capture, allowing you to full-screen and whatnot. Your right screen is where your Streamlabs OBS app lives. It will show you your levels and allow you to change scenes, etc.The premise is the same, except you choose Display Capture and then select the appropriate mode. For a multi-display setup, capturing the entire desktop itself is probably the best course of action.",1293
Fixing a slow WordPress installation with thousands of hierarchical taxonomy values,"Recently while working on a large-scale WordPress site, I encountered a tricky situation. I had quite a lot of locality data for Australian states, suburbs and LGA’s (Local Government Areas). We are talking about tens of thousands of items in the database.At first, I made all of these custom taxonomies hierarchical. Part of the reasoning was that users could visually select values from the meta boxes (hierarchical taxonomies look better). Initially, it wasn’t a problem, but as more data was added, saving posts took forever, I was getting timeouts, and queries were slow.If you are reading in hopes of a solution that allows you to keep using hierarchical taxonomies with tens of thousands of items, I am sorry to disappoint you.The solution was to make all of my large taxonomies non-hierarchial. Literally setting the argument for the taxonomy to false. ""hierarchical"" => false,""hierarchical"" => false,I leveraged the Advanced Custom Fields plugin to create the taxonomy hierarchy using post relationships for those uses where I need a hierarchy. It’s not an ideal situation, but it works.Sadly, WordPress has had this issue for several years now, and because of the way taxonomies work, I don’t think it will be fixed any time soon.TL;DR — hierarchical taxonomies will not work properly for thousands of items. Use non-hierarchical taxonomies instead.",340
Thrice announce new album Horizons/East,"It’s official. Thrice have announced their new album titled Horizons/East is dropping digitally on September 17, 2021. The physical version isn’t releasing until October 8, 2021.It’s a 10 song record that the band produced and recorded themselves. If you are a fan of their older stuff from Major/Minor or Alchemy Index, you would know the band produce some of their best work when they do it themselves. What makes the LP release more interesting is the LP releases come with chromadepth glasses offering a 3D viewing experience including assets the band say they’ll provide on social media. This is an excerpt of the description itself:The LP version of “Horizons/East “ will include special chromadepth glasses, packaged in the LP, for an enhanced ‘3D style’ viewing experience of the LP cover and packaging elements as well as unique video assets that the band will post on YouTube and Instagram.The LP version of “Horizons/East “ will include special chromadepth glasses, packaged in the LP, for an enhanced ‘3D style’ viewing experience of the LP cover and packaging elements as well as unique video assets that the band will post on YouTube and Instagram.The new single Scavengers has also dropped and it’s so damn groovy. Riley has really become such a pocket-oriented drummer, so tasty.

The artwork in which we saw the band tease prior looks awesome.The track-list for Horizons/East is as follows for the two-sided release:1. The Color of the Sky2. Scavengers3. Buried in the Sun4. Northern Lights5. Summer Set Fire to the RainSide B6. Still Life7. The Dreamer8. Robot Soft Exorcism9. Dandelion Wine10. Unitive/EastIf you’re a Thrice alliance member, you can pre-order a special exclusive vinyl right now (you should have received the email). The Alliance variant is limited to 500 copies and they will go quite quickly like all Thrice vinyl does.Side B",466
Valve finally enters portable gaming with the Steam Deck,"When people think of big names in gaming, they will often cite companies like Nintendo, Microsoft or Sony, followed by a select handful of gaming studios like Electronic Arts or CD Projekt Red. Do you want to know who one of the biggest names in gaming is that you rarely hear of? Valve.If anything, I would argue, given just how much of the gaming market Valve controls through Steam alone, it’s one of the biggest gaming companies around, probably second to only Nintendo.Yes, I am aware Valve is behind the popular Half-Life franchise a lot of gamers know and love, but there haven’t actually been that many Valve games released in the past decade. The biggest Valve game that comes to mind is Dota 2.Gaming aside, Valve began to transition into a hardware company in the late 2000s. Valve’s most notable piece of hardware produces the Valve Index virtual reality headset, which is regarded as one of the best VR headsets money can buy right now.Rumours had been swirling for a couple of years now about Valve’s desire to enter the portable gaming market and produce a portable console similar to the Nintendo Switch, but one that could play games from your Steam library instead. It was initially rumoured to be called the Neptune.Valve calls their foray into portable gaming the Steam Deck, listed as starting from USD 399.Steam DeckThe first problem I can see before we discuss the finer details is the name. Valve calls it the Steam Deck, but the name is close to an existing product called the Stream Deck by Elgato. While the two are not gaming consoles, the name confused me as a Stream Deck user.Stream DeckAs for the tech specs, let’s see what Valve have thrown into this thing. The important thing to note here is before you even get into the tech specs, the wording starting from USD 399 tells you in marketing speak that this console will come in different tiers, with the $399 version being the base model.starting from USD 399The CPU and GPU in the Steam Deck put it into the same league as the PS5 and Xbox Series X, not quite a PC and not quite a walled-off garden like the Nintendo Switch is.AMD APUCPU: Zen 2 4c/8t, 2.4-3.5GHz (up to 448 GFlops FP32)GPU: 8 RDNA 2 CUs, 1.0-1.6GHz (up to 1.6 TFlops FP32)APU power: 4-15WIf anything, these specs put it into gaming laptop territory in a small Switch like form factor, complete with a 7″ screen.The real tell in the differences between the various models is the tech specs detail three storage levels (in addition to the MicroSD slot).64 GB eMMC (PCIe Gen 2 x1) — $399 USD ($537 AUD)256 GB NVMe SSD (PCIe Gen 3 x4) — $529 USD ($712 AUD)512 GB high-speed NVMe SSD (PCIe Gen 3 x4) — $649 USD ($873 AUD)64 GB eMMC (PCIe Gen 2 x1) — $399 USD ($537 AUD)256 GB NVMe SSD (PCIe Gen 3 x4) — $529 USD ($712 AUD)512 GB high-speed NVMe SSD (PCIe Gen 3 x4) — $649 USD ($873 AUD)The interesting thing about these tiers is the lowest tier which also comes with storage, is the slowest of the bunch. Still, for the price, it’s considerably cheaper than a PS5 or Xbox Series X; even the mid-tier Steam Deck is cheaper than the cost of a console right now (considering it’s all scalper prices).What Valve has done here is smart. However, once you throw GST on top and other costs, the base level Steam Deck will probably be AUD 650, the mid-level AUD 800 and the top-tier probably around the AUD 1000 mark. A $500 OLED Nintendo Switch suddenly becomes the more affordable albeit less powerful option of the bunch.The real value adds here is that Steam Deck can play all games in your Steam library (allegedly). It also works with a Switch-esque dock and has impressive resolution output listed. The Steam OS this thing runs is also based on Arch Linux (which is awesome).But that’s not even the best part. The Steam Deck will be an open platform, allowing you to wipe it and install whatever you want on it. This means you could install Windows on this thing if you wanted to or another Linux distribution.The Steam Deck will be an open platform, allowing you to wipe it and install whatever you want on itEven Epic Games CEO Tim Sweeney seems to have nothing but praise for the Valve Steam Deck:
Amazing move by Valve! A handheld PC/console hybrid running the SteamOS fork of Arch Linux, and it’s an open platform where users are free to install software or their choosing – including Windows and other stores. https://t.co/jf5TWUWGP5— Tim Sweeney (@TimSweeneyEpic) July 15, 2021
Amazing move by Valve! A handheld PC/console hybrid running the SteamOS fork of Arch Linux, and it’s an open platform where users are free to install software or their choosing – including Windows and other stores. https://t.co/jf5TWUWGP5— Tim Sweeney (@TimSweeneyEpic) July 15, 2021Amazing move by Valve! A handheld PC/console hybrid running the SteamOS fork of Arch Linux, and it’s an open platform where users are free to install software or their choosing – including Windows and other stores. https://t.co/jf5TWUWGP5https://t.co/jf5TWUWGP5July 15, 2021I wonder if Epic Games will follow Valve’s lead and also produce a gaming console of its own? Given how successful Fortnight has been for Epic and the amount of money it has made them, they probably have the cash to spare to at least try.If you think of the Steam Deck being more of a gaming laptop, it makes more sense than comparing it to the Nintendo Switch. This thing could rival AMD gaming laptops quite easily.While I don’t need one, I was looking forward to being able to reserve one. Sadly, the Steam Deck isn’t available for reservation in Australia, so that sucks. I’ve already pre-ordered the OLED Nintendo Switch, so maybe I should be happy with that.",1413
The Thrice Alliance Is More Like Thrice Silence,"Ask anyone that knows me who my favourite band is, and they’ll tell you Thrice. The song my wife and I danced to at our wedding was a Dustin Kensrue song (not a Thrice song, but still). This band has significant meaning in the lives of my wife and me.My closet is a Thrice shrine (minus the candles and creepy statues). I have bought every single release they’ve put out throughout the years, even the expensive, limited vinyl releases, merch. For the hard to get items, I have even paid way above what they were at retail to complete my collection. I own one of the only copies I am aware of, The Artist In The Ambulance (instrumental).The Artist In The Ambulance (instrumental)So, I was stoked when my favourite band announced they were bringing back The Alliance (I was a member of the original one they had in the early ’00s). I wanted to support my band, and the perks were amazing. As soon as I could, I bought an annual subscription.,c.I consumed the few videos they had up on the site, the welcome video and a few studio videos and photos. And ever since, besides presale tickets for members of which are no benefit to me (I’m in Australia), nothing else has really happened.The Thrice Alliance, in my opinion, was launched way too early. It’s clear the band was thinking ahead for the pending release of their as-yet-unnamed album (and new track Scavengers dropping immediately).I am not trying to sound entitled because I love Thrice as a band, and they are all such genuine human beings who will gladly nerd out and talk with their fans about literature and other obscure topics. But, I think maybe the Alliance concept just needed more time in the oven before being taken out. That’s all.I was at least expecting they would have their Discord (which is going to be the forum) in place now, but it’s still not live.",457
Why you should choose Aurelia over React (mostly),"The popularity of React is undeniable. When bright-eyed developers think about creating a new app, React is usually the first thing that crosses their minds. React has all become the defacto front-end library of choice.React is comfortable. React is familiar. React is used by almost everyone. React is the library you learn if you want to make yourself an employable front-end developer.The truth is, front-end developers have painted themselves into a corner with React. To understand why React became so popular, we have to go back to 2013, when React first made its grand debut. AngularJS was the dominant and most popular option for web applications at the time, and boy did AngularJS suck.However terrible AngularJS was, everyone was using it. AngularJS had some interesting ideas, but confusing terminology (provider, service, factory) and some performance issues meant building an app was sometimes half the battle.Let me state the obvious bias here: I am on the Aurelia core team, everything I say is tainted with bias. However, I want this to be an accurate summation of what both Aurelia and React are. If I make a misleading or completely incorrect claim, let me know, and I will fix it.React became the most popular kid in school because the alternatives were much worse. I actually worked with React and came from an AngularJS background. Like other developers in 2013, I was absolutely charmed and smitten. The whole marketing hype around Virtual DOM was infectious. The lack of digest cycle issues and other Angular problems went away.Building apps in React was fresh and exciting; React was a lot easier to grasp and use than AngularJS or other options at the time were.I don’t want this to be a “let’s bash React” article because it’s not. This article aims to point out that while React might be the most popular option, it’s not always the best option. The truth is most front-end developers could not give you a valid reason for why they use React over something that might be better suited. The best you will get is, “it’s the most popular option.”Developers who choose React because of its size and stature very much remind me of the old ’90s saying (or was it 80s?): Nobody ever got fired for buying IBM. And look, nobody is going to be fired for choosing React in most instances. Unless you’re this guy who wrote about how he almost got fired for choosing React for an enterprise app. Who knows if React was to blame here, but you can’t argue with the facts presented in that post; React can be a time sink if you’re not using something like Gatsby or Next.Unless you’re this guyBut, there is definitely a degree of “bigger is better” when it comes to React, a lot of people choose it because it’s the biggest. It’s where that IBM saying came from, IBM was huge in the enterprise (not so much these days), not because their quality was better, but because they were the biggest. If anything ever went wrong, IBM would get the blame.We are now seeing a generation of developers graduating from coding boot camps and doing online courses focusing on React, instead of the language behind it (Javascript). This isn’t a new phenomenon, I remember back in the jQuery days where some developers knew how to write jQuery, but lacked knowledge of actual Javascript, not even knowing how to query the dom without resorting to $() which explains why a lot of old StackOverflow questions from a few years ago about Javascript have jQuery answers.$()React has a place and purposeI have seen some amazing things built with React, this article will not be arguing that it is inferior or incapable of allowing you to build applications. I also want to point out that while this is an article recommending Aurelia over React, many of these same arguments can be made to choose Svelte over React.I think React has a place in modern front-end development; it shines for smaller projects and things like landing pages, but for a large-scale line of business application and enterprise purposes, React is a world of pain waiting to envelop you.For projects where you don’t need to maintain hundreds of components, pages, communicate heavily with API endpoints, deal with large amounts of data and complex UI’s, React is a great choice. If you’re building landing pages or simple apps, I would definitely agree that its lighter approach works well for simple to intermediate use-cases.And once again, I am sure many of you can point me to examples of large React apps as well as companies who are using it. Nobody is saying you can’t build apps in React. You can pop the top of a beer with a spoon or knife, but a bottle opener is better suited.But, Aurelia isn’t…I know some of you are going to throw out some of these retorts about Aurelia, so to save you the trouble and some time, let me do it for you:“Aurelia’s community is too small” — Nice observation, Sherlock. Yes, it is. The size of a community doesn’t make a framework or library better or worse. If anything, the smaller community is a bonus as you’re more likely to get help from Aurelia core team members when you run into a problem. Can you ask React core developers for help?“The documentation sucks” — A valid criticism for Aurelia 1. Some people still in 2021 also think the React documentation sucks as well. Fortunately, for Aurelia 2, the documentation doesn’t suck.“Smaller selection of plugins” — once again, a valid criticism. The smaller community and market share means sometimes you’ll have to do things yourself. Fortunately, Aurelia is so easy to work with and works with all third-party libraries (see no VDom) that it’s not that big of a deal. You can even use jQuery libraries in an Aurelia application.“Aurelia isn’t popular” — In comparison to other choices out there, Aurelia is one of the lesser known options. Fortunately, not every developer sees choosing technologies as a popularity contest. If it meets your needs, is well-maintained and the technical requirements match the project, isn’t that enough? Do you remember what else used to be popular? Blockbuster, VHS Tapes and handshakes. Times change.“There aren’t that many Aurelia jobs” — quite valid. I am fortunate enough I get to work with Aurelia every day, but the job market for Aurelia is small. Do you know what other library has fewer jobs at present too? Svelte. It hasn’t prevented it from being slowly embraced by front-end developers.“Aurelia’s community is too small” — Nice observation, Sherlock. Yes, it is. The size of a community doesn’t make a framework or library better or worse. If anything, the smaller community is a bonus as you’re more likely to get help from Aurelia core team members when you run into a problem. Can you ask React core developers for help?Aurelia’s community is too small“The documentation sucks” — A valid criticism for Aurelia 1. Some people still in 2021 also think the React documentation sucks as well. Fortunately, for Aurelia 2, the documentation doesn’t suck.The documentation sucks“Smaller selection of plugins” — once again, a valid criticism. The smaller community and market share means sometimes you’ll have to do things yourself. Fortunately, Aurelia is so easy to work with and works with all third-party libraries (see no VDom) that it’s not that big of a deal. You can even use jQuery libraries in an Aurelia application.Smaller selection of plugins“Aurelia isn’t popular” — In comparison to other choices out there, Aurelia is one of the lesser known options. Fortunately, not every developer sees choosing technologies as a popularity contest. If it meets your needs, is well-maintained and the technical requirements match the project, isn’t that enough? Do you remember what else used to be popular? Blockbuster, VHS Tapes and handshakes. Times change.Aurelia isn’t popular“There aren’t that many Aurelia jobs” — quite valid. I am fortunate enough I get to work with Aurelia every day, but the job market for Aurelia is small. Do you know what other library has fewer jobs at present too? Svelte. It hasn’t prevented it from being slowly embraced by front-end developers.There aren’t that many Aurelia jobsMany of these arguments against React apply elsewhereWhile this article makes the case for Aurelia over React, many of the same arguments could be made for other technology choices. The three that come to mind are Angular, Svelte and Vue.To me, Angular and Aurelia are similar when it comes to large-scale application development, particularly enterprise applications. Angular is a decent choice, but it is a lot more opinionated and requires a bit of boilerplate which Aurelia intentionally tries to avoid you needing to write.Svelte is a promising up and coming lighter alternative, marketed as a compiler, you write code that more closely resembles plain Javascript, it also has some libraries that let you hit the ground running without having to do a heap of research and development.Then you have Vue. Arguably, we have Vue to thank for concepts like single file components (SFC’s) and it should be praised for offering binding features such as two-way binding. However, with Vue 3, they appear to have gone in more of a React direction. You still get a lot of what you need out of the box, but the introduction of their own React-like hooks seems to blur the lines between Vue and React.Aurelia is better out of the box, React is too unopinionatedMany developers will tell you that the lack of standards or opinions regarding building React applications is a good thing. In a lot of situations, it’s actually not. React for such a long time (and maybe, still does) marketed itself as the “V” in MVC, just a humble view library.The lack of conventions or even basic guiding principles of where things should live, what they should be called and how a basic React app is supposed to look like can result in a lot of mess. and paralysis when it comes to even make decisions about what the app should look like or use for basic features.If you were to follow the official getting started documentation, the resulting app you would have would provide nowhere near the features and functionality that most web applications require. Out of the box, React is borderline useless for most use-cases. Displaying a list of array data? No problem. A CRUD based application with authentication, validation, internationalisation and routing? Not so much.When is the last time you encountered a React application that didn’t need a router, didn’t require working with forms, didn’t use some form of state management (possible with hooks and context API now), validation and a slew of other crucial pieces of a modern web application or site?With Aurelia, a basic generated app using the CLI will give you almost everything you need right from the get-go. You get an HTTP Client for working with native Fetch, and you get a powerful validation plugin, a router, intuitive template control syntax and a convention-based approach that gets out of your way and allows you to have more power when you need it.When you use React, you have to make many choices if you’re going the conventional path. You’ll be npm installing numerous third-party packages, all maintained by separate authors. Because popular React packages with one maintainer underpin the React ecosystem, it’s a house of cards.What router package do I use?What state management (if any) library do I use?Should we use browser native Fetch or a library like Axios?Tailwind CSS, Styled Compoents, SCSS or plain CSS?What form library will you use? Especially if you’re using something like Redux.What router package do I use?What state management (if any) library do I use?Should we use browser native Fetch or a library like Axios?Tailwind CSS, Styled Compoents, SCSS or plain CSS?What form library will you use? Especially if you’re using something like Redux.Just answering these questions before work even can take weeks. Everyone on your team who has worked with React will have worked with different libraries and have conflicting opinions based on their experiences. Can you imagine arguing over what router to use?After you’ve spent all that time researching and validating your technological choices, everyone has to familiarise themselves if you’re working in a team. Not everyone would have worked with all of the chosen packages you have gone with. Another couple of weeks passes, still not much work done.Then what happens if there a major React release and your chosen library or tool isn’t compatible? You can’t use the new exciting features because a tiny part of your app hasn’t been updated yet. Turns out that the package you rely on is maintained by a single author being paid $0 who has a full-time day job.Convention over configuration; easier to structureWhen structuring a React app, the community can’t decide on a best practice standard. It’s rare two React apps will be alike. You familiarise yourself with one React codebase; the next codebase you work on will be completely different (most likely).The issue with React is that no two projects are the same. Knowledge cannot be transferred from one project to another. It seems every developer has their own approach to structure, naming and dependencies they choose to build an application. Maybe okay for a side project, but in the real world, like enterprise, not so much.Is it any wonder that some of the most popular React plugins/libraries are frameworks built around React itself? You have Next.js, Gatsby, and a few other smaller frameworks that aim to address the out of the box limitations of React. Essentially turning React from a view library into a framework with opinions and standards. Next.jsGatsbyThe beautiful thing about Aurelia is that it is convention over configuration as a default. If you want to override something, Aurelia gets out of your way. But, for most use-cases, the conventions are actually what you want, and it saves a lot of time.It’s also not rare to encounter production Aurelia apps that feel familiar to one another. Stepping into different React codebases is like travelling to different countries; the food tastes different, people might speak a different language, the speed limits are in a different unit of measurement.Aurelia offers stabilityDue to the lack of batteries included in React, you need to make your own technological choices when it comes to several aspects of large-scale applications. But, here is the problem: The React ecosystem is constantly changing, new patterns and approaches emerge, especially if we are talking about Hooks. New libraries emerge for routing, state management and who knows what else.With Aurelia, you can choose it knowing that because it gives you many features you need out of the box, you don’t have to worry about outdated packages or being left behind when a new major release of React comes out. It’s this kind of stability that matters in large projects (>2 years) and the enterprise.Not everyone has the luxury of being able to continually update and change their apps every six months when something new and shiny comes about.Aurelia is a friend to allBecause Aurelia is enhanced HTML, Javascript and CSS, it plays nicely with almost every front-end package you can think of. Need a charting package or want to use a legacy jQuery plugin your company built 10 years ago nobody is brave enough to rewrite? The lack of Virtual DOM or any other needless abstractions means you can interact directly with the dom, mutating it and even creating component wrappers around it.Want to bind to nodes of an SVG element and modify them on a molecular basis? You can do that and not have to worry about weird side-effects or side-stepping abstractions.Want to get crazier? You can even use other frameworks or libraries inside of Aurelia. I’ve integrated React inside of an Aurelia app before, just because I could.Aurelia scales more efficientlyI don’t mean bundle size or performance when I say scale. I refer to Aurelia’s ability to accommodate large-scale applications while managing to keep things clean and structured. At my day job, I am working on an Aurelia application that is three years old. One of our component folders contains 56 components, another contains 32 components. In total, we have well over a couple of hundred components in our style guide underpinning a suite of applications that can four (and counting) applications all using shared components and configuration logic. Even with a codebase this big, the longest I’ve seen it take a new developer to become familiar enough with our codebase that they start contributing code, it was one week and they had never worked with the front-end before.In that time, there has never been a single update released by the Aurelia team that has required going and changing anything inside of my application en masse. They got the design right the first time, unlike React, which still feels like it’s an experiment (classes, function components, hooks).I understand that structure and scale concerns are somewhat personal, but I have yet to encounter a clean large-scale React codebase. It doesn’t mean they don’t exist, but I have yet to see one.Reactive binding is awesomeDespite React marketing the Virtual DOM as this amazing performant thing, the Virtual DOM is actually slow. When you really look at what the Virtual DOM is, it is an over-glorified dirty-checker, and it uses a lot of memory. Profile a large React app, and you’ll see first-hand how great the VDom is.In Aurelia, there is no Virtual DOM. Svelte and Elm’s same thing is a reactive binding approach to displaying data and keeping it in sync. Values change when updated, and the entire application doesn’t get re-rendered, no expensive in memory comparisons. You don’t have to worry about re-rendering your entire UI or considering how your properties are passed or referenced.This is the direction a lot of non-React libraries are taking now.Aurelia makes working with forms easyIf you want to experience immense pain, try working with forms in React. Working with forms (especially if you want validation) will make you curl up into a ball and furiously mash the keys on your mechanical keyboard. Forms and React are a form of torture.Working with forms is so bad that there are libraries out there to make working with them easier. The most popular one right now is React Hook Form. I understand working with forms even outside of React can be painful, but React somehow makes working with them worse.Form inputs are perfectly capable of holding their own ephemeral state. Still, side effects and all of that functional programming nonsense React developers fight so strongly against means you have to use callback functions and references to work with input values.In Aurelia, if you want to watch a text input for changes and have the value update in your view model, you do this: <input type=”text” value.bind=”myVal”>  it just makes so much more sense: no callback functions, no need for references.<input type=”text” value.bind=”myVal”>Aurelia has nicer syntaxFiled under: JSX is an abomination.Filed under: JSX is an abomination.Some developers really love the JSX syntax that React heavily promotes, but even when I started out using React and used it for a while after, the JSX syntax always left a bad taste in my mouth. This is a personal opinion here and not necessarily a fault of React, but it feels so far removed from HTML.Take this snippet of code from the official React docs on working with forms:render() {
    return (
        <form onSubmit={this.handleSubmit}>
        <label>
            Name:
            <input type=""text"" value={this.state.value} onChange={this.handleChange} />
        </label>
        <input type=""submit"" value=""Submit"" />
        </form>
    );
}render() {
    return (
        <form onSubmit={this.handleSubmit}>
        <label>
            Name:
            <input type=""text"" value={this.state.value} onChange={this.handleChange} />
        </label>
        <input type=""submit"" value=""Submit"" />
        </form>
    );
}Now, here is how you would write something like this in Aurelia:<form submit.trigger=""handleSubmit()"">
    <label>
        Name:
        <input type=""text"" value.bind=""state.value"" />
    </label>
    
    <input type=""submit"" value=""Submit"" />
</form><form submit.trigger=""handleSubmit()"">
    <label>
        Name:
        <input type=""text"" value.bind=""state.value"" />
    </label>
    
    <input type=""submit"" value=""Submit"" />
</form>Because form inputs are two-way data-bound, you don’t need a change callback to know when the value changes. Binding to native events is intuitive and familiar submit.trigger will call our handleSubmit function when the form is submitted.submit.triggerhandleSubmitBetter still, let’s compare how you iterate over data in a component in React and Aurelia. First, the React way of doing it:function NumberList(props) {
  const numbers = props.numbers;
  const listItems = numbers.map((number) =>
    <li>{number}</li>
  );
  return (
    <ul>{listItems}</ul>
  );
}

const numbers = [1, 2, 3, 4, 5];
ReactDOM.render(
  <NumberList numbers={numbers} />,
  document.getElementById('root')
);function NumberList(props) {
  const numbers = props.numbers;
  const listItems = numbers.map((number) =>
    <li>{number}</li>
  );
  return (
    <ul>{listItems}</ul>
  );
}

const numbers = [1, 2, 3, 4, 5];
ReactDOM.render(
  <NumberList numbers={numbers} />,
  document.getElementById('root')
);The Aurelia way of doing this is purely inside an HTML-only component (no view model). In a real application, your numbers array would be inside a view model and accessible to the view, but we inlined it for this example because we can.<let numbers.bind=""[1, 2, 3, 4, 5]""></let>

<ul>
    <li repeat.for=""n of numbers"">${n}</li>
</ul><let numbers.bind=""[1, 2, 3, 4, 5]""></let>

<ul>
    <li repeat.for=""n of numbers"">${n}</li>
</ul>Two-way data binding in Aurelia is awesomeIn Aurelia, you have multiple binding modes. By default, a lot of the bindings you use will be one-way. However, you can also have two-way bindings (the default for forms), allowing you to keep bindable values in sync across your view and view models.As you saw in the previous form examples, React requires you to use a change callback and other unideal approaches to getting updates from a form. There is a reason that both Svelte and Vue also have two-way binding because, despite what you’ve been told, it has very few downsides (unless you’re trying to use them with state management).Aurelia uses classes (which aren’t bad, btw)A lot of React developers well and truly drank the anti-classes Kool-Aid. With Facebook introducing Hooks to stop supporting class components one day (probably), many developers used this to fuel the anti-class rhetoric that existed long before React did.If you’re not using “pure functions” in React, you’re sinning. Funnily enough, nobody can give you a proper explanation of why classes are bad outside of some theoretical academic environment or made-up scenario that hasn’t been a thing since 1998.In Aurelia, everything is a class (much like it is in Angular), and many developers have been deploying Aurelia applications since 2015. The sky hasn’t caved in, and a mythical anti-class deity hasn’t come down from the clouds to scorn the Earth and leave only pure functions behind.Do you know how many problems I have encountered in my Aurelia applications that were caused by classes? Zero.Classes are natural namespaces. And yes, while the anti-OOP/class crowd have some valid arguments against classes, there are none to outright shun them. When used correctly, classes can be a beautiful way to encapsulate functionality instead of a bunch of singular pure functions you have to follow one by one to know what is going on.This is what a basic custom element looks like in Aurelia:This is what a basic custom element looks like in Aurelia:export class PeopleListCustomElement {
    
}

<ul>
    <li>John Smith</li>
    <li>Jack Black</li>
	<li>Bob Jones</li>  
</ul>export class PeopleListCustomElement {
    
}

<ul>
    <li>John Smith</li>
    <li>Jack Black</li>
	<li>Bob Jones</li>  
</ul>Then you can reference it like this:Then you can reference it like this:<people-list></people-list><people-list></people-list>Aurelia has HTML only custom elementsOne of my favourite features in Aurelia is the ability to create HTML only custom elements. When I say HTML only, I mean a component that probably more closely resembles a Web Component, except there is no Javascript whatsoever, no view model or inline JS.Say you want to create a loading component, but it’s CSS only and doesn’t have any Javascript. Here is what it could look like. You would save this as something like app-loader.html — in Aurelia 2, you don’t even need to write the template tags.app-loader.html<template>
  <p>Loading...</p>
</template><template>
  <p>Loading...</p>
</template>Then, reference it like this:Then, reference it like this:<app-loader if.bind=""isLoading""></app-loader><app-loader if.bind=""isLoading""></app-loader>Aurelia is more familiar to .NET developers (dependency injection, etc)There is a lot of overlap between Aurelia and .NET. Both utilise similar concepts (views and view models), classes, and so on. I have seen first-hand how easy it is for .NET developers to get familiar and up and running with Aurelia numerous times.This is where React begins to form some cracks. The lack of familiar concepts and, more specifically, the lack of dependency injection makes it quite an incompatible pairing with modern React, which fights against the use of classes and has no DI (amongst other familiar comforts).Some will argue with you quite strongly that you don’t need DI, but many will beg to differ (myself included).Did you know that .NET ranks consistently for being the most loved framework according to the StackOverflow developer survey? In enterprise environments, .NET is one of the most popular options around.ranks consistentlymost loved frameworkAurelia is faster to build withI will issue a challenge to anyone reading this who wants to take me up on this. I bet given a brief for an application of some sort, I could build something in less time it takes for someone to build that same application using React. I have prototyped entire features and apps in a day or two with Aurelia, not having to worry about certain technological choices just makes it faster to work with.While you’re busy deciding what router or state management library to use, I am already writing code.Make your own choicesAt the end of the day, I am just another biased developer with an agenda. You should always make informed decisions, never blindly follow trends or choose technology based on a blog post or how many stars it has on GitHub. It’s time front-end developers started to make informed decisions again.",6722
Microsoft Teams is ditching Electron and hopefully more projects follow suit,"It’s no surprise that Electron is a slow, poor performing memory hog. However, the value proposition here is that it allows you to distribute Web-based applications as native desktop apps without writing programming code or anything else non-web.For some reason, all of these work chat/productivity apps decided to use Electron. Perhaps the most notable is Slack which continues to be a steaming pile of garbage in itself.When Electron first arrived, it was actually a game-changer for desktop app development. Despite its flaws, companies including Microsoft flocked to it. However, over the years, it has started to show its age. This is why it is not surprising that Microsoft Teams is ditching Electron for Edge Webview2 in its 2.0 release due out soon.i ditching Electron for Edge Webview2Ask any developer who has had the misfortune of working with Electron, it has serious issues and despite best attempts to improve Electron, it continues to be a resource hog.
With this change, we are taking a major step in #MicrosoftTeams Teams architecture. We are moving away from Electron to Edge Webview2. Teams will continue to remain a hybrid app but now it will be powered by #MicrosoftEdge. Also Angular is gone. We are now 100% on reactjs— Rish Tandon (@rishmsft) June 24, 2021
With this change, we are taking a major step in #MicrosoftTeams Teams architecture. We are moving away from Electron to Edge Webview2. Teams will continue to remain a hybrid app but now it will be powered by #MicrosoftEdge. Also Angular is gone. We are now 100% on reactjs— Rish Tandon (@rishmsft) June 24, 2021With this change, we are taking a major step in #MicrosoftTeams Teams architecture. We are moving away from Electron to Edge Webview2. Teams will continue to remain a hybrid app but now it will be powered by #MicrosoftEdge. Also Angular is gone. We are now 100% on reactjs#MicrosoftTeams#MicrosoftEdgeJune 24, 2021I have never actually used Edge Webview2 (I didn’t even know it existed), but I am curious about how it performs. Something tells me that it’s not hard to beat Electron in the performance department. Everything looks like a 10 when you’re starting from 0.Despite the fact Visual Studio Code still uses Electron and manages to perform well, I have hopes that more projects like VSCode follow suit and we can finally kill Electron off once and for all.",589
Thoughts on Windows 11,"In what has been the worst kept secret in the world, Microsoft finally lifted the lid on the soon to be released Windows 11 after leaks and teasers before the official announcement.Funnily enough, the leaked build (which I did download) revealed most of what Microsoft had in store for Windows 11, and they still had some tricks up their sleeves.

Admittedly, this is the nicest Windows I have seen. From the corners to the newly centred start button, redesigned and simplified logo and use of translucency: Windows 11 looks beautiful. It’s very much inspired by macOS and ChromeOS and I am all for it.Perhaps the biggest announcement of all was the fact Android apps are coming to Windows, sorta. Android apps are coming to WindowsThanks to some help from Intel and its Intel Bridge tech, it effectively emulates Android apps and allows them to work on Windows. How performant this will be remains to be seen.Intel BridgeFor years people have been wanting to see Windows support apps from other operating systems. So when Microsoft announced Windows Subsystem for Linux that allows you to run a Linux installation inside Windows, people were shocked and blown away. Microsoft has ventured far from its monopolistic ways of the nineties.Seeing Intel work more closely with Microsoft shouldn’t be a surprise since Apple started manufacturing its own chips (the M1) and moving away from its reliance on Intel. In many ways, we see a return to the early 2000s when Mac’s ran non-Intel chips, and Intel dominated on PC.With Android support, Microsoft is going into ChromeOS territory. I wonder if Google will be bothered by this or see it as a boon for Android venturing into new markets? Considering Microsoft is also partnering with Amazon to use their Appstore apps and not Google, something tells me Android apps that rely on Google services will not be supported. It’ll be interesting to see if Google tries making it harder for Android apps to work in Windows.It might sound like a silly thing to say about an operating system, but I am excited. Even if Windows 11 is largely a cosmetic update over Windows 10, it will be free for existing Windows 10 users, and I think it’s time Microsoft started speaking their Fluent design language they’ve been working on for the last few years (which we finally see in their OS).",580
"Dear McDonald’s: bring back the Warm Cookie Sundae, you cowards","What are you so afraid of, McDonald’s?McDonald’s have been contacted for comment, but at the time of publishing this, I have not heard back from them just yet.McDonald’s have been contacted for comment, but at the time of publishing this, I have not heard back from them just yet.The world was a different place eleven years ago. There was no pandemic, TikTok didn’t exist, and the iPhone was barely out of nappies. More importantly, McDonald’s served a dessert that has seemingly been lost in time: the Warm Cookie Sundae.It’s quite a bizarre thing; Google searches for the Warm Cookie Sundae return almost nothing. The only proof that this dessert existed is a Facebook page demanding its return. The owner of this page appeared to give up their fight quite fast. Many of you who are reading this probably never even got to try this dessert (I am getting old now), but it had such a profound impact on me that I often think about it.is a Facebook page demanding its returnLet me describe what this incredible dessert was like. It was a classic McDonald’s sundae with a Cadbury cookie at the bottom, heated up (probably in a microwave). The centre of this cookie was warm and gooey; when you dug your spoon into it, you would encounter a soft chocolate centre that sent you into a blissful sugar coma.In the time that has passed since this delicious dessert came and went, McDonald’s has brought back numerous limited edition items, but the warm cookie keeps on being forgotten. I know you must be thinking, “But Dwayne, maybe the warm cookie sundae wasn’t that popular” I can tell you that I have traumatic memories of going to McDonald’s trying to get this dessert, and it would sell out quite often.No exaggeration, I would sometimes drive to different McDonald’s (there is one in almost every second suburb here in Australia) trying to get my hands on this. I was like an addict desperate for his fix, and I needed this dessert. I still remember when they started phasing this dessert out; I drove to so many stores trying to get the last of their stock.What are McDonald’s so afraid of? Did the big cookie lobby threaten and intimidate McDonald’s into burying this dessert deep in the Ronald McDonald HQ archives?",555
China f#*ks Bitcoin and cryptocurrencies (again),"When it comes to cryptocurrency, despite its world-changing power to overthrow the traditional banking system and kill fiat currency (according to some crypto proponents), there are two gatekeepers who determine whether it goes up or down: China and Elon Musk.Today, it was China’s turn to line up Bitcoin and the other top ten cryptocurrencies in a firing squad and open fire.
Channel $BTC pic.twitter.com/jxRZ9OHfBV— MartyParty (@martypartymusic) June 21, 2021
Channel $BTC pic.twitter.com/jxRZ9OHfBV— MartyParty (@martypartymusic) June 21, 2021Channel $BTC pic.twitter.com/jxRZ9OHfBV$BTCpic.twitter.com/jxRZ9OHfBVJune 21, 2021Even though the crackdown on Bitcoin announced by China was known weeks ago, today China showed they are not bluffing with numerous mining operations shutting down. The price at the time of writing this is down almost 11% in 24 hours.Now, Bitcoin is no stranger to price volatility, we’ve seen these kind of swings before. This is also not the first time China has set its sights on Bitcoin and cryptocurrencies, but this is probably the most aggressive we have seen them regarding crypto. China brought a nuclear missile to a knife fight and the nuclear option is almost always the most effective.Who can forget the first cryptocurrency ban that China instituted in 2017 and the subsequent prolonged volatility that ensured after? Although to my knowledge, the ban was never lifted, China and its local municipalities turned a blind eye to mining operations as they sucked up the cheap electricity on offer, all the while amassing a concentrated network hash rate China accounts for around 75% of Bitcoin mining.China accounts for around 75% of Bitcoin miningWhile miners are making money from computing those increasingly difficult cryptographic math problems, they are also ensuring the network operates normally and transactions are processed in a timely and safe manner. The less miners there are, the less efficient Bitcoin becomers.It is kind of ironic when you think about it. Cryptocurrency proponents love to spruik their decentralised ideals and see crypto as a better alternative to fiat (which is in many ways). Still, at the same time, Chinese miners have centralised mining operations accounting for over 50% of mining (closer to 100% than 50%).Where things get more concerning is China’s aggressive stance against Bitcoin mining in a country where collectively miners have control over the Bitcoin could allow it to order Bitcoin miners to perform rogue transactions on the network, resulting in a fork and some of the worse volatility that Bitcoin (followed by those beneath it) would ever see. Would China ever do such a thing? The fact they’ve ordered these mining operations shut makes it seem unlikely.Many of these mining operations will look to move elsewhere, most likely colder, more crypto-friendly regions like Canada or a more likely candidate: Iceland, who are certainly no strangers to the perils of the corrupt banking/financial industry, the only country to jail bankers for what happened during the GFC.Bitcoin is a threat to a country like China who is known for the tight controlling grip it has over its citizens, as well as other countries who do the same thing. Cryptocurrencies are a threat to monetary policy and government control, something an influential country like China want to stamp out before it becomes too big to control.",850
Wild Natural Deodorant Review,"I ditched harmful non-natural deodorants several months ago. Growing concerned with what they put into those store-bought deodorants like aluminium and other unnatural things that are not good for your body and take a few weeks of detoxing before you rid your body of them.I am also at odds with myself. I am the worlds biggest skeptic when it comes to most things, the effectiveness of natural deodorants is one of those things. My first foray into natural deodorants was this MooGoo one. This is a wet roll-on cream, so it’s not so much a roll-on in the traditional sense that it has that wax-type surface transfer you get from a traditional stick deodorant that I honestly prefer. I find the wet deodorants leave your armpits feeling sticky and moist, even if the MooGoo one was the most effective of a cornucopia of natural deodorants I have tried.was this MooGoo oneThe sad reality is many natural deodorants do not work, at least not compared to many non-natural alternatives. I was and still am an avid Speed Stick fan. When it comes to deodorants that are guaranteed to stop you from sweating, Speed Stick is one of those consistent products that always worked for me and that I compare any natural alternative against.The first thing you’ll learn about Wild is their marketing is grade A. They are a part of this group of new-age businesses using social media to build their brand and sell products effectively. I was excited, and everything seemed great about Wild.A sustainable product free of plasticA cool looking refillable aluminium caseA great selection of scents to choose fromA sustainable product free of plasticA cool looking refillable aluminium caseA great selection of scents to choose fromThe price seemed right and I placed my first order for Wild. I patiently waited and it took well over a month for my first order to arrive. I am in Australia and this product was shipping from the UK (presumably economy shipping), so it took longer than expected to get my first order. It took so long, that my Wild subscription was ready to renew for a refill, which I delayed.Unlike others who have to go through the two week or so detox period, I detoxed from aluminium based deodorants months ago. I had been using the MooGoo roll-on cream successfully and not smelt at the end of the day, so I had a base level to compare to.My first observation when it came to Wild was that it made me sweat, quite a lot more than usual. And I work on a computer, so I am sitting a lot of the day and not exerting myself. I know natural deodorants don’t stop you from sweating, but it is as if Wild somehow amplified the sweat.It would get to the end of the day and my armpits would be soaked. Not to mention, the number of shirts that have been ruined because this product will stain your clothes. If you sweat with Wild on and it touches your clothes, you will never wash those marks out, just throw it away.And there is the smell. Even the most ineffective natural deodorants I have tried have not left me as stench-laden as Wild has at the end of the day. I can sit mostly at the computer all day working, and by the end of the day, I smell like I am allergic to showering. If I worked in the office every day, I would be embarrassed.I wasn’t sure if this was just me; maybe Wild doesn’t work for me. So, I looked up their reviews, and on TrustPilot they have a 4.7 star rating out of 5,150 reviews, most of those overwhelmingly positive. It wasn’t until I filtered by the average, poor and bad ratings that I read similar stories to mine.on TrustPilot they have a 4.7 star rating out of 5,150 reviewsaverage, poor and bad ratingsWild seems to work well for some people, and for others like me, it works for a little period of time, and then you smell worse than you did before you used Wild. I found it even more disappointing that I experimented with not using Wild and just showering twice a day, once in the morning and one in the evening. I sweated less than I did use Wild, and I smelled better. How is it not using anything at all seemed to be more effective for me?Furthermore, the stick seems to get used up quite quickly (maybe I am using too much). It has been a couple of weeks, and the stick is already low to the point where the deodorant falls out if it is pushed up too far. Other natural alternatives I have used seem to last a month, minimum, so this was disappointing.I wanted to like Wild so badly. The marketing is slick, the packaging is slick, and on paper, the product is slick. But, for me, it did not work at all. They need to work on their formula and fix that God-awful case, a nightmare to remove the contents from if you want to replace it.The product definitely has promise, and if it works for you, then that is great. But, if you sweat more than a normal amount like me, you exert yourself (exercise, gym, vigorous activity) — then you are going to probably smell and sweat more than you ever have in your life.",1238
The latest Windows 10 update breaks gaming,"No matter how big you are, how many resources your company has, Microsoft has once again shown that you’re never too big to fail. The latest Windows 10 KB5001330 update (which many get automatically) has broken gaming.While gamers will be mostly affected, if you work with video or stream, you’ll notice the framerate drops and stuttering. The latest update actually makes streaming games through Streamlabs OBS a painful exercise.While the issues seem to be primarily NVIDIA graphics cards, AMD also seems to be affected based on some of the online chatter I’ve seen.The only solution is to roll back the update on your machine or tolerate the reduced performance and stuttering, until Microsoft pokes the code monkeys writing the patches enough to fix it.Maybe it’s time Microsoft rehired the QA division to start testing their patches again. It seems Microsoft over the last couple of years has botched their patch releases.",232
"Cyberpunk 2077 Is Slowly Being Fixed, but It’s Still Bandaids on Bullet Holes","A couple of weeks ago after a few delays, CD Projekt Red released the highly anticipated 1.2 patch which began to address some of the games biggest issues, some of which meant the game was unplayable (often fixed by a hard restart), as well as more core game level bugs.released the highly anticipated 1.2 patchAnd now, CD Projekt Red has just released a 1.21 hotfix which continues fixing some of the issues in Cyberpunk 2077. The changelog is quite long for what is just meant to be a hotfix (aka fixing highly targeted breaking bugs).released a 1.21 hotfixWith all of these new patches and hotfixes, I decided to give the game another shot. Has it improved to the point where it’s not a complete hot mess any longer? The answer is well… complicated.Cyberpunk 2077 is more or less the same game at launch. However, not exactly. The latest patches have made the game a lot more playable, many of the missions no longer bug out when you play them so you can complete them first time.Police no longer randomly spawn in unrealistic locations as they did at launch, but the police’s spawns are still terrible and unrealistic. Grand Theft Auto has had this stuff worked out since GTA 3, and it surprises me that this is even an issue in a game in 2021. Don’t get me started on the driving mechanics that feel like they are from a game maker application instead of a proper game engine.Cyberpunk 2077 is a beautiful game from a graphical perspective (especially with ray tracing on); it’s one of the most beautiful and hardware intensive games out there right now. Thanks in part to NVIDIA releasing drivers to improve compatibility and CD Projekt Red fixing things, it is way more stable.But the core underlying issues that patches cannot simply fix are still there. The main issue is CDPR released a game that was clearly six to eight months underdeveloped. This is the reason why all of these months on, we are still seeing patches for bugs that shouldn’t have been present at launch.Instead of getting new content and developing the anticipated online component, they scrapped the online aspect of Cyberpunk 2077 and have yet to release any new content in the form of DLC. CDPR are still playing catch up, buried by their own hype and overpromotion.For a game world that seems so big and detailed on the surface, once you dig beneath the neon exterior you soon discover that Cyberpunk 2077 is a hollow shell. Even the missions are a grind, “Speak to this passive aggressive NPC, drive here, shoot there, go back to NPC” nothing interesting seems to happen.Then you have one of the biggest cameos in gaming history: Keanu Reeves as the character who annoyingly lives in your head and bounces between cool and asshole like a haunted yoyo. How did CDPR stuff this up? They had the opportunity to make his presence so much more, instead they turned the nicest guy in Hollywood into an annoying jerk that you end up hating a couple of hours into the game.Can CDPR fix Cyberpunk 2077? Maybe. First impressions matter and so far they’ve blown it. ",760
NFT’s Might Resurrect the Corpse of the Music Industry,"The music industry in the last couple of decades has undergone serious change. Some might say for the better, and others might say for the worse. The music industry is not the illustrious money printing machine it was in the 90’s, thanks in part to the internet.The truth is, labels and bands were slow to adapt to the change the Internet brought. Metallica famously sued Napster in the early 2000s. While they won their case, ultimately, they changed nothing.Metallica famously sued Napster in the early 2000sAdapting to change, bands started to sell “swag”, t-shirts, tour-only items and other things that were not piratable. Labels, in turn, introduced 360 contracts allowing them to take a cut of an artists revenue from all revenue streams (not just sales) as traditional revenue streams dried up.Continuing along with the theme of exclusivity, there was a vinyl renaissance. Bands selling their albums on vinyl, including limited edition vinyl releases (sometimes numbered) which fans are willing to pay a lot of money for.Hello, NFT’sYou’ve probably heard about NFT’s before, or their longer non-abbreviated name non-fungible tokens. An NFT is a digital representation of metadata that lives on a blockchain.When many people think of blockchain, they think of cryptocurrency. While NFT’s are technically a form of cryptocurrency (a custom token living on a blockchain) their purpose is not to be transactable, an NFT is a receipt of sorts, a series of random numbers/characters representing space on a blockchain like Ethereum.Perhaps, one of the most famous examples of an NFT release is when 3LAU ran an auction for an NFT which gave you certain perks.3LAU ran an auction for an NFT
         View this post on Instagram            A post shared by ▽ (@3lau)
         View this post on Instagram            A post shared by ▽ (@3lau)         View this post on Instagram            A post shared by ▽ (@3lau)        View this post on Instagram                    View this post on Instagram View this post on Instagram                  A post shared by ▽ (@3lau)A post shared by ▽ (@3lau)Please don’t believe that Kings of Leon’s hype were the first band to release an NFT album (they weren’t). Kings of Leon might be one of the first rock bands to release an NFT album, but they’re not the first band or music artists to do so.I believe NFT’s are the second coming of the music industry, a way for artists (and in turn, labels) to find a revenue stream that cannot be imitated. An NFT takes the exclusivity aspect that bands once had with vinyl and collectables to a whole new level.",648
Eufycam Overexposure Bug Solution,"I recently bought a wireless Eufycam 2 setup for our house for piece of mind for my family and our property. The setup process was a breeze, mounting the cameras and getting things up and running also equally as easy.Everything seemed to be going well, until I noticed quite an annoying bug. The Eufycam 2 software seems to overexpose the first couple of seconds before correcting itself.The issue results in the first second and a half being lost of the trigger event.As you can see it starts out completely white and then fades to the footage it captured. Here is an event a stray dog triggered on my camera while I was writing this post.Let me tell you something, nothing you do will fix this issue. It’s actually a serious software bug and Eufy are completely aware of it. There is a community thread about the issue here.hereEufy support told me that they would have a fix ready for this in 2-3 weeks. This means most likely late March, early April 2021. It’s a bit disappointing to have such a big issue hang around for as long as it has. I’ve given them my camera serial number, and they’re promising to push a priority update to me shortly.",287
It Looks Like Thrice Are (Possibly) in the Studio Working on a New Album,"The saving of 2021 is upon us. Riley Breckenridge (the drummer of Thrice) posted a shot of a studio setup with some amps, a drum kit and a few other bits and pieces. The drum kit is also mic’d up.
         View this post on Instagram            A post shared by Riley Breckenridge (@rileybreck)
         View this post on Instagram            A post shared by Riley Breckenridge (@rileybreck)         View this post on Instagram            A post shared by Riley Breckenridge (@rileybreck)        View this post on Instagram                    View this post on Instagram View this post on Instagram                  A post shared by Riley Breckenridge (@rileybreck)A post shared by Riley Breckenridge (@rileybreck)While Riley may be working on non-Thrice related music, we know Thrice have been writing an album for over a year now (since late 2019 and throughout 2020). It seems plausible Riley is laying down some drums; Dustin shared some demo stuff a little while ago.Here is where things get even more interesting. I also believe that Thrice is working with Brian McTernan again for this new album as a producer. Some avid Thrice fans might know that Brian produced The Illusion of Safety and their most known release, The Artist In The Ambulance.There is also the possibility Riley is just recording new Less-Art material, so who honestly knows what is happening.",343
WordPress download_url Function That Supports Request Headers,"I have been working on a WordPress site that parses remote news API’s and requires not only parsing the news format JSON but getting any associated media and downloading them.Herein lies the problem: the download_url function that WordPress provides does not support passing in headers. If you are working with a remote API that requires passing an authorization header, then this function won’t work.Unbbeknownst to some, the underlying download_url function uses WordPress request methods to make the request for the file. Taking the original function from the codebase and making some simple tweaks, we end up with something like this.download_urlfunction download_url_with_headers($url, $headers = []) {
    // WARNING: The file is not automatically deleted, the script must unlink() the file.
    if ( ! $url ) {
        return new WP_Error( 'http_no_url', __( 'Invalid URL Provided.' ) );
    }
 
    $url_filename = basename( parse_url( $url, PHP_URL_PATH ) );
 
    $tmpfname = wp_tempnam( $url_filename );
    if ( ! $tmpfname ) {
        return new WP_Error( 'http_no_file', __( 'Could not create Temporary file.' ) );
    }
 
    $response = wp_safe_remote_get(
        $url,
        array(
            'timeout'  => 600,
            'stream'   => true,
            'filename' => $tmpfname,
            'headers'  => $headers
        )
    );
 
    if ( is_wp_error( $response ) ) {
        unlink( $tmpfname );
        return $response;
    }
 
    $response_code = wp_remote_retrieve_response_code( $response );
 
    if ( 200 != $response_code ) {
        $data = array(
            'code' => $response_code,
        );
 
        $tmpf = fopen( $tmpfname, 'rb' );
        if ( $tmpf ) {
            $response_size = apply_filters( 'download_url_error_max_body_size', KB_IN_BYTES );
            $data['body']  = fread( $tmpf, $response_size );
            fclose( $tmpf );
        }
 
        unlink( $tmpfname );
        return new WP_Error( 'http_404', trim( wp_remote_retrieve_response_message( $response ) ), $data );
    }
 
    return $tmpfname;
}function download_url_with_headers($url, $headers = []) {
    // WARNING: The file is not automatically deleted, the script must unlink() the file.
    if ( ! $url ) {
        return new WP_Error( 'http_no_url', __( 'Invalid URL Provided.' ) );
    }
 
    $url_filename = basename( parse_url( $url, PHP_URL_PATH ) );
 
    $tmpfname = wp_tempnam( $url_filename );
    if ( ! $tmpfname ) {
        return new WP_Error( 'http_no_file', __( 'Could not create Temporary file.' ) );
    }
 
    $response = wp_safe_remote_get(
        $url,
        array(
            'timeout'  => 600,
            'stream'   => true,
            'filename' => $tmpfname,
            'headers'  => $headers
        )
    );
 
    if ( is_wp_error( $response ) ) {
        unlink( $tmpfname );
        return $response;
    }
 
    $response_code = wp_remote_retrieve_response_code( $response );
 
    if ( 200 != $response_code ) {
        $data = array(
            'code' => $response_code,
        );
 
        $tmpf = fopen( $tmpfname, 'rb' );
        if ( $tmpf ) {
            $response_size = apply_filters( 'download_url_error_max_body_size', KB_IN_BYTES );
            $data['body']  = fread( $tmpf, $response_size );
            fclose( $tmpf );
        }
 
        unlink( $tmpfname );
        return new WP_Error( 'http_404', trim( wp_remote_retrieve_response_message( $response ) ), $data );
    }
 
    return $tmpfname;
}You can then call it like this:You can then call it like this:$temp_file = download_url_with_headers($image_url, $headers);

$filepath = ABSPATH . 'wp-content/uploads/' . $image_filename;

copy($temp_file, $filepath);
@unlink($temp_file);$temp_file = download_url_with_headers($image_url, $headers);

$filepath = ABSPATH . 'wp-content/uploads/' . $image_filename;

copy($temp_file, $filepath);
@unlink($temp_file);This is a little more simplified than the original method, as WordPress does some other stuff involving md5 hashes and whatnot you might want to add in, but I had no use for given I am working with private API’s and files.",1030
The Easiest Way To Mine Cryptocurrency in 2021,"With the price of Bitcoin hitting a new all-time high and altcoins following suit, there has been renewed interest in cryptocurrency, specifically, mining cryptocurrencies. This has also been helped by Nvidia’s new power RTX 30xx series cards (not that anyone can find a 3080 anywhere).There are numerous ways in which you can mine cryptocurrency. You have the various command line mining software that requires configuration and, sometimes, even compiling. You also have Cudo Miner, which will switch between coins to mine based on the most profitable, which is a little less configuration.One of the most profitable and easiest ways to mine cryptocurrency and earn Bitcoin is Nicehash (not an affiliate or referral link).NicehashOn my RTX 3070, I can mine around $12 at the current Bitcoin price per 24 hours. The approximate mining figure for me is $308 Australian per month, which is a decent profit. If Bitcoin keeps climbing, a month profit could be as much as 10-20% more.Having tried quite a few auto mining platforms, command line miners and other means of generating crypto, Nicehash is the most profitable I have encountered. Furthermore, it’s the only crypto mining service that doesn’t thrash my card. Cudo really smashed my graphics card, there was an almost 10 degree temperature difference between Cudo and NiceHash.In summary, Nicehash is the nicest. They offer auto mining software that interacts with their website, you can see your earnings happening in almost real-time and it’s easy. You don’t even have to install anything.",387
How To Install Eufy Security Cameras Without Drilling or Using Screws,"While we build our house, we are currently in a rental property. Sadly, the area we are renting in due to the incessant demand for rental properties is not exactly brimming with respectable individuals.Given we have two young children and we’ve worked hard to buy the items we have, we don’t want some meth-head breaking into our house and taking our stuff. After some research, we settled on the Eufy security cameras (not the Eufy 2c ones).The problem with the Eufy cameras is not only are they quite heavy, but they come with outdoor mounting brackets which require you to drill into brick/concrete with a masonry bit and then hammer in some plastic masonry plugs and screw into them. All well and good, if you’re not renting.My first instinct was double-sided mounting tape. I bought some 3M mounting tape would claimed to support upwards of 6.7kg in weight. However, to support a decent amount of weight, you need to use quite a few inches of tape to get the needed support.My first attempt to secure my Eufy camera to the brick with double-sided tape resulted in the camera staying in place for about 3 hours before plummeting to the ground. Fortunately, the camera was not damaged in the fall, just scuffed on one of the edges.For this exact reason, I do not recommend you use double-sided mounting tape to mount your Eufy cameras. I know many people online say it’s fine and have had good experiences, but why risk damaging your expensive cameras? The solution turns out to be much more simple: gutter mounts. This Amazon gutter clamp is what you are after.This Amazon gutter clamp is what you are afterThe aforementioned link to the gutter mount I bought required no drilling, taping, or permanent modifications to the house: perfect for a rental property. They come in two colours: black and white. If you have dark gutters, get the black ones and vice-versa.Despite the image not looking overly impressive, these mounts are quite sturdy and seem weather proof. Not to mention, you get more flexibility to swivel and move your cameras around than you do with the provided exterior mounting brackets that come in the Eufy box.These gutter mounts also seem to support all Eufy cameras, including Eufy 2k, Eufy 2 and Eufy 2c. The only thing you need is a ladder or a really tall person on a sturdy chair.Even after we move into our house after the build is complete, I think I prefer the idea of using gutter mounts over permanent mounts, it means you can move them around if you ever want to change the angles or positions later on, they also feel more sturdy to me.",644
Fender 75th Diamond Anniversary Telecaster Review,"I’ve been playing guitar for about 15 years now. And in those 15 years I have gone between playing every style of music you can imagine (except for country). From rock to metal, to djent, blues and shred.From extended scale Ibanez’s through to solid body Les Paul’s and semi-hollow body guitars. I’ve tried and owned a cocophany of guitars.For my birthday, my wife surprised me with the Fender 75th Anniversary Telecaster. It comes in the custom “Diamond Anniversary” metallic colour, and it’s beautiful.Unlike the ash body variant, this guitar is not made in California; it’s made in Mexico. Furthermore, it comes with a gig bag instead of a case. The gig bag is effective, and the metal Fender pick on the zips is a nice touch, but it’s certainly no case.This guitar comes with Vintera 50s pick-ups and a 6 saddle bridge. The one thing I really dig is the matching painted headstock. I know not everyone will like this, but the matching painted headstock is beautiful. I quite like it.In terms of sound, the pick-ups have a lovely warm twang to them, not overly bright or fuzzy, and they play really well for rock and blues styles of music. If you’re after a guitar for heavier music styles, you probably don’t want to buy this.For me, this guitar is a collector’s item and something I’ll have hanging on my wall. I do love the sound of it, but it looks beautiful hanging up on a wall as it stands out from every other guitar I own. I’ll probably spend more time looking at it than I will be playing it.",376
How To Stream the Neblio Blockchain With Node.js,"As interest in blockchain and cryptocurrencies heats back up in 2021, there are more blockchains in terms of choice than you can poke a stick at. It just so happens that many of these blockchains are not a beginner or intermediate friendly, requiring tooling and other steps.If you currently want to build on Ethereum, you need to write smart contracts using Solidity. It’s a whole song and dance. For EOS and other blockchains, it’s a similar story.In Neblio, things could not be any easier. They provide a REST API and Swagger documented endpoints you can use to query things like balances, token information and more on the Neblio blockchain. After working with the Hive blockchain, I had a need for the ability to stream the blockchain and monitor each block.Swagger documented endpointsWhile the Neblio REST API does not currently offer the ability to stream blocks, you can interact with the Neblio wallet (either locally or on a server) and make JSON-RPC calls to it. The Neblio wallet works very similarly to the Bitcoin wallet in what kind of calls you can make to it and quite a few other cryptocurrencies which are based on a similar premise.Why would I want to stream the Neblio blockchain?In my case, I wanted to monitor transactions on the Neblio blockchain and then I wanted to look for specific NTP1 transactions with metadata inside of them and parse it. Every 30s a new block is produced, a block can contain one or more transactions.Basically, I want to use Neblio as a database, with the ability to read to and from the blockchain. This will allow you to build your own sidechain, as you can parse each Neblio block, store it into your own database (MongoDB, PostgreSQL, Firebase, etc) and use Neblio for verifying the blocks.Before we get to the code, you need to update your neblio.conf file.neblio.confTo find the location of your neblio.conf file, in the wallet click help and then select “Debug Window”neblio.confNow, click open for the “Show data directory” and this is where your configuration file will live.Add the following into your neblio.conf and feel free to change anything, from the username/password as well as the port.neblio.confserver=1
rpcuser=user
rpcpassword=password
rpcport=6326server=1
rpcuser=user
rpcpassword=password
rpcport=6326We now have a wallet which can accept JSON-RPC commands, we just need to make sure when we make a request, we supply the user and password values along with the request in plaintext.Before we proceed, I am assuming you have the latest version of Node.js installed, if not, download it from the official Node.js website here.hereCreate a new file called streamer.js and add in the following:streamer.jsconst request = require('request');

// Create an async function which queries the local Neblio wallet
// It accepts a method name and an array of parameter values
async function walletRequest(method, params = []) {
  return new Promise((resolve, reject) => {
    
    // By default, the Neblio wallet will run on port 6326
    let options = {
      url: ""http://127.0.0.1:6326"",
      method: ""post"",
      headers:
      { 
       ""content-type"": ""text/plain""
      },
      auth: {
          user: 'user',
          pass: 'password'
      },
      body: JSON.stringify( {
        ""jsonrpc"": ""1.0"", 
        ""id"": ""neblio-contracts"", 
        ""method"":method, 
        ""params"": params
      })
    };
    
    request(options, (error, response, body) => {
      if (error) {
          reject(error);
      } else {
          resolve(JSON.parse(body));
      }
    });
  });
}const request = require('request');

// Create an async function which queries the local Neblio wallet
// It accepts a method name and an array of parameter values
async function walletRequest(method, params = []) {
  return new Promise((resolve, reject) => {
    
    // By default, the Neblio wallet will run on port 6326
    let options = {
      url: ""http://127.0.0.1:6326"",
      method: ""post"",
      headers:
      { 
       ""content-type"": ""text/plain""
      },
      auth: {
          user: 'user',
          pass: 'password'
      },
      body: JSON.stringify( {
        ""jsonrpc"": ""1.0"", 
        ""id"": ""neblio-contracts"", 
        ""method"":method, 
        ""params"": params
      })
    };
    
    request(options, (error, response, body) => {
      if (error) {
          reject(error);
      } else {
          resolve(JSON.parse(body));
      }
    });
  });
}This forms the basis of the code that will make requests to our Neblio wallet. You can change the ID to whatever you want, this name I am using symbolises an app I am building on Neblio, some developers like to supply Date.now() as the ID value.Date.now()Add the following code beneath the function we created above. This is where we will stream the Neblio blockchain and will get the current block (including any transactions).// The last valid block height
let lastBlockNum = 0;

// Utility function we can await to introduce delays into our app
async function sleep(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

// Continuously polls the blockchain every second for the latest block height
const stream = setInterval(async () => {
  const latestBlockHeight = await getBlockCount();

  if (latestBlockHeight) {
    lastBlockNum = latestBlockHeight;
  }
}, 1000);

// Helper function for getting the latest block height (block number)
async function getBlockCount() {
  const response = await walletRequest('getblockcount');

  if (response.result) {
    return response.result;
  }

  return null;
}

// Helper function for getting a block by number as well as metadata
async function getBlock(blockNumber) {
  return walletRequest('getblockbynumber', [blockNumber, true]);
}

// Function is called for every new block
async function handleBlock(blockNum) {
  // If the latest block from the blockchain is greater than or equal to the supplied block
  if (lastBlockNum >= blockNum) {
    
    // Load the block
    const block = await getBlock(blockNum);
	
    // Display contents of the block
    console.log(block);
    
    // If result is not null, it's a valid block
    if (block.result) {
      console.log(`New block height is ${blockNum} ${block.result.time}`);
      
      // Recrusively call this function again, incrementing the block number
      handleBlock(blockNum + 1);
    } else {
      // We have tried parsing a block that does not exist, call this function recursively
      console.error(`Block does not exist`);
      handleBlock(blockNum);
    }
  } else {
    // Block isn't ready yet, sleep for 500 milliseconds
    await sleep(500);
    
    // Call this function again recrusively, pass block number
    handleBlock(blockNum);
  }
}

// This function starts the streaming process
// if you do not pass in a block number, always starts from the latest
async function streamNeblio(startingBlock = 0) {
  // No starting block specified, we'll get the latest
  if (startingBlock === 0) {
    // Get latest block height
    const latestBlockHeight = await getBlockCount();
    
    // If value is valid, set block height
    if (latestBlockHeight) {
      lastBlockNum = latestBlockHeight;
    }
  } else {
    // User has supplied a block number to start from
    lastBlockNum = startingBlock;
  }

  console.log(`Starting from block ${lastBlockNum}`);
  
  // Kick off the block loading process
  handleBlock(lastBlockNum);
}

// Call streamNeblio to begin ""streaming"" the Neblio blockchain
streamNeblio();// The last valid block height
let lastBlockNum = 0;

// Utility function we can await to introduce delays into our app
async function sleep(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

// Continuously polls the blockchain every second for the latest block height
const stream = setInterval(async () => {
  const latestBlockHeight = await getBlockCount();

  if (latestBlockHeight) {
    lastBlockNum = latestBlockHeight;
  }
}, 1000);

// Helper function for getting the latest block height (block number)
async function getBlockCount() {
  const response = await walletRequest('getblockcount');

  if (response.result) {
    return response.result;
  }

  return null;
}

// Helper function for getting a block by number as well as metadata
async function getBlock(blockNumber) {
  return walletRequest('getblockbynumber', [blockNumber, true]);
}

// Function is called for every new block
async function handleBlock(blockNum) {
  // If the latest block from the blockchain is greater than or equal to the supplied block
  if (lastBlockNum >= blockNum) {
    
    // Load the block
    const block = await getBlock(blockNum);
	
    // Display contents of the block
    console.log(block);
    
    // If result is not null, it's a valid block
    if (block.result) {
      console.log(`New block height is ${blockNum} ${block.result.time}`);
      
      // Recrusively call this function again, incrementing the block number
      handleBlock(blockNum + 1);
    } else {
      // We have tried parsing a block that does not exist, call this function recursively
      console.error(`Block does not exist`);
      handleBlock(blockNum);
    }
  } else {
    // Block isn't ready yet, sleep for 500 milliseconds
    await sleep(500);
    
    // Call this function again recrusively, pass block number
    handleBlock(blockNum);
  }
}

// This function starts the streaming process
// if you do not pass in a block number, always starts from the latest
async function streamNeblio(startingBlock = 0) {
  // No starting block specified, we'll get the latest
  if (startingBlock === 0) {
    // Get latest block height
    const latestBlockHeight = await getBlockCount();
    
    // If value is valid, set block height
    if (latestBlockHeight) {
      lastBlockNum = latestBlockHeight;
    }
  } else {
    // User has supplied a block number to start from
    lastBlockNum = startingBlock;
  }

  console.log(`Starting from block ${lastBlockNum}`);
  
  // Kick off the block loading process
  handleBlock(lastBlockNum);
}

// Call streamNeblio to begin ""streaming"" the Neblio blockchain
streamNeblio();If you have your Neblio wallet running, running the above code would begin to stream the Neblio blockchain. But, how do we get the block data out? We’re streaming the blockchain, but we have no way to get the block itself.There are a few ways to do this, but I’m going to add in a subscription based callback approach here.We are going to add in two new functions and variables. First things first, beneath the lastBlockNum variable, add the following:lastBlockNumconst subscriptions = [];
const ntp1Subscriptions = [];const subscriptions = [];
const ntp1Subscriptions = [];Now, above the streamNeblio function, let’s add two functions which will register our subscription callback functions for us.streamNebliofunction addSubscription(callback) {
  subscriptions.push({ callback });
}

function addNtp1Subscription(callback) {
  ntp1Subscriptions.push({ callback });
}function addSubscription(callback) {
  subscriptions.push({ callback });
}

function addNtp1Subscription(callback) {
  ntp1Subscriptions.push({ callback });
}The code is self-explanatory. We take the supplied function and push it into an array of subscriptions. These subscriptions will be called from within the handleBlock method.handleBlockNow, let’s refactor the handleBlock function to call our callback functions when things change.handleBlock// Function is called for every new block
async function handleBlock(blockNum) {
  // If the latest block from the blockchain is greater than or equal to the supplied block
  if (lastBlockNum >= blockNum) {
    
    // Load the block
    const block = await getBlock(blockNum);
    
    // If result is not null, it's a valid block
    if (block.result) {
      console.log(`New block height is ${blockNum} ${block.result.time}`);
      
        // Loop over all subscriptions and call the supplied callback, passing the block result in
      for (const sub of subscriptions) {
        sub.callback(block.result);
      }

      // Does this block contain any ntp1 token transactions?
      // we use a reduce to create a new array of ntp1 tokens
      const ntp1Transactions = block.result.tx.reduce((acc, value) => {
        if (value.ntp1) {
          acc.push(value);
        }

        return acc;
      }, []);

      // Does this block have any ntp1 transactions?
      if (ntp1Transactions.length) {
        // Call one or more ntp1 subscriptions, pass the block result as the first argument
        // and pass the array of ntp1 transactions as the second argument
        for (const sub of ntp1Subscriptions) {
          sub.callback(block.result, ntp1Transactions);
        }
      }
      
      // Recrusively call this function again, incrementing the block number
      handleBlock(blockNum + 1);
    } else {
      // We have tried parsing a block that does not exist, call this function recursively
      console.error(`Block does not exist`);
      handleBlock(blockNum);
    }
  } else {
    // Block isn't ready yet, sleep for 500 milliseconds
    await sleep(500);
    
    // Call this function again recrusively, pass block number
    handleBlock(blockNum);
  }
}// Function is called for every new block
async function handleBlock(blockNum) {
  // If the latest block from the blockchain is greater than or equal to the supplied block
  if (lastBlockNum >= blockNum) {
    
    // Load the block
    const block = await getBlock(blockNum);
    
    // If result is not null, it's a valid block
    if (block.result) {
      console.log(`New block height is ${blockNum} ${block.result.time}`);
      
        // Loop over all subscriptions and call the supplied callback, passing the block result in
      for (const sub of subscriptions) {
        sub.callback(block.result);
      }

      // Does this block contain any ntp1 token transactions?
      // we use a reduce to create a new array of ntp1 tokens
      const ntp1Transactions = block.result.tx.reduce((acc, value) => {
        if (value.ntp1) {
          acc.push(value);
        }

        return acc;
      }, []);

      // Does this block have any ntp1 transactions?
      if (ntp1Transactions.length) {
        // Call one or more ntp1 subscriptions, pass the block result as the first argument
        // and pass the array of ntp1 transactions as the second argument
        for (const sub of ntp1Subscriptions) {
          sub.callback(block.result, ntp1Transactions);
        }
      }
      
      // Recrusively call this function again, incrementing the block number
      handleBlock(blockNum + 1);
    } else {
      // We have tried parsing a block that does not exist, call this function recursively
      console.error(`Block does not exist`);
      handleBlock(blockNum);
    }
  } else {
    // Block isn't ready yet, sleep for 500 milliseconds
    await sleep(500);
    
    // Call this function again recrusively, pass block number
    handleBlock(blockNum);
  }
}We remove the console.log we had in our handleBlock method and then we add in some loops which iterate over our subscriptions and call them. For ntp1 transactions, we loop over the tx array and if the boolean ntp1 is truthy, we create an array of transactions.console.loghandleBlocktxntp1Putting it all togetherThe complete result of our streamer looks like this.const request = require('request');

async function walletRequest(method, params = []) {
  return new Promise((resolve, reject) => {
    let options = {
      url: ""http://127.0.0.1:6326"",
      method: ""post"",
      headers:
      { 
       ""content-type"": ""text/plain""
      },
      auth: {
          user: 'user',
          pass: 'password'
      },
      body: JSON.stringify( {
        ""jsonrpc"": ""1.0"", 
        ""id"": ""neblio-contracts"", 
        ""method"":method, 
        ""params"": params
      })
    };
    
    request(options, (error, response, body) => {
      if (error) {
          reject(error);
      } else {
          resolve(JSON.parse(body));
      }
    });
  });
}

let lastBlockNum = 0;
const subscriptions = [];
const ntp1Subscriptions = [];

async function sleep(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

const stream = setInterval(async () => {
  const latestBlockHeight = await getBlockCount();

  if (latestBlockHeight) {
    lastBlockNum = latestBlockHeight;
  }
}, 1000);

async function getBlockCount() {
  const response = await walletRequest('getblockcount');

  if (response.result) {
    return response.result;
  }

  return null;
}

async function getBlock(blockNumber) {
  return walletRequest('getblockbynumber', [blockNumber, true]);
}

async function handleBlock(blockNum) {
  if (lastBlockNum >= blockNum) {
    const block = await getBlock(blockNum);

    if (block.result) {
      console.log(`New block height is ${blockNum} ${block.result.time}`);

      // Loop over all subscriptions and call the supplied callback, passing the block result in
      for (const sub of subscriptions) {
        sub.callback(block.result);
      }

      // Does this block contain any ntp1 token transactions?
      // we use a reduce to create a new array of ntp1 tokens
      const ntp1Transactions = block.result.tx.reduce((acc, value) => {
        if (value.ntp1) {
          acc.push(value);
        }

        return acc;
      }, []);

      // Does this block have any ntp1 transactions?
      if (ntp1Transactions.length) {
        // Call one or more ntp1 subscriptions, pass the block result as the first argument
        // and pass the array of ntp1 transactions as the second argument
        for (const sub of ntp1Subscriptions) {
          sub.callback(block.result, ntp1Transactions);
        }
      }

      handleBlock(blockNum + 1);
    } else {
      console.error(`Block does not exist`);
      handleBlock(blockNum);
    }
  } else {
    await sleep(500);
    handleBlock(blockNum);
  }
}

function addSubscription(callback) {
  subscriptions.push({ callback });
}

function addNtp1Subscription(callback) {
  ntp1Subscriptions.push({ callback });
}

async function streamNeblio(startingBlock = 0) {
  if (startingBlock === 0) {
    const latestBlockHeight = await getBlockCount();
    
    if (latestBlockHeight) {
      lastBlockNum = latestBlockHeight;
    }
  } else {
    lastBlockNum = startingBlock;
  }

  console.log(`Starting from block ${lastBlockNum}`);
  handleBlock(lastBlockNum);
}const request = require('request');

async function walletRequest(method, params = []) {
  return new Promise((resolve, reject) => {
    let options = {
      url: ""http://127.0.0.1:6326"",
      method: ""post"",
      headers:
      { 
       ""content-type"": ""text/plain""
      },
      auth: {
          user: 'user',
          pass: 'password'
      },
      body: JSON.stringify( {
        ""jsonrpc"": ""1.0"", 
        ""id"": ""neblio-contracts"", 
        ""method"":method, 
        ""params"": params
      })
    };
    
    request(options, (error, response, body) => {
      if (error) {
          reject(error);
      } else {
          resolve(JSON.parse(body));
      }
    });
  });
}

let lastBlockNum = 0;
const subscriptions = [];
const ntp1Subscriptions = [];

async function sleep(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

const stream = setInterval(async () => {
  const latestBlockHeight = await getBlockCount();

  if (latestBlockHeight) {
    lastBlockNum = latestBlockHeight;
  }
}, 1000);

async function getBlockCount() {
  const response = await walletRequest('getblockcount');

  if (response.result) {
    return response.result;
  }

  return null;
}

async function getBlock(blockNumber) {
  return walletRequest('getblockbynumber', [blockNumber, true]);
}

async function handleBlock(blockNum) {
  if (lastBlockNum >= blockNum) {
    const block = await getBlock(blockNum);

    if (block.result) {
      console.log(`New block height is ${blockNum} ${block.result.time}`);

      // Loop over all subscriptions and call the supplied callback, passing the block result in
      for (const sub of subscriptions) {
        sub.callback(block.result);
      }

      // Does this block contain any ntp1 token transactions?
      // we use a reduce to create a new array of ntp1 tokens
      const ntp1Transactions = block.result.tx.reduce((acc, value) => {
        if (value.ntp1) {
          acc.push(value);
        }

        return acc;
      }, []);

      // Does this block have any ntp1 transactions?
      if (ntp1Transactions.length) {
        // Call one or more ntp1 subscriptions, pass the block result as the first argument
        // and pass the array of ntp1 transactions as the second argument
        for (const sub of ntp1Subscriptions) {
          sub.callback(block.result, ntp1Transactions);
        }
      }

      handleBlock(blockNum + 1);
    } else {
      console.error(`Block does not exist`);
      handleBlock(blockNum);
    }
  } else {
    await sleep(500);
    handleBlock(blockNum);
  }
}

function addSubscription(callback) {
  subscriptions.push({ callback });
}

function addNtp1Subscription(callback) {
  ntp1Subscriptions.push({ callback });
}

async function streamNeblio(startingBlock = 0) {
  if (startingBlock === 0) {
    const latestBlockHeight = await getBlockCount();
    
    if (latestBlockHeight) {
      lastBlockNum = latestBlockHeight;
    }
  } else {
    lastBlockNum = startingBlock;
  }

  console.log(`Starting from block ${lastBlockNum}`);
  handleBlock(lastBlockNum);
}Is it the prettiest? Definitely not. Does it need more work before using it in production? Absolutely. But, for what started out as a proof of concept, it works beautiful.Running itNow we have a function streamer, we can run it.Stream the Neblio blockchain and call a function for every new blockEvery new block loaded, we will console log the output of it.addSubscription(block => {
    console.log(block);
});

streamNeblio();addSubscription(block => {
    console.log(block);
});

streamNeblio();Running the above should result in seeing blocks being console logged in your console that looks similar to the following. Every 30s you should see a new block printed out.Get a specific block containing one or more NTP1 transactionsWe tell the streamer to start at a specific block height which contains an NTP1 transaction I put onto the chain. The ntp1 subscription will fire and the ntp1 transactions console logged for this particular block.addNtp1Subscription((block, txs) => {
  console.log(txs);
});

streamNeblio(2699709);addNtp1Subscription((block, txs) => {
  console.log(txs);
});

streamNeblio(2699709);This is the result you should see in your console.Get a specific block containing one or more NTP1 transactions (and parse the userData)The above block contains an NTP1 transaction I made, this transaction also contains some metadata. Here is how you would get the metadata for each transaction.addNtp1Subscription((block, txs) => {
    for (const tx of txs) {
        const { metadataOfUtxos: { userData: { meta } } } = tx;
        console.log(meta);
    }
});

streamNeblio(2699709);addNtp1Subscription((block, txs) => {
    for (const tx of txs) {
        const { metadataOfUtxos: { userData: { meta } } } = tx;
        console.log(meta);
    }
});

streamNeblio(2699709);This is the result you should see in your console from the above command.ConclusionThis blog post shows you how you can effectively use the Neblio blockchain as a database. I am currently in the process of turning the above into a reusable library you can use to build sidechain-esque applications that leverage Neblio behind the scenes but have a layer on-top.DonationsIf this blog post helped you and you’re feeling generous, I am accepting Neblio donations to my Neblio wallet here: NQvjvfzRh2DS5CfeebSNY4dByMwrRQGTYX — Any support will go towards building the above into the planned sidechain library (which will have database support and adapters for different databases, including Firebase).NQvjvfzRh2DS5CfeebSNY4dByMwrRQGTYX",6013
Coinspot Is Terrible: Here’s a Better Alternative for Australian Cryptocurrency Trading,"I have been trading cryptocurrencies since early 2017. And for a good while, Australians had limited options for reputable and secure exchanges to trade on. The first exchange I joined was Coinspot, which seemed like a decent option at the time in 2017. This was when Binance didn’t have an Australian version of its exchange either.Since then, a cornucopia of cryptocurrency exchanges and platforms have become accessible to Australians, competition is now fierce.Look, Coinspot has proven itself to be a well-run platform, but how they structure their fees is a whole other ballgame. What you do not realise is you are losing more money than you need to on high fees.how they structure their feesThe buy and sell order functionality is pushed front and centre, which just so happens to have fees of 1%, which is exorbitant for a cryptocurrency exchange. If you want lower fees on Coinspot, you have to use the market orders functionality, which many people don’t use or don’t know is there.A 1% fee might sound low already to inexperienced cryptocurrency traders. That’s $10 for a $1000 trade which is no big deal, right? If you’re planning on buying multiple currencies and trading between them, all of a sudden, you’re spending 1% for each trade.Fees matter. You should care about how much you’re spending on fees.I don’t think Coinspot is intentionally misleading people here, they are trying to offer a platform which rivals Coinbase for ease of use. Making it as frictionless as possible to buy and sell cryptocurrencies, people intimidated by charts and real-time order books will always go for the easiest option.But, look at this UI. Can you tell me where the market functionality is to get the 0.1% fees?The buy, sell and swap buttons will result in 1% fees. If you’re frequently making buys, sells and swaps, that can add up to a few per cent. And then you have the spreads, on some cryptocurrencies like StormX the spread rate (the difference between buy and sells) recently was exorbitant.I understand why these buttons are more prominent, and I don’t think it’s a dark UI pattern designed to get an additional few points of a per cent in fee revenue. It’s just the easiest option for most people. And offerings like Coinspot are appealing to newcomers and inexperienced traders because they don’t overload you with options.If there is only one piece of advice you take, it is the best exchange around is Binance. Not only do they have more markets and pairings you can trade, but if you hold BNB (Binance’s native token), you can use it to lower your fees, so the fees go below 0.1%. Seriously, look at how low my trading fees on Binance are.If you plan on making frequent trades, you want your fees to be as low as possible. I’ve been trading on Binance actively since 2018, and now they have an Australian offering. They support instant deposits using a card and Pay ID, as well as instant Pay ID withdrawals.Despite having some of the lowest fees around and more coin pairings than most small to medium exchanges, one of the biggest reasons to join Binance is their support of airdrops. Not all exchanges will support all airdrops, but Binance always supports airdrops (most recently for me the XYM airdrop) even if they don’t actively trade the airdropped token.If you join Binance using my referral link here, you’ll get back 5% fees on every trade you make. I get a bonus for referring new users to Binance, but by using the link, you will be getting back money on trading fees (which you were already going to do regardless of the link) can add up. Really, it’s a win-win, and you’re saving money compared to the likes of Coinspot.using my referral link here",920
"If Linkin Park Were Ever Looking for a New Vocalist, Sam Carter of Architects Should Be First in Line","For some reason, the tragic loss of Chester Bennington the frontman of Linkin Park still affects me to this day. I didn’t personally know him, but Linkin Park was the soundtrack to my teenage years (like many others my age).I don’t even know if Linkin Park will ever return, Chester is irreplaceable, in my opinion. One of the most talented vocalists in the genre could switch between his unique singing style to Usain Bolt stamina levels of screaming.However, if Linkin Park were to ever reform, in terms of vocalists who could fill the big shoes Chester Bennington left, Sam Carter (frontman of UK band Architects) should be the first consideration. After listening to the new Architects album For Those That Wish To Exist, the thought just naturally popped into my head.If you were fortunate to witness Architects play their Royal Albert Hall online performance in 2020, then you would have witnessed Sam Carter give everyone a master class in vocals. Being a studio singer/screamer is one thing, but being able to sing and scream with power in a live setting without the aid of double/triple/quad tracking is a whole other ballgame.I think Sam could help Linkin Park continue in their more experimental/electronic territory. Still, his screams/lows could also have the older catalogue of LP tracks covered and some new heavier stuff if they were to explore revisiting some heavy aspects.The important thing worth noting here is that Chester cannot be replaced, that the last thing Linkin Park needs a Chester soundalike to try and continue the band with the same vocal style. While there are similarities between Sam and Chester in vocal techniques, Sam is unique enough in his own way that it wouldn’t feel like an attempted 1:1 replacement.If any of the Linkin Park band members are reading this, if you want a new vocalist: call Sam Carter of Architects first before anyone else.",472
Would Bing or DuckDuckGo Even Pay for Linking to News in Australia?,"In case you missed it, Australia is currently trying to introduce a media bargaining code forcing Facebook, Google and presumably any other company that links to news to pay money to media organisations.It’s as crazy as it sounds.media bargaining codeGoogle has recently said that if the code were approved in its current form, it would pull search out of Australia. Some think Google might be bluffing, but I assure you that they are not. They will make good on their threat if they’re willing to go on the record.it would pull search out of AustraliaThis poses the question, if Bing or DuckDuckGo were to become the likely alternatives, would Microsoft (owner of Bing) or DuckDuckGo cave into the government’s demands and pay news companies?The issues I have with the code are the precedent it would set. Where do we draw the line and open up the floodgates for other content producers and providers to make the same demands that search engines and media companies pay for linking to other types of content?I’m not a lawyer, but I know what a precedent is. This would set what I think is a dangerous precedent in its current form. Think beyond Australia, and this would be rolled out to other countries as empowered news organisations go on a feeding frenzy. When you think about how Google works, they are linking to news sites from organisations forced to go online after the internet killed their traditional print distribution. If anything, Google is helping news organisations by providing them with free reach, the same reach they tried to fight in the early 00’s as print media began to die.

The only losers in this situation are Australian internet users. I doubt Bing or DuckDuckGo will commit to paying news organisations any money. This code isn’t about playing fair. This code is retaliation from media organisations who had their businesses severed by the internet and are now looking for a cash handout because they second-guessed the impact of the internet on their business models.If we want to do what is fair, let’s make multinationals pay their fair share of tax, including News Corp, who receive millions in grants but have paid zero tax. News Corp is the most likely beneficiary of this proposed code anyway.",558
Microsoft Flight Simulator 2020 Settings for The HP Reverb G2 Headset and RTX 3070 Graphics Card,"Having recently picked up a HP Reverb G2 headset and being excited to play Microsoft Flight Simulator 2020, I thought I would share my settings with the game running nicely with this headset and my Nvidia RTX 3070 graphics card.It is no secret that MSFS 2020 is a game that will stress even the highest spec of graphics cards. The 3070 not being the highest-rated GPU you can get right now won’t let you max the settings for MSFS 2020 and provide optimal FPS, but it is adequate with some tweaking.These settings are a culmination of trial and error and numerous Reddit threads, blog posts and other settings/feedback shared by other VR users (not always necessarily Reverb G2). Settings will always be dependent on the type of hardware you have, what drivers you have installed, what apps you have open and other variables beyond this post’s control.Game settingsGame settingsThe following settings will give you the best quality visuals as well as adequate performance for the Reverb G2 WMR headset. The 3070 is not one of the higher spec 30xx series cards, so it can only be pushed so far for VR purposes.Before we get into the game-specific graphics settings, you need to disable V-SYNC. If you do not, the refresh rate will be limited to your monitor.RENDER-SCALING: 100ANTI-ALIASING: TAATERRAIN LEVEL OF DETAIL: 50TERRAIN VECTOR DATA: MEDIUMBUILDINGS: MEDIUMTREES: MEDIUMGRASS AND BUSHES: MEDIUMOBJECTS LEVEL OF DETAIL: 50VOLUMETRIC CLOUDS: MEDIUMTEXTURE RESOLUTION: HIGHANISOTROPIC FILTERING: 8xTEXTURE SUPERSAMPLING: 2X2TEXTURE SYNTHESIS: MEDIUMWATER WAVES: MEDIUMSHADOW MAPS: 1024TERRAIN SHADOWS: 256CONTACT SHADOWS: OFFWINDSHIELD EFFECTS: HIGHAMBIENT OCCLUSION: OFFREFLECTIONS: OFFLIGHT SHAFTS: OFFBLOOM: OFFGLASS COCKPIT REFRESH RATE: HIGHRENDER-SCALING: 100RENDER-SCALING:ANTI-ALIASING: TAAANTI-ALIASING:TERRAIN LEVEL OF DETAIL: 50TERRAIN LEVEL OF DETAIL:TERRAIN VECTOR DATA: MEDIUMTERRAIN VECTOR DATA:BUILDINGS: MEDIUMBUILDINGS:TREES: MEDIUMTREES:GRASS AND BUSHES: MEDIUMGRASS AND BUSHES:OBJECTS LEVEL OF DETAIL: 50OBJECTS LEVEL OF DETAIL:VOLUMETRIC CLOUDS: MEDIUMVOLUMETRIC CLOUDS: TEXTURE RESOLUTION: HIGHTEXTURE RESOLUTION: ANISOTROPIC FILTERING: 8xANISOTROPIC FILTERING:TEXTURE SUPERSAMPLING: 2X2TEXTURE SUPERSAMPLING:TEXTURE SYNTHESIS: MEDIUMTEXTURE SYNTHESIS:WATER WAVES: MEDIUMWATER WAVES:SHADOW MAPS: 1024SHADOW MAPS: TERRAIN SHADOWS: 256TERRAIN SHADOWS:CONTACT SHADOWS: OFFCONTACT SHADOWS:WINDSHIELD EFFECTS: HIGHWINDSHIELD EFFECTS:AMBIENT OCCLUSION: OFFAMBIENT OCCLUSION:REFLECTIONS: OFFREFLECTIONS:LIGHT SHAFTS: OFFLIGHT SHAFTS:BLOOM: OFFBLOOM:GLASS COCKPIT REFRESH RATE: HIGHGLASS COCKPIT REFRESH RATE: One thing to keep in mind is that these settings are more of a baseline of which you can adjust to suit your tastes, and in some cases, out of necessity. If you’re flying in less busy areas like deserts and small island areas, you can increase Render Scaling and Terrain Level of Detail values up. For busy cities like New York or London, you’ll have to come back to the baseline.Where these settings will differ for you is CPU bottlenecks. MSFS 2020 is a notorious game for CPU bottlenecks, and in VR, more CPU is needed than rendering on the monitor. This means if your CPU can’t handle VR, you might even have to bump things like Volumetric Clouds down to low.Considering I do not have the highest spec GPU, I am surprised how good MSFS 2020 works with these settings in VR. I find it difficult to play MSFS 2020 any other way now I have tried VR.",870
"Google Threatens To Pull Search Out of Australia Over the Proposed News Media Bargaining Code, and I Am Honestly All for It","The proposed Australian News Media Bargaining Code is a contentious code of conduct that will compel digital companies like Facebook and Google to pay for any news content they link to. Australian News Media Bargaining CodeIn simple terms, if Google links to a news story, they are required to pay for it under this proposed mandatory code of conduct, as far Facebook and other digital companies.And if you think this sounds like a stupid idea that would never be enacted into law, you seriously underestimate the influence that Rupert Murdoch and News Corp have in Australia.Don’t just take my word for it, the inventor of the World Wide Web himself Tim Berners-Lee has come out against this proposed code of conduct.Tim Berners-Lee has come out against this proposed code of conductAnd things could possibly get a lot worse. Google has said that if the code of conduct were to be implemented as it currently stands, that they would withdraw Google search from the Australian market.they would withdraw Google search from the Australian marketThis would put Australia into a unique category alongside China and North Korea as well as a few other countries where Google search is not available. Imagine not being able to use Google Search in Australia like you’re living in a tyrannical dictatorship?Speaking of North Korea, they recently just criticised Australia for humans rights abuses amongst other claims. The sad thing is, they are right, 2021 is off to a great start.recently just criticised Australia for humans rights abuses amongst other claimsLet’s be honest, a majority of Australian news is produced by News Corporation which is owned by Rupert Murdoch, a controversial and widely hated media figurehead who has done a lot of damage through the numerous media organisations that he owns.Look no further than Fox News, which enabled the terrible Trump presidency and has caused significant harm. Similarly, Sky News over in Australia (which is just an Australian version of Fox News) is more of the same biased drivel Fox News is known for.The biggest recipients of any such compensation from Facebook and Google would be News Corp. Let’s really let the irony of that sink in for a moment. In the five years to 2018-2019, News Corp paid zero Australian tax on $360 million in profits.News Corp paid zero Australian tax on $360 million in profitsZero.Let it sink in further. The Australian Government and ACCC heavily lobbied by large multinational news organisations who pay zero tax want Facebook and Google to pay their “fair share” and level the playing field? What is fair about that?They might as well rename this the, “News Corp media mafia protection racket code” at this point. News Corp stands to benefit the most for their abysmal soulless reporting.And look, I don’t feel sorry for Facebook or Google. Let’s face it, these companies (amongst many other multinationals) are taking taxpayers for a ride and not paying their fair share. I think Facebook and Google should pay, not for links to clickbait and biased reporting, but taxes.How does Google benefit from linking to the news? I have yet to see a factual and accurate dataset that proves Google or any other site is profiting from news articles. I am a millennial, and I get most of my news from other sources, but I certainly do not get it from Facebook or Google.If you want a level playing field, forget media organisations who have failed to embrace the internet, make multinationals (especially media organisations masquerading as Australian companies when they are not) pay tax. Close the loopholes, stop letting Australians get ripped off.It’s clear that the Australian government does not care about companies exploiting tax loopholes, opting to go after everyday Australians (see robodebt which saw over 470,000 erroneous debts issued, some people committing suicide over them) instead of the fruit at the top of the tree. And, now protecting the Murdoch news mafia once again with this proposed code.see robodebtImagine being the leader of an allegedly first-world country responsible for lumping your country in with China and North Korea being unable to freely access information on Google because it is banned?",1051
WordPress Custom REST API Endpoints With Spaces,"I have been working on quite an ambitious WordPress project that has a lot of custom REST API endpoints to work alongside an Aurelia SPA application running on top of WordPress.In this project, I am working with a location JSON file which is 34mb in size. Returning the entire file contents would be a disaster, so I created an endpoint with the ability to search this endpoint. As you might have learned if you are reading this, search queries can sometimes have spaces.The space in a URL is denoted by the following %20 in between each word. My URL looks something like this: http://somewebsite.com/wp-json/utilities/v1/locations/south%20america%20http://somewebsite.com/wp-json/utilities/v1/locations/south%20americaIf you attempt to create an endpoint, you are probably going to run into an error (a 404) as the space won’t be parsed and the URL won’t be recognised as being valid.Here is how I solved this issueHere is how I solved this issueregister_rest_route( 'utilities/v1', '/locations/(?P<region>([a-zA-Z]|%20)+)', array(
  'methods' => 'GET',
  'callback' => 'rest_get_locations',
) );register_rest_route( 'utilities/v1', '/locations/(?P<region>([a-zA-Z]|%20)+)', array(
  'methods' => 'GET',
  'callback' => 'rest_get_locations',
) );Notice I am including the %20 in the regular expression? You need to do this, or it won’t be parsed when the regular expression is being checked to match the URL.%20Inside of my callback function, I am also decoding the URL to ensure no special characters remain in the URL.$region = strtolower(urldecode($request->get_param('region')));$region = strtolower(urldecode($request->get_param('region')));",412
Working With slot & au-slot in Aurelia 2,"In Aurelia 2, we have a couple of different ways of working with slotted content. If you are familiar with Web Components, the slot element is a placeholder that can have content projected into it, the common use for this is reusable components.In Aurelia 1, we had support for the slot element and could project almost anything we wanted inside of it, but there were limitations (as per the spec) which meant it wasn’t always the best tool for the job.In Aurelia 2 we still have the slot element, but we now also have an au-slot element as well.Here is a basic modal that I created in one of my Aurelia 2 applications. Notice the use of au-slot inside of this element instead of slot?<div class=""modal"" if.bind=""showing"" ref=""modal"">
    <button type=""button"" data-action=""close"" class=""close"" aria-label=""Close"" click.trigger=""close()"" ><span aria-hidden=""true"">&times;</span></button>
    <div class=""modal-inner"">
        <au-slot></au-slot>
    </div>
</div><div class=""modal"" if.bind=""showing"" ref=""modal"">
    <button type=""button"" data-action=""close"" class=""close"" aria-label=""Close"" click.trigger=""close()"" ><span aria-hidden=""true"">&times;</span></button>
    <div class=""modal-inner"">
        <au-slot></au-slot>
    </div>
</div>And here is a couple of different ways that I use the modal:<modal showing.bind=""showImagePreview"">
    <attachments-preview au-slot attachments.bind=""imagesForPreview""></attachments-preview>
</modal><modal showing.bind=""showImagePreview"">
    <attachments-preview au-slot attachments.bind=""imagesForPreview""></attachments-preview>
</modal>Notice how I can project a custom element inside of the modal? Because I defined a slot inside of my modal without a name, it gets a value of default as the default name. When you want to inject content inside of your component if you’re using the default, you just put au-slot on the content.And here is an example where I project HTML into the modal. It could not be any easier.<modal showing.bind=""showLocalityModal"">
  <div au-slot>
    <ul>
      <li repeat.for=""term of localities"" class=""${isTermSelected(term.term_id) ? 'locality-selected' : ''}""><a href=""javascript:void(0);"" click.trigger=""selectTerm(term, $event)"">${term.name}</a></li>
    </ul>
  </div>
</modal><modal showing.bind=""showLocalityModal"">
  <div au-slot>
    <ul>
      <li repeat.for=""term of localities"" class=""${isTermSelected(term.term_id) ? 'locality-selected' : ''}""><a href=""javascript:void(0);"" click.trigger=""selectTerm(term, $event)"">${term.name}</a></li>
    </ul>
  </div>
</modal>When to use au-slot and slotThe reason for there being two elements and subsequent attributes is because Aurelia keeping inline with its “follow the spec” philosophy dicerns the difference between proper native specification behaviour and non-standard behaviour.The slot element is native to the Web Components spec, as such, you cannot use the slot element if you have Shadow DOM disabled in your application. You can only use the slot element if you have Shadow DOM enabled, keeping Aurelia instep with how the spec sees slot elements.However, there are instances where you want slot behaviour (components that support replacing the inner contents) which is why au-slot was created. It gives you the same functionality as the native slot element, except it doesn’t rely on Shadow DOM being enabled.In-fact, if you try using slot with Shadow DOM disabled, Aurelia 2 will throw an error in the console telling you that you can’t use slot with Shadow DOM turned off.However, going further, there are instances where even when Shadow DOM is turned on, you don’t want to use the native slot. One of the limitations (by design) is styles won’t cross the component boundary in Shadow DOM, using slot means you lose the ability to style the contents with styles defined outside of your component (unless you have shared styles using Adaptive Stylesheets).If you are creating components that you want to support styling from outside of the component itself, you would opt for the au-slot element and subsequent attribute instead of the native slot.In most uses, you will most likely opt for au-slot over slot, but it is good to have a spec-abiding option and then a more flexible Aurelia-esque option which isn’t too different from the native version.",1074
HP Reverb G2 Virtual Reality Headset Review,"The HP Reverb G2 is easily the most exciting virtual reality headset that you can buy in 2021. Furthermore, it is also one of the more affordable options.When evaluating virtual reality headset options, I considered the more established VR names first: HTC, Valve and Oculus. HP was admittedly low on the ladder of options until I did some digging into the HP Reverb G2.You will probably hear about this in every review, but the most impressive thing about the Reverb G2 is the screen resolution and audio quality. While it only has a 114-degree field of view, don’t let that trick you into thinking more expensive options are necessarily better because they offer larger FOV.Screen-wise you get MURA free 2160 x 2160 LCD panels per eye and full RGB stripe and mechanical IPD adjustment, which results in a high clarity display.Most noteworthy is the screen door effect that has plagued VR headsets since Oculus CV1, where you can see between the pixels like you’re staring through a screen door is no more. This ultimately makes the Reverb G2 the highest quality virtual reality headset on the market right now.Installation & SetupThe setup process is simple and effortless. Really, there is not much to the headset. You get a little dongle box which has three cables running off of it. Two of those are a DisplayPort and USB-C cable which run into your computer.Fortunately, my motherboard has dedicated USB-C ports; however, I have read you will run into issues if you attempt to use the front USB-C ports on your case (most likely underpowered). If you don’t have any USB-C ports, you will get a USB-C to USB-A adapter in the box.During setup (because I am stupid) I admittedly took longer than expected to find where on the headset you plug the long 6m cable into. I then realised the face mask needs to be removed and you’ll find a deep hole and connector at the top left of the headset.The cable size, while great for moving in a room, is MASSIVE for seated use. I purchased this headset primarily to experience VR seated, not standing in a room (I would have chosen something with proper room tracking if I wanted that).Neatly running the cable through the headset also proved to be difficult for me (once again, I’m not very bright). Even once I got the cable neatly in, one thing that irked me was I could feel the weight of the cable against me (the cable is quite chunky). It’s a minor thing, but still annoying.Once you get everything connected, provided you’re on Windows 10, and you’re up-to-date, the Windows Mixed Reality setup process should automatically start with some calm music. This step took about 20 minutes or so and will get you to pair your controllers and then it’ll spend an inordinate amount of time downloading something.From pairing the controllers to getting things set up, it’s an easy process mostly anyone could follow. Getting the cables to run to your headset and computer neatly seems to be the tricky part of the setup, the 6m cable will always get in the way.SteamVr is an extra stepPresumably, you got this headset to play the trove of VR games on Steam. To play Steam VR games, you will need to setup SteamVR which is an additional step (you don’t just open Steam and play).Microsoft has provided some documentation on getting SteamVR setup here. Once you have installed, make sure you restart your PC. I didn’t restart and had problems getting SteamVR working.hereHow comfortable is it?I have owned Google Cardboard VR headsets, I have owned a couple of the Samsung VR headsets and I even owned a PSVR headset before I sold my PS4. The one thing these all have in common is they are not very comfortable.Wear a VR headset on your face for a while and many are prone to getting a little sweaty, your face hurts from improper padding or the weight imbalance gives you a neck cramp. I was surprised how comfortable the Reverb G2 was to wear, not only in weight distribution, but also padding.Further compounding this is the fact that I wear glasses. Look, I am not going to lie, I tried wearing my glasses and it was a tight fit. Fortunately, my glasses are mostly for reading/small screens, so I just didn’t wear them in the headset, but I can foresee glasses being a problem.I try and limit my VR experiences to maximum one hour. Because I am not used to VR, it has a tendency to make me a little sick and I think for many, this is a normal reaction.Also, don’t be discouraged if you are new to VR. There will be a period of time when you will see the frame (plastic shrouding) of the lenses. I liken this to getting glasses for the first time. For a couple of weeks when I first got glasses, I could see through them, but my eyes would also see the frames themselves, and it was a little jarring at first.Eventually, you will reach this moment where you don’t see the outer part of the headset and just through the lenses, completing the immersion.TrackingIn terms of tracking, the Reverb G2 is a case of you get what you pay for. While it has 4 cameras for tracking on the headset, don’t expect laser level tracking you will find on other more expensive VR headsets like the HTC Vive or Valve Index.Still, I didn’t really experience any issues with tracking. I have primarily used the Reverb G2 for seated VR experiences and had no issues. I haven’t really done any room level movement VR, and I don’t feel the need.At the end of the day, the Reverb G2 is going for the consumer market and not the hardcore gamer or VR developer who will undoubtedly want something more fully-featured. For the price you pay, you get one of the best screens, the audio is fantastic, and field of view at 114 degrees is respectable.The ControllersOnce again, you get what you pay for. The Reverb G2 controllers are the standard Microsoft Windows Mixed Reality controllers. And for the most part, they are great.The layout of the buttons and way they sit in your hands is intuitive, and you do get used to them quite fast. Tracking, the headset seems to work really well with them and provided you’re keeping them in the bounds of your headset, they will never drop out.The only bad thing I have to say about these controllers is that battery life doesn’t seem impressive. I mean, they are not going flat after a few hours, but after light-ish use, I found the batteries lasted just a little over a week. So, I recommend getting rechargeables to ensure you always have batteries (two AAs per controller).AudioOne thing you will be most impressed with (besides the screen resolution) is the audio. Two little pull down speakers that sit above your ears have fantastic response and immersion. The audio is the same tech used in Valves more expensive Index headset (Valve and HP collaborated on the Reverb G2).With other headsets I have tried, using headphones/earphones just breaks the immersion. There is something about speakers sitting above your ears (as opposed to being on them) that makes the sound feel so much more real, spatial and immersive.In terms of distortion or anything else that might dull the audio experience, I didn’t encounter any issues. I wasn’t exactly listening to heavy bass music through this headset though, Half-Life: Alyx (recommended below) sounds amazing through these speakers.Half-Life: AlyxWhen it comes to flagship VR titles, the one game that you show people when they come over, undoubtedly Half-Life: Alyx is the title you should show them. As far as VR titles go, Half-Life: Alyx hits the mark in every single aspect.From visuals to audio, to immersion, Half-Life: Alyx is the title you want to load first when you get your Reverb G2. And despite the fact that Valve has a more expensive headset of its own called the Valve Index, the G2 works well with SteamVR and this game.If you buy any VR experiences, make sure this game is the first one that you buy and you won’t regret it. This will validate your VR purchase, instead of other VR experiences which feel like demos more than full games.Microsoft Flight Simulator 2020: VRWhile I would consider Half-Life: Alyx to be the showcase game for VR, in terms of polish and immersion, Microsoft Flight Simulator 2020 is a close second. Despite the fact MSFS only added VR support in December 2020, if you have the PC specs, it’s an incredible game for VR.I did have to play around with the settings a bit to get my RTX 3070 working with the Reverb G2. MSFS 2020 is a taxing game spec-wise, even those with 3080’s struggle to get good performance at times. If you want to try it out, this Reddit thread was immensely helpful.this Reddit thread was immensely helpfulI wouldn’t put MSFS 2020 first on the list even though VR in this game is unlike anything else, and not everyone has the specs to run this game in VR at a resolution decent enough to make using a VR headset worth using.NoLimits 2: Rollercoaster SimulatorNow, here is a title that I never thought I would enjoy. NoLimits 2 is a well-known rollercoaster simulator with virtual reality support. The graphics are surprisingly well done, and while NoLimits is more of a sandbox simulator than a game, it’s quite fun.NoLimits 2You can download a heap of rollercoaster designs, and if you’re adventurous, you can even make your own roller coasters well. The only downside is you have to use your mouse to interact with this game, the Reverb G2 controllers won’t work, and apparently, it’s an issue with other headsets.Word of warning, you don’t want to play this game for extended periods of time, it will wear you out.ConclusionFor the price, the HP Reverb G2 is one of the best headsets around. If you’re only going to be casually experiencing VR and you can live without features such as room-scale tracking, the Reverb G2 is the best headsets you can buy right now.",2436
"Parler comes back online, but should you trust it?","Fool me once, shame on you, fool me twice, shame on me. It looks like after being dropped by Amazon Web Services and other providers, Parler is back online (sort of) with a message.I guess step one of the revival process for Parler was finding a web host who would take them and handle the scale Parler was running at. I have no idea who is hosting Parler (maybe they are hosting themselves), but the site does not appear to be fully operational just yet.If this post is anything to go by, the Parler team have a lengthy battle on their hands to actually migrate from the AWS infrastructure they most likely depended on heavily. this post is anything to go byGiven Parler was hacked and all of its data scraped, I don’t have high hopes the migration will be in the ballpark of experience for whoever has the task of setting up the site on a new host.was hacked and all of its data scrapedOver on Reddit, users are having a field day in the comments. Many are speculating that Parler is an FBI honeypot, an attempt for the government to monitor and gather far-right extremists’ information. And honestly, it’s not that far fetched.Despite the fact, there is a real possibility Parler is a honeytrap, the fact that Parler is a social media platform built on WordPress (which you shouldn’t use WordPress for) is a warning sign in itself the owners of this site have no idea what they are doing (and I say that as someone who loves and uses WordPress).Many of the people who were Parler members and participated in the Capitol riots and made threats online who have not been caught might be stupid enough to start using Parler again when it comes back online and fully operational. Like flies to honey.",425
Roland TD-17KVX Drums Review,"When I’m not writing JavaScript or brewing beer, I’m an avid guitarist and drummer. I’ve been playing guitar for a while, but drums are somewhat new to me.My first electronic kit was a Roland TD-1KV, an excellent beginner kit to see if I actually stuck to drumming before pursuing something more expensive. It’s a compact kit I found myself growing out of quickly.When assessing my next kit, I knew I would stick to playing electric, so I could play quietly any time of the day and record without needing to mic anything up.Roland was at the top of the list in terms of kit options, but I also considered Alesis. I played many kits, watched a lot of YouTube videos, and read blog posts and Reddit threads. I didn’t rush into choosing a new kit, especially considering I would be spending a couple of thousand Australian Dollars minimum.First and foremost, Alesis was quickly scrubbed off the list. Although Alesis sounds good and is great value for money in what they give you, Alesis kits’ durability is still a let-down. When I was playing Alesis kits, they didn’t feel as nice as the Roland kits do. Roland kits’ response seems to be a lot better (but this is quite possibly subjective).I am by no means a professional drum reviewer. This is my experience both researching and choosing the Roland TD-17KVX and setting it up and playing it. If you don’t care about technical specifics and want to know whether you should buy this kit or not, keep reading.What’s not included There seems to be some confusion about what you get and what you don’t get in the box. You pretty much get everything you need in the package except the following you need to purchase separately:A hihat standA single or double kick pedal A hihat standA single or double kick pedal The Roland TD-17KVX doesn’t require anything special, any hi-hat or kick pedal will do the trick. In the spirit of keeping everything one brand, I did try and order a Roland branded hi-hat stand and double kick pedal. Sadly, they didn’t have the Roland double kick in stock, so I just got the Roland hi-hat stand.If you are fortunate enough to have an old acoustic kit around, you can use your hi-hat stand and pedals from that. Just remember, you will need these to play the kit.Setup and UnboxingThe TD-17KVX comes in two boxes, one smaller rectangular box with a stand and a larger box containing many smaller sizes boxes.As you begin to unpack the TD-17KVX, the first thing you will notice is a lot of boxes. Each cymbal, each pad, the stand, the module and everything in between is wrapped in plastic and inside of a box.Seriously, look at this mess. I actually found that half the time spent setting up the drum kit is just opening boxes and removing taped-on plastic and Styrofoam wrapped around the pipes. Be prepared to have a huge mess to deal with after you’re done unpacking and setting up.Setting upThe setup process is quite straightforward, but do yourself a favour and ignore the paper instructions that come in the box, they make no sense and will have you pulling your hair out. The only assembly instructions you need are the following video:

Thanks to the instruction video above, I had my kit assembled in about 30 minutes.The TD-17 moduleThe main component of the TD-17KVX is the module which controls everything, the brain so-to-speak. Sadly, my first experience with the TD-17KVX wasn’t good, and the module lasted a couple of hours as the display started to slowly fade away until the LCD was backlit, but had no text on it whatsoever.I ended up having to send the module back to the music store I bought the kit from and they were just as shocked as I was this happened and said they’ve never had a defective Roland kit before.Even the national sales representative for Roland seemed to be shocked to hear that my module was defective. So, this was a case of bad luck, and perhaps, the pandemic might have played a part in the failure as well (who knows).The TD-17 module is quite fully featured. There are many buttons on the surface, a dial you use to navigate through kits and other menu items, a few knobs for ambience, bass, treble, and volume. And then you have a plethora of other labelled buttons you won’t interact with most of the time such as buttons for tuning and muffling.The reality is unless you’re a power user, many of the features of the TD-17 while nice to have, you will end up ignoring.How it soundsProbably one of the biggest and most burning questions you have: does it sound any good? Here is where subjectivity factors in. Some people love how Roland kits sound and others hate the sounds entirely.I am pro-Roland, so I think the sounds on the TD-17 are great and I believe it’s one of the best sounding modules around. The TD-17 module shares the same prismatic sound engine of the TD-50 (the flagship second mortgage model). Are the sounds of studio-quality drum samples? No. You very much get that Roland drum sound, it is an e-kit after all, For studio real-like drums, you will need a VST like Superior Drummer 3 or Addictive Drums 2.The Roland sounds are not going to sound like a real acoustic drum kit. Even the Roland TD-50, which is almost 10,000 Australian dollars, sounds very much like an electric kit. I think many confuse electric drum kits with acoustic kits and for the foreseeable future, they are never going to be comparable.Buy some custom kitsBuy some custom kitsI think the stock sounds depending on what you are going for are more than fine. However, you do need to do a bit of work tuning them to get them sounding how you like. This is where custom kit sounds come into play.I highly recommend the drum-tec or The Drum Workshop sounds if you want to make your TD-17 sound a little less robotic and nicer. I ended up buying all three Drum Workshop packs as well as a couple of the drum-tec kits. Unfortunately, the TD-17 doesn’t have enough space for all sounds, so you have to pick and mix.drum-tecThe Drum WorkshopThe biggest issue I have with the stock sounds is the snare on the default TD-17 kits, and it lacks any kind of punch or personality on any of the kits you will try. The above packs (both come highly recommended) if anything is great for getting a better snare sound.For some perspective, I upgraded from a Roland TD-1KV, so the TD-17 is a MASSIVE upgrade for me, the sounds are so much better; the response and feel are astronomically different. It’s like upgrading from a Ford to a Tesla.Here are some uploads showcasing sounds directly from the kit. These were recorded using the audio outputs on the back into my Presonus Audiobox. No plugins were used, only editing of the kits on the module itself.Cymbal, Pad & Kick QualityThe cymbals are the Roland rubber that adorns all of their e-drums. You get a CY-13R ride (which is three zones). There have been some complaints online about needing to hit this quite hard to trigger the bell, and I recommend you mess with the pad sensitivity settings to address this.You get three PDX-8 toms which are eight inches and have mesh heads. They’re not the biggest, but much bigger than they sound and easy to play. You get three two-zone CY-12C crash cymbals as well, and they have a nice swing to them when you hit them.One of my favourite things about the TD-17KVX is the PDX-12 snare pad. It’s twelve inches and while other kits do have a 14″ snare, this snare is technically 14″ if you factor in the plastic shell around it. The snare is plenty big for an e-kit and can be tuned for feel.For the hi-hat, you get the VH-10 hi-hat which mounts onto a normal hi-hat stand. You get the pad itself and a trigger which works using some spring mechanism. You will need to spend some time calibrating and adjusting the sensitivity to get this working to your liking, but it allows you to work on your hi-hat technique.You get the KD-10 kick pad for the kick drum, which works with both single and double kick pedals. I am using it with a Mapex double kick pedal, and it works quite well. I do implore you to consider third-party Roland kit sounds (recommended above), giving you tuned kicks that sound much nicer than the default ones.Q&AThere are some known knowns, and then there are some unknowns with electric kits. While the Roland TD-17KVX is not top of the range like the TD-50, it’s an intermediate kit that is more than capable of meeting most people’s needs.Can you play ghost notes and dynamics on the snare?Can you play ghost notes and dynamics on the snare?While the snare on the TD-17KVX is anything but a real snare, it is quite responsive and sensitive (it can also be adjusted). You can play ghost notes and practice your dynamics no problem on this snare. Hit it softly, it plays a quiet note, hit it hard and it’s loud.Can you blast beat on this drum kit?Can you blast beat on this drum kit?Some cheaper kits are notorious for latency in speedy playing, particularly metal genres that command a faster bpm. The TD-17KVX allows you to blast beat until your hearts content no problem.Does the TD-17KVX suffer from machine gunning?Does the TD-17KVX suffer from machine gunning?Another common complaint with e-drums as a whole is the concept of machine-gunning, in which the snare sounds like a machine gun when you do rolls on it (no dynamics or variety to the notes). You will be happy to know that the snare samples are stereo (left and right are different) so rolls on it won’t sound like someone firing a machine gun.Can you play along with music on Spotify and other sources?Can you play along with music on Spotify and other sources?Yes. The TD-17 not only features a 3.5mm stereo input so you can connect an audio source directly to the TD-17 module, but it also has Bluetooth support, which means you can play music through Spotify, YouTube and other audio sources and it will mix with your kit sounds.Can I record my playing?Can I record my playing?Yes. You are spoiled for choice. First and foremost, you can connect using the two 1/4″ outputs (Left and Right) and run them into a board or audio interface. The TD-17 supports MIDI over Bluetooth, you can connect the module to a PC using the USB cable and record the audio, or you can record directly to an SD card.Can you use a double kick pedal?Can you use a double kick pedal?Yes. While the KD-10 kick pad might look tiny, you can easily use a double kick pedal on it and not just a Roland one, you can use any double kick pedal on this kit and the beaters will align with the pad just fine.How does the TD-17 compare to the TD-27?How does the TD-17 compare to the TD-27?I played the TD-27, and I thought it was a fantastic kit, but you’re looking at almost double the price in Australia to get it compared to the TD-17KVX. If your budget extends to the TD-27, it does sound better, the snare is bigger, and it’s more akin to a TD-50. The TD-17 is a good buy for the price.ConclusionI don’t regret buying my TD-17KVX. For the price I paid for it, I am pleased with what I got. I love the large snare, the all mesh toms, the ability to practice hi-hat techniques with a real stand. This kit is great for a beginner, intermediate or even an experienced drummer looking for a practice kit or quiet studio kit.",2795
"Parler is gone, will it ever return?","The free-speech alternative to Twitter called Parler has finally gone down after Amazon said it would be pulling the plug on hosting the controversial platform that houses a wide variety of people from self-taught experts to self-described patriots.If you are not sure why Parler went down, it comes down to the platform being used to organise the Capitol Hill attack and forcing the hand of tech companies to reconcile with the part they played in all of it. The platform was being used to organise all kinds of terrible things, and the community as a whole leant quite heavily into toxic territory.Before Amazon pulled the plug on AWS hosting, Apple and Google pulled the Parler app from their respective app stores. Although, Apple and Google did say they would restore the app when Parler moderates its content better (which is at odds with the site’s ethos).While many argue this is an attack on free speech, people need to be reminded that free speech does not extend to private companies and platforms. And hate speech is not free speech. It’s a two-way street, consumers have the freedom of choice when it comes to the platforms they signup for, and the platforms have the freedom of choice who is allowed to access these services. You can’t argue for free speech and then argue that companies shouldn’t be afforded the same right.That doesn’t seem to have deterred Parler from suing Amazon, alleging that they removed the site from AWS for political reasons. The lawsuit is clearly not going to succeed, and it’s just the last dying breath of a platform that has finally met its match.Parler from suing AmazonWe can only hope that if Parler returns, it’s not the same amateur slapped together service it was previously. I was shocked to learn Parler was built on top of WordPress. While I love and use WordPress myself, it’s clear that it wasn’t built with security in mind and an easy target for hackers. Somehow, I can’t see Parler making a return any time soon.",493
It’s barely been a month and I am already bored with Cyberpunk 2077,"When Cyberpunk 2077 was released, I, like many others, was excited to finally get this game we first saw seven years ago. The game was eventually released, and to say that it was a disaster would be putting it lightly.While non-next-gen console gamers got the short end of the stick, those of us on high-end consoles and beefy gaming PCs could play the game, but it was littered with more bugs than a cheap motel.While there were bugs and there continue to be, they never stopped me from playing the game. If I encountered a serious bug, I would close the game and reopen it, with the bug disappearing. To me, bugs were never a problem, and I didn’t really complain about them either.The real issue I have with Cyberpunk 2077 is that it is boring. The constant interrupting phone calls that just get answered anyway, the cut-scene-esque scenes where you shoot at some bad guys, and no matter what you do, the result is the same scripted ending.A world filled with passive-aggressive NPCs that tell you to fuck off when you try speaking to them (everyone in this game is angry about something), buildings you cannot enter, and driving and shooting mechanics that somehow feel worse than games such as Grand Theft Auto 3 which had this stuff worked out 20 years ago.I constantly feel like I am just following people around in this game, like I am not even the main character, just watching everyone else do their thing. There feels like a legitimate lack of choice in that no matter what I do, the game will decide for me anyway.But, yes, the game is beautiful. Turn on ray tracing and bump up the settings, Cyberpunk 2077 is one of the best-looking games out right now, and it oozes next-gen. But, it is evident they spent more time on the game’s appearance than the immersion and content.And the soundtrack, the soundtrack is the boss in this game. There isn’t a bad song in it (whoever picked the music deserves a raise). But, playing Cyberpunk 2077, regardless of the nice graphics and soundtracks, still inevitably ends up feeling like a chore.And let’s talk about V. What is going on there? Regardless of whether or not you pick male or female V, he/she ends up being a complete jerk to everyone they encounter. Like everyone else in this universe, it seems V is angry about something. It’s a shame because it looks like a great city.I managed to put in 28 hours into this game before I ultimately just gave up. I might end up revisiting the game in a year or two when they hopefully get this mess cleaned up, and when Cyberpunk: Online is released, I will try that out too. Although, who knows if Cyberpunk: Online will even be a thing. They’ll be fixing issues with this game for years to come.Cyberpunk 2077 is the biggest missed opportunity in gaming history. Maybe my expectations were just unfairly high, but expectations or not, it doesn’t make the game any more fun.",720
Inferred bindable properties in Aurelia 2,"About a year or so ago in the internal Aurelia core team chat, I raised an idea for inferred bindings. A lot of the time I find myself binding to values that have the same name as the bindable property.The idea is simple, say I have a custom element that has a bindable property called products, I would then most likely do something like the following:<product-list products.bind=""products""></product-list><product-list products.bind=""products""></product-list>It has always felt a bit redundant to me that I am specifying a property and then more often than not passing in a value of the same name. Well, after raising this again in the Aurelia core team chat, I was shocked to discover that this has been implemented.You can now write the following:You can now write the following:<product-list products.bind></product-list><product-list products.bind></product-list>The ="""" part gets removed and Aurelia will look to your view-model for a property of the same name. While this might seem small, it makes the world of difference when you have custom elements with 4+ bindable properties, you save a lot of characters here.=""""",282
"Social media platforms finally deplatform Donald Trump, but is it too little, too late?","Donald Trump has been notorious for spewing hate from his Twitter account long before he became the president. A history of vile attacks including racism and fat shaming.After the violent attacks on the U.S. Capitol building in Washington D.C and some Tweets, Twitter first issued Trump with a 12-hour temporary ban and then subsequently a permanent ban, also limiting access to other accounts including the official @POTUS account.Joining in, Facebook has also suspended Trump indefinitely from Facebook and Instagram (not outright banned like Twitter). For a bigger lol, even Shapchat has banned Donald Trump amongst others who have temporarily suspended the sale of Trump merchandise including PayPal and Shopify. The “DonaldTrump” subreddit was also removed over on Reddit.Where was all of this action years ago when Trump was using his platform to attack people and not condemn dangerous actions such as shootings and attacks?The sad reality of this ban is that we shouldn’t be congratulating Twitter or patting Facebook on the back, these companies have only acted because Trump as a threat has all been neutralised. Twitter has clearly been too afraid to ban Trump out of fear of the regulatory hell he could put them in, now Biden is confirmed, Twitter has nothing left to lose. The same can be said for other platforms.The damage has already been done. For years, Trump has built up his MAGA movement, empowering far-right self-described patriots some of which have gone on to commit some heinous acts.If Twitter truly cared about limiting access to its platform from those spreading hate and inciting violence, maybe they should have banned Trump four years ago.",418
2021 is saved: It looks like Thrice have begun work on a new album (or concept albums),"Palms was released in 2018 and Thrice fans have been eagerly awaiting news of a follow-up album. Obviously, in between the release the world has completely gone down the drain with the pandemic and the political situation in the US being so volatile.In a recent Tweet by frontman Dustin Kensrue, he showed work being done on some Thrice demos
The work continueth… pic.twitter.com/Oq3EWLmJGl— Dustin Kensrue (@dustinkensrue) January 5, 2021
The work continueth… pic.twitter.com/Oq3EWLmJGl— Dustin Kensrue (@dustinkensrue) January 5, 2021The work continueth… pic.twitter.com/Oq3EWLmJGlpic.twitter.com/Oq3EWLmJGlJanuary 5, 2021We know Dustin is working on new Thrice material because someone asked in a reply and he responded in the most Dustin way possible with a Schitt’s Creek gif referencing the word Thrice.Continuing with the approach Dustin took to writing guitars on Palms, he is using his Line 6 Helix for the guitar tones (which he also uses live) and based on the sounds he got out of this thing on Palms; it’ll be interesting to see how he has undoubtedly mastered it, what he does.But, this is not where the excitement ends. In a recent podcast episode of Crash Bang Boom, Riley Breckenridge appeared in an episode, and he mentioned “records” when talking about working on new Thrice music (at 52:50). Riley Breckenridge appeared in an episodeRiley says, “We’re writing a record or records or, I don’t even know. We’re just writing a lot of stuff very slowly” and that the intent was to put out an album quite soon, but with the pandemic throwing touring plans into disarray, they are taking their time.We’re writing a record or records or, I don’t even know. We’re just writing a lot of stuff very slowlyThe impression I got from the interview Thrice realised things will not be going back to normal any time soon, so they are using the extra time they have been afforded to write many new albums. Although it does sound like a lot of experimentation and uncertainty, it might not materialise as a concept record.Thrice fans might recall that in 2019 Dustin was asking for concept record ideas for a potential Thrice album.
Hey everyone. We in @Thrice have always been looking for another concept like The Alchemy Index to write a record around, but we’ve never found something that we liked as much. Please share any ideas you  have of what could be cool. 🙏🏼 You shall be credited should we use it.— Dustin Kensrue (@dustinkensrue) December 1, 2019
Hey everyone. We in @Thrice have always been looking for another concept like The Alchemy Index to write a record around, but we’ve never found something that we liked as much. Please share any ideas you  have of what could be cool. 🙏🏼 You shall be credited should we use it.— Dustin Kensrue (@dustinkensrue) December 1, 2019Hey everyone. We in @Thrice have always been looking for another concept like The Alchemy Index to write a record around, but we’ve never found something that we liked as much. Please share any ideas you  have of what could be cool. 🙏🏼 You shall be credited should we use it.@ThriceDecember 1, 2019While we don’t know the exact concept, Dustin did reply enthusiastically around the idea of a Nordic mythology concept. We do not actually know what the concept might be (if there is going to be a concept).
https://twitter.com/dustinkensrue/status/1201257404790263808
Something tells me that if the demo process works out and the guys feel it, we might be getting concept albums from Thrice in 2021, in a similar vein to The Alchemy Index.",882
How To Setup Roland Drums For Streaming,"I have gone down the path of streaming my drum playing as I learn to become a better drummer. There are a few ways you can stream with audio, but here is how I do it.streaming my drum playing as I learn to become a better drummerWhile the title specifically references Roland, this guide will work for any electronic drum kit, whether it be Alesis or some other brand. You just need the ability to pass the sound out of your kit. You either use dedicated outputs or a nifty trick using the headphone output.What I wanted to achieve was the following:– To play music on Spotify, YouTube and other sources– To capture both the music playing as well as the drum kit itself– To have zero latency between the drums and audio (playing and audio should be in sync)– To have the ability to adjust the volume of both the drums and music being played (so the drums and music are not drowning each other out)– To capture video as I play– To also be able to capture audio from a separate microphone (room noise/talking)What you will needTo set this all up properly, you will need a couple of things: a computer and an audio interface. You will also need some cables (keep reading) depending on your setup.I use the Presonus Audiobox 96 USB audio interface. Not only is this audio interface cheap, it’s bulletproof. I have had mine for a couple of years now and it is on 24/7, no problems with it whatsoever. It has two microphone/instrument ports on the front, independent volume knobs and allows you to adjust the balance of the input and mix.Presonus Audiobox 96 USB audio interfaceSoftware wise, I highly recommend Streamlabs OBS. You can add in individual sources for both audio and video, allowing you to control the volume of each one. You can also turn on the ability to monitor your input (handy if you’re playing music through your computer).If you’re streaming, you are going to need a camera. Right now, I am just using my Logitech C920 HD Pro webcam. Some streamers use cameras such as the Sony a6300 and then use Elgato camera links to connect them, but if you’re just starting out, don’t go spending a heap of money on something you might give up on.Logitech C920 HD Pro webcamA good quality webcamIf you prefer not to spend much money, you can also use your phone. If you’re an Android user, you can install an app called Droidcam which will allow you to use your camera on your phone and wirelessly broadcast to your computer, which you can then use to stream in Streamlabs OBS. The webcam route is the easiest and cheapest way to go if you want something easy.DroidcamConnecting everythingIf you’re using a Roland kit, you most likely have two 1/4″ audio outputs on the back of your module. I have the Roland TD-17 module (Roland TD-17KVX kit), which has two outputs on the back. Using standard 1/4″ instrument cables (the same you use for a guitar), run those into the audio box.You can get away with running the Roland TD-17KVX as mono using just one cable, but I recommend using two cables as you’ll be able to control the sound a little more and mix it better than a mono stream.If you’re using a cheaper Roland kit (or something else entirely) and you don’t have dedicated outputs, hope is not lost. You can use the headphone output. Get yourself a 1/4″ to 3.5mm (1/8″) TRS cable like this cheap one on Amazon here. This does mean you can no longer listen to your kit through the module itself and now have to do it on your computer.hereThis nifty little “hack” allows you to run your headphone output straight into your audio interface. It’s crude, but it works. Best of all, you can adjust the output/input volume either using the volume control or the input control on your interface (I did this for my Roland TD-1KV).Now, we need the ability to feed in some music. The easiest method is to connect your phone, a tablet, laptop or computer directly to your module. The TD-17 module, for example, has both Bluetooth and auxiliary 3.5mm input.Once again, my module allows you to control the volume of the audio in the mix, so I can adjust the volume myself. If you’re not using a Roland module, you might not have this ability, so you will have to adjust the volume on whatever device you’re feeding into the module.Configuring Streamlabs OBSI use Streamlabs OBS, so your software might differ here settings wise. First, you want to make sure that any dedicated audio devices you have are configured for Streamlabs, to do that, you need to create a new source. In Streamlabs OBS, you want a source type of “Audio Input Capture”.Pay attention to my audio sources, yours will obviously differ. Because my module is handling both drums and music, it’s a singular source from the audio interface. I have a room microphone (labelled accordingly) which is set to my USB condenser microphone.Also, if you’re using a webcam like I am, don’t forget to mute it or your stream will be polluted with room noise as webcam microphones are quite sensitive and you’ll hear typing, clicking, fans and possibly your computer if it’s a little loud.That’s all there is to it, it’s not rocket science. The audio interface will make your life easier and I of no other way, so it’s a requirement to stream. From there, you can add in a Stream Deck, lighting, additional audio sources and even cameras.Follow me on TwitchIf this post helped you out, consider returning the favour by following or subscribing to me on Twitch. It would mean the world to me.following or subscribing to me on Twitch",1371
The best way to develop with WordPress locally in 2021 (and beyond),"When it comes to developing on WordPress locally, you have a few options. You have the tried and tested XAMPP, WAMP, MAMP and if you’re feeling adventurous: Docker.I will recommend an option that isn’t any of the above, it’s one you might not even be familiar with, but spoiler alert: it’s the best. It’s called Local, and it’s the best way to develop WordPress sites locally.LocalWhat makes Local one of the best options around for WordPress development is:It’s 100% free (they have a pro plan, but it doesn’t offer anything that you need to pay for)One-click WordPress installationsSSL supportSSH root accessWP-CLIHot-swap PHP versions, allowing you to test your site on different versions of PHPDeploy your sites to Flywheel or WP EngineSupport for addons and allowing developers to write their own addons for LocalIt’s 100% free (they have a pro plan, but it doesn’t offer anything that you need to pay for)One-click WordPress installationsSSL supportSSH root accessWP-CLIHot-swap PHP versions, allowing you to test your site on different versions of PHPDeploy your sites to Flywheel or WP EngineSupport for addons and allowing developers to write their own addons for LocalOne of the most interesting things about Local that makes it stand out from the pack is that unlike XAMPP and other options, there is no installer process. Instead, you open up the app, create a new site, choose your PHP version, the server you want (Nginx or Apache), and the database. This allows you to choose what sites have what environments.Furthermore, the site creation process even supports WordPress Multisite, allowing you to deploy a new fresh installation of WordPress without the headaches of creating databases, configuring your site and editing any files.What blows my mind is Local is completely free and their paid features are not crucial things that most developers need when running WordPress locally. I would pay money for this kind of app, but I am grateful that it is free.Uninstall XAMPP, throw out MAMP and toss WAMP into the shredder, Local is the only option you should be considering for local WordPress development in 2021 (and beyond).",536
"Twelve Foot Ninja have been busy, a new album finally announced for May 2021","We can all agree 2020 was a write off, as far as many are concerned, 2020 doesn’t exist, it didn’t have. Everyone’s favourite musical Australian hybrids Twelve Foot Ninja despite not releasing a new album and saving 2020, have announced a new album is coming 2021 (better late than never).

It appears the boys have been busier than a cat burying catshit in concrete. 2020 has been saved, it’s just being saved in 2021 instead.",107
What a time to be alive: the KFConsole,"At first I thought this was a joke. But the dirty bird (KFC) has teamed up with CoolerMaster to create the aptly titled KFConsole. It’s a high end PC that can keep your chicken warm with an inbuilt chicken warmer that uses the system heat, but also has some beefy specs for what would seem to many as a gimmick and thought was never real after KFC teased it in early 2020 after the ps5 reveal.KFConsole
The console wars are 𝒐𝒗𝒆𝒓.Introducing the KFConsole. #PowerYourHunger pic.twitter.com/k7AM6g61Ip— KFC Gaming (@kfcgaming) December 22, 2020
The console wars are 𝒐𝒗𝒆𝒓.Introducing the KFConsole. #PowerYourHunger pic.twitter.com/k7AM6g61Ip— KFC Gaming (@kfcgaming) December 22, 2020The console wars are 𝒐𝒗𝒆𝒓.Introducing the KFConsole. #PowerYourHunger pic.twitter.com/k7AM6g61Ip#PowerYourHungerpic.twitter.com/k7AM6g61IpDecember 22, 2020Inside it has two Seagate Barracuda 1tb NVMe drives, a hot swappable GPU slot, an Intel Nuc 9 CPU and what kind of GPU it comes with is anyone’s guess but it’s clearly a new 30 series NVIDIA card because the marketing says it supports 4k and raytracing.There is no news on when this thing drops, but if it is priced right, I’m tempted to pick one up just like those nights when you find yourself in a KFC drive thru after you said the previous visit was going to be your last.",331
A workaround for Shadow Dom shared global styles in Aurelia 2,"One of my favourite additions to Aurelia 2 is proper support for Shadow DOM and with it, the ability to encapsulate my styles on a per-component basis, it works well most of the time. However, if you’re using a CSS library such as Bootstrap or legacy CSS, you will need to add in some global CSS styling.Fortunately, Aurelia makes it easy to add in shared CSS styles. In-fact, if you use npx makes aurelia to create a new Aurelia 2 application and choose Shadow DOM, it automatically adds some code into main.ts which allows you to add shared CSS styles.npx makes aureliamain.tsFor those of you using Bootstrap or some generic CSS reset such as Normalize, you’re going to encounter a problem where CSS styles targeting classes will work, but anything that targets generic HTML tags like body will not be “shared” across your app, they will be ignored.I encountered this issue while attempting to use Bootstrap 5 in an Aurelia 2 application. The solution I came up with might not be the best solution, but it’s an acceptable one.Inside of your index.ejs file you want to add the Bootstrap 5 CSS file from the CDN they provide:index.ejs<link href=""https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css"" rel=""stylesheet"" integrity=""sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1"" crossorigin=""anonymous""><link href=""https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css"" rel=""stylesheet"" integrity=""sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1"" crossorigin=""anonymous"">Secondly, you want to make sure that Bootstrap 5 is installed (or relevant CSS library) and import the CSS from within your main.ts file.main.tsimport shared from './shared-styles.css';
import bootstrapStyles from 'bootstrap/dist/css/bootstrap.css';

Aurelia.register(
  StyleConfiguration.shadowDOM({
    sharedStyles: [bootstrapStyles, shared],
  })
)
  .app(MyApp)
  .start();
import shared from './shared-styles.css';
import bootstrapStyles from 'bootstrap/dist/css/bootstrap.css';

Aurelia.register(
  StyleConfiguration.shadowDOM({
    sharedStyles: [bootstrapStyles, shared],
  })
)
  .app(MyApp)
  .start();
Because of the limitations of Shadow Dom and shared styles, I had to include it in my index.ejs file for the styling for HTML elements and then shared for the classes. It might seem a bit redundant, but it worked for my needs. I am sure there must be a way to handle all of this with Webpack, right now, I am just too lazy to find out.index.ejsI wrote this article because I know others are going to encounter the same issues and it just saves time and hassle.",661
Working with configuration based routes in Aurelia 2,"In Aurelia 2, despite the framework itself being completely overhauled and rewritten, many aspects of the framework remain the same familiar Javascript framework many of us know and love. What is new is the router in v2, it works differently to v1 in many aspects.The router can work a few different ways. The first being the new direct-routing functionality which allows you to route to components without having to write any route configuration code. I love the direct router, which you can read more about here in the official Aurelia 2 docs.read more about here in the official Aurelia 2 docsThe second being component configured routing which is a mixture of direct routing, but allows you to override and define some router configuration settings inside of the components you are routing to. It allows you to name parameters and other aspects you might want to change, without really needing to write any configuration code.The third (and focus of this article) is configured routing. If you come from an Aurelia 1 background or have worked with any other framework/library which most likely has a router that works off configuration, this would be the approach you would be most familiar with.While I would recommend trying out the other routing options first, there are valid use-cases for configured routes. One of those being migrating Aurelia 1 applications to Aurelia 2, configured routing makes the most sense here and allows you to gradually port over Aurelia 1 routes to Aurelia 2 component-by-component.Another use-case you might find yourself needing to meet is where it matters what the routes look like. When using the direct routing functionality or even the component configured routing, your URLs might look a little foreign to you like the following. http://localhost:9000/component(somevalue=github)http://localhost:9000/component(somevalue=github)One thing to rememberYou need to ensure all of your components are registered with Dependency Injection. I import my components inside of the view where the au-viewport element is at the top which seems like the easiest place.au-viewport<import from=""./components/profile""></import>
<import from=""./components/profiles""></import><import from=""./components/profile""></import>
<import from=""./components/profiles""></import>You can also opt to specify your dependencies on the customElement definition, but it requires a lot more work to do it this way. Importing inside of the view is safe, fast and the easiest solution right now. There is talk of a @dependencies decorator, but it might not be coming anytime soon (and it’s not guaranteed either).customElement@dependenciesAlso, at the time of writing this, there are still some WIP router changes that have not been merged into the core package just yet. A decorator for defining routes called @routes does exist, but is not merged. So, for the moment, you will need to use static routes to define configured routes (which isn’t a big deal and the decorator will work the same).@routesstatic routesCreating a simple routeTo use configured routes, you simply need to define a static property on the viewport view-model (the view-model for the view that contains an au-viewport element. If you used npx makes aurelia to generate your application, you will have two files (one ending in .ts and another .html) my-app which will contain a &lt;au-viewport> element pair.au-viewportnpx makes aureliamy-app&lt;au-viewport>import { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            component: Profile
        }
    ];
}import { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            component: Profile
        }
    ];
}In this simple example, we create a route using familiar routing syntax from Aurelia 1 and other frameworks to create a URL /profile/:id where id is a dynamic ID value passed into the component (this case, Profile). For this example, we are not doing anything too crazy, just a basic route configuration object./profile/:ididProfileOptional route parametersJust like the v1 router and other routers once again, you can specify optional route parameters which are ignored if they are not supplied in the URL.import { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id/:name?',
            id: 'profile',
            component: Profile
        }
    ];
}import { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id/:name?',
            id: 'profile',
            component: Profile
        }
    ];
}By adding a question mark to the end of the parameter, it will become optional.Adding a title to your routeTo specify a title for the route, we can add in a title property to our route configuration object and it will display in our browser.titleimport { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            component: Profile,
            title: 'My Profile'
        }
    ];
}import { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            component: Profile,
            title: 'My Profile'
        }
    ];
}Passing data through routesWe are still only scratching the surface here. What if you want to pass some data through with your routes? If you worked with Aurelia 1, you would be familiar with settings in Aurelia 2’s router, it’s data. Same concept, a different name that makes way more sense.settingsdataimport { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            component: Profile,
            data: {
                requiredRoles: ['admin', 'author']
            }
        }
    ];
}import { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            component: Profile,
            data: {
                requiredRoles: ['admin', 'author']
            }
        }
    ];
}This theoretical example sees us adding a requiredRoles array to the route, this will allow us to do permissions based checks on a per-route basis which is something I have done numerous times with Aurelia 1.requiredRolesRoute instructions (specify viewports and whatnot)If you are working with multiple viewports in your application, say you might have a viewport for the footer, another for the head and perhaps one for the sidebar and want to load different content into those, the instructions property that takes an array has you covered.instructionsimport { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            instructions: [
              { component: Profile, viewport: 'main' }
            ]
        }
    ];
}import { Profile } from './components/profile';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            instructions: [
              { component: Profile, viewport: 'main' }
            ]
        }
    ];
}This powerful feature allows you to not only specify singular route instructions (which you would never use with this approach), you can have one route theoretically contain multiple instructions, allowing you to render different components into different viewports on a singular route.If you had a profile page, maybe you have a dynamic sidebar which you inject some ads into and other components for widgets specific to a users profile.import { Profile } from './components/profile';
import { ProfileSidebar } from './components/profile-sidebar';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            instructions: [
              { component: Profile, viewport: 'main' },
              { component: ProfileSidebar, viewport: 'profile-sidebar' }
            ]
        }
    ];
}import { Profile } from './components/profile';
import { ProfileSidebar } from './components/profile-sidebar';
import { ICustomElementViewModel } from 'aurelia';

export class MyApp implements ICustomElementViewModel {
    static routes = [
        {
            path: 'profile/:id',
            id: 'profile',
            instructions: [
              { component: Profile, viewport: 'main' },
              { component: ProfileSidebar, viewport: 'profile-sidebar' }
            ]
        }
    ];
}By adding in an additional viewport instruction, we are now composing the element ProfileSidebar into a viewport with the name profile-sidebar. Cool, huh?ProfileSidebarprofile-sidebar",2506
Creating your first Aurelia 2 application,"At the time of writing this, Aurelia 2 is almost about to have its alpha release. Even so, if you’re reading this in the future and I have forgotten to update this post, everything here should still apply (some of the configuration options might change).Fortunately, Aurelia 2 makes creating a new application from scratch easy. You can use Webpack, TypeScript, Babel and choose the way you write CSS and testing strategies as well (Jest, Cypress, etc).Unlike Aurelia 1 and other frameworks/libraries, you don’t need to install anything. You can still bootstrap a new Aurelia 2 application from the command line, the only requirement needed is NodeJS and Npm installed.To get started, open up a command line window and run the following:npx makes aurelianpx makes aureliaNow, what you will see will be a variation of the following depending on your operating system and what command prompt/terminal you are running the npx makes aurelia command in.npx makes aureliaNow, you will be presented with 3 options. If you’re not wanting to configure everything and you just want something fast, the first two options will give you a TypeScript or Javascript based starter, minimal questions asked of you.I always opt for the custom option, I love being able to choose what bundler I use, what language-variant, what testing strategy I want to implement and other choices.If you choose the default option, many of the questions will be somewhat easy to follow. Do you want Webpack, do you want to use TypeScript and so on. The CSS question is where some users might get confused. Do you use Shadow DOM with Aurelia 2 or do you opt for CSS Modules?One of the limitations with Shadow DOM is that third-party libraries which rely on global styles (such as Bootstrap) require a little configuration work to ensure those global styles work. This is because Shadow DOM works on the concept of boundaries, attempting to solve the issues that can arise from CSS’ global approach.If the thought of Shadow DOM and the complexities it can add scares or deters you, CSS Modules is a valid alternative. For CSS Modules, your CSS classes will be rewritten to have additional random letters/numbers in the class names, this makes them all unique.We are towards the end of the CLI now, you will be asked if you want a bare minimum app or one with direct routing. If you know your way around Aurelia 2, choose bare minimum. However, if you’re trying out Aurelia 2 for the first time or not overly familiar with its new concepts, choose direct routing.The direct routing is a new featured added into the router which works on the concept of zero-configuration routes. Routing is traditionally very configuration heavy (even Aurelia 1 was), this makes it so you can just route to components automatically registered in DI. It’s very cool and recommended.",707
CD Projekt Red Glassdoor reviews seemingly confirm poor Cyberpunk 2077 management,"I kind of suspected that the current mess that is Cyberpunk 2077 is a result of poor management, lack of focus and poor communication. Well, it appears the employee reviews are beginning to trickle in on Glassdoor.So far, we only have one review from an employee, but expect that to change over the coming months as people leave CDPR and the focus on the mess this game is shifts away from the bugs to the missing content.It sounds like one of the issues with this game is that previous developers who wrote code did not document it properly and new developers have come in and not understood the code. This has most likely resulted in numerous rewrites, as existing spaghetti code gets thrown out and started again.",179
Cyberpunk 2077 Default Settings on PC Look Terrible (here’s how to fix them),"As flawed as Cyberpunk 2077 is, it still has glimpses of fun here and there. I am not quite ready to give up on the game just yet. In terms of aesthetic appeal, Cyberpunk 2077 is not pretty or pleasing on the eye, some of this is intentional and some of it is a result of the default early 00’s-esque stylstic settings they have turned on.What I found surprising was I had my graphics set to ultra, with ray tracing enabled and DLSS set to balanced (I am using an Nvidia RTX 3070). I expected better graphics with these settings turned on than I got when I first opened up Cyberpunk 2077. Once again, maybe my monitor size, but the game looked anything but next-gen to me.I notice some of these graphical nuisances because I am playing on an ultrawide monitor, for others playing at smaller resolutions, the game might look less jarring with the defaults. For me, the game looked like it was straight out of 2008 with some of these defaults. Although full disclosure: I found even tweaking these aforementioned settings, the game still looks terrible in some parts. The NPC’s still look like poorly designed stock assets from a free Unity assets collection.There are five settings which I highly recommend disabling to make the game a little more pleasing, especially if you’re on an ultrawide monitor like I am.Film grainChromatic AberrationDepth of FieldLens flare (this one especially)Motion blurFilm grainChromatic AberrationDepth of FieldLens flare (this one especially)Motion blurI won’t bore you with the details, but these effects are understandably designed choices with the intent of giving the game that “Cyberpunk feel”, but I found they are all at odds with one another and the neon-lit skyline of Night City, it results in what I consider to be a bit of a mess. The lens flare effect is annoying, it ends up washing out parts of the sky, so you definitely want to turn that shit off.",474
A theory about missing Cyberpunk 2077 features,"First and foremost, a special shout out to everyone who worked on Cyberpunk 2077. Those forced to endure the six-day 100+ hour work weeks for excessive periods of time to try and make it the best they could, I genuinely feel sorry for everyone involved in this game to have it turn out the way that it has.I think Cyberpunk 2077 is beautiful, the design teams really hit it out of the park in the aesthetics department. You really do feel as though you are walking around in a dark gritty Cyberpunk-esque world where corporations and rebels are at odds with one another.From the non-existent subway system to poor combat mechanics, dumb AI enemies, instant spawning police who shoot first and ask questions later, weird key mappings and lack of wanted system, the game definitely feels half-baked and rushed.How can a game that was going into development seven years ago feel rushed? It’s a valid question.By piecing together information revealed over the years in interviews, the game itself and tidbits from CD Projekt Red themselves, a picture of what happened to Cyberpunk 2077 begins to form.The game hasn’t been in continuous developmentThe first trailer was released in 2013. Now, if we assume this trailer was driven by the core game itself, the game would have had to be in development prior to the 2013 trailer. This would bump the development time up by a year or more.In the following video from the CD Projekt Red Group Summer Conference 2012 (around the 11:48 mark), a team was supposedly working on this game in 2012.

Based off of these figures, Cyberpunk 2077 has been in development for 8.5 to 9 years, so almost a decade. The end result is certainly not representative of a game in development for 9 years.It’s not until an interview in 2018 where we get some more details about the inner workings of CDPR and development on Cyberpunk 2077. One of the lines that immediately stands out is the following.an interview in 2018If we don’t like something, we have no problem saying, ‘OK, we have to redo this part.’ It can mean we are throwing away six months of work, and there were bits and pieces happening like that.If we don’t like something, we have no problem saying, ‘OK, we have to redo this part.’ It can mean we are throwing away six months of work, and there were bits and pieces happening like that.It sounds like CDPR had some issues in the early development of this game, not knowing which direction to go and possibly facing pushback from developers internally which caused a reset (quite possibly numerous times).Further down, we get more information. Essentially, CDPR thought they could work on The Witcher 3 and Cyberpunk 2077 concurrently. However, it was all hands on deck to finish The Witcher 3 and its expansions, with a small team working on Cyberpunk 2077. This is all speculation, but looking at the current state of the game, my hunch is the real development on Cyberpunk 2077, never started until around 2016/early 2017. This explains why in 2018 we started hearing about the game a little bit more. At this stage, the game had been in development for a year or two.The theoryWith everything that we know and do not know, it’s clear what happened with Cyberpunk 2077 is they severely underestimated the ambition and scope of the game they were trying to create. Starting an entirely new IP from the ground up, understandably that is a huge undertaking.Coupled with the fact it seems they spent a good 3 to 4 years barely treading water trying to work out what kind of game they wanted to make and went back to the drawing board a few times on some of the aspects of the game.I suspect that many of the missing features (that we know were not scrapped entirely) actually already exist, even if only in some half-baked form. Over at CDPR, I suspect they have a much more sophisticated AI for NPCs, that they have a wanted system and smarter police that don’t just instantly spawn.With pressure from investors, the constant pressure from gamers and media publications asking where the game was and why it was being delayed to the death threats the studio received, it’s a cacophony of things that led to the disaster we are now bearing witness to.As a developer myself, I have been in situations before where you have no choice but to deliver and get to the point where you have to sadly remove features and make last-minute decisions to get something out the door. Many of the behaviours and broken aspects of the game reek of last-ditch effort to release something.Now, of course, this is all speculation. I am just going off feeling more than I am any hard evidence they pulled out incomplete features to launch the game. But, it makes sense given there are things we were being promised in 2018 noticeably absent without an explanation.",1195
Positive Grid Spark 40 Amp Review,"Unbeknownst to some, I am an avid guitar player and I have been playing for around 15 years now. Over the years, I have seen the landscape when it comes to guitars, effects and amplifiers dramatically evolve.These days, digital amps and effects are commonplace. Amp modelling is so good in fact, and you probably don’t realise many of your favourite artists are using guitar modelling software, VST plugins and digital amplifiers instead of traditional pedals and amplifiers in the studio and live performances.Metallica famously made the switch to Axe-FX somewhere around 2013, the savings on not shipping heavy speaker cabinets and amplifier heads would be astronomical for them. The point I am making here is digital modelling has evolved to the point where it is not only good enough for home use but also touring and in the studio. When the Positive Grid Spark Amp was first announced, it piqued my interest for a multitude of reasons. I already have a modeller, a Line 6 Pod HD500x multiprocessor, which I’ve learned to use and really hone my sounds over the past few years I have owned it.The downside with my Pod is that it’s not exactly plug-and-play, it also doesn’t have a power switch, and it’s usually wired up for recording, not for picking up and playing. The Pod also does not have a speaker either, so I need to find my headphones (my kids have usually taken) and fidget about.From an ease of use perspective, flick the switch and plug my guitar in the Spark amp really interested me. A practice amp that sounded good, I could easily travel with and had enough volume for most situations where you would want to use it.First ImpressionsThe box comes with the amp, a power adapter and a USB lead: that’s it. Give it some power and turn it on. The first thing that stood out to me is the Spark looks like a real amplifier albeit a small one. It’s not overly heavy, but it has enough of a weighted feeling that makes it feel like a solid amplifier and construction-wise, it’s quite pleasant to look at.While the Spark amp does not require the use of the accompanying smartphone app (it has dials for volume, tone, amps and more as you would expect), you definitely want to pair this thing via Bluetooth to your phone.The process was super easy. I turned the amp on, opened up the app and it found the amp and paired with it. That is all there is to it. If you don’t want to bother with the app initially, you don’t have to set anything up except what sounds you want.The appI think it is worth focusing on the accompanying Spark amp application that you install on your phone and use to interact with the amp. If you’re expecting an interface that offers the same level of customisation as the standalone Bias FX modelling software, taper your expectations now. Unlike the standalone software, you have a more limited selectio.I love the simplistic interface on the app and it is incredibly responsive on my Galaxy Note 10+. Cycle through the different sounds categorised by genre and select one, it’s instantaneous as the amp switches over to your new chosen sound.You are quite limited in terms of how many effects you have for your sounds, so you have limited options, and I couldn’t see the ability to choose and configure things you might be accustomed to in other digital modelling interfaces such as EQ for tuning the sound. You can still add a noise gate, a tube screamer, distortion and reverb pedals.Just remember, this is being sold as a practice amp, not a studio amplifier and recording tool you can use to create endless sounds. It’s meant to be good enough you can use it when you need something to practice or as you will learn, use in different settings.The one area where Positive Grid dropped the ball is the tuner. Many seasoned guitarists will have a standalone tuner or use their multi-effects processor (I use my HD500x tuner). You can’t tune down to different tunings, and it seems the tuner is primitive in that it assumes standard tuning, which seems strange for an amp that offers hi-gain amp models which would primarily be used for metal and more often than not, be in tunings lower than standard.It’s not worth marking down points on this amp just because the tuner is lacking in tuning options. I would be interested in seeing if, in the future, they add this feature into the app as clearly the amp and app are connected, I would imagine it might be possible. Long story short, not a big deal.Auto ChordsDespite falling under the app heading, I thought this feature warranted a section of its own in this review. I am a happy paid user of a service called Chordify. Essentially, it analyses a YouTube video and does its best to give you the chords that it hears.The Spark app has such a feature of its own called Auto Chords. What it will do is analyse a YouTube video or Spotify track and do its best to transpose the chords that it hears. It doesn’t work for all styles of music, technical tracks laden with solos and octave jumping (hello, Architects) will not be picked up, but rock music and other easier to analyse tracks are no problem.It’s not perfect, but in my testing it worked really well and just as well as Chordify. While this will not be replacing my Chordify subscription anytime soon, it is nice to have it in app and auto chords can be a great way to try and learn a new song if you don’t know much about music theory or have yet to develop your listening ear.Tonality & VolumeWhile 40 watts might not seem like a lot to some, it’s a lot for an amp of this size and for a practice amp. The reality is nobody has space soundproof enough where they can crank an amp to its full volume. If you’re using this in a bedroom or at a friends place while you work on new material, you’re only leveraging a fraction of the volume this thing has anyway.As a test, despite living in suburbia, I did crank this amp to its highest volume (for research purposes) and I was surprised how loud it was. You could easily use this amp for busking or small performances. If you wanted to use it for larger performances, you could mic it up and run it into a PA system no problem (you wouldn’t be cranking it all the way up in a setting like this anyway).There is nothing stopping you from using this as a live amp. I will be taking this with me to family gatherings where we often break out the guitars and everyone gets drunk and starts singing along. It’s loud enough for small gatherings and busking, but I wouldn’t play live with it (unless all of my equipment broke and it was a last-resort).Now, in terms of tonality, I found some of the sounds I played around with a little hit and miss. There are some great clean tones, some awesome gritty rock/blues tones, but when it comes to hi-gain territory, things get interesting.I primarily play a lot of down-tuned metal, deathcore, djent style music. Playing the heavier amp models and sounds, I found the sound to be very bass-heavy through the speaker and using headphones kind of solved the problem. Look, it’s a lot to ask for an amp of this size to be able to go toe-to-toe with the bigger amps (especially tube ones).Oddly enough, I did discover that some of the sound selections go through what I would call a “burn in” phase where once you choose a sound, if you wait a few seconds, the bass levels seem to come down. I don’t know if this is a bug or maybe just my ears adjusting to the sound change, but it was noticeable to me.I don’t know if it is possible, but I do believe that Positive Grid needs to add in EQ to this amp. Yes, it’s a practice amp and shouldn’t replace a real pedal or rack-mount effects system, but an EQ is essential for some. The average player will skip over such a feature and I just realised maybe I am missing the point here.You’re not going to get crystal clear tones for hi-gain amps on this amp and I think it is something to be aware of as I have seen some complaints about this. When you’re sold endless opportunities, it’s only natural your mind begins to wander and common sense goes out of the window a little bit.Dig around on ToneCloud, as there are some great sounds other guitarists have created. I have found some solid rock and metal tones on there, and even a hidden gem of a Polyphia tone for the sound on the track G.O.A.T which is clean and pure in its tone.ConclusionAs long as you remember the Spark Amp is not intended to replace a real amplifier nor is it meant to be a 1:1 hardware version of the Bias FX modelling software, you will be pleasantly surprised how great this little practice amp is. Since owning it, I have found myself playing the guitar a lot more than I usually would because it’s so accessible and just currently lives under my desk.If you’re buying this because you’re a beginner or thinking of buying it for someone else starting out, this is a fantastic learning and practice amp. Arguably, for its price, it is in a league of its own. If you’re an experienced guitarist who has owned real amps and dabbled in other modelling software and hardware, the Spark amp might feel limiting to you.You can find the Spark Amp on Amazon for a reasonable price here and lightning-fast shipping as always.here",2294
Cyberpunk 2077 (PC Review),"It’s hard to believe that it was eight years ago when we were told of Cyberpunk 2077’s existence. CD Projekt Red is a studio that has earned a lot of praise and goodwill thanks in part to the Witcher 3 game and subsequent expansions and free content. Although, with the launch of Cyberpunk 2077, that earned goodwill is rapidly depleting.I am not going to bury the lede here, Cyberpunk 2077 is at times fun, but it gets monotonous very quickly. The game we got is not the game many of us (myself included) thought we would be getting. The open-world is not as immersive as many would have hoped, the AI and NPC’s in the game are stupid.Usually, a review will go through the various components of a game, the gameplay, mechanics, story writing, acting. Sadly, this is going to be more an opinion piece because Cyberpunk 2077 is an incomplete game amounting to what I would consider a beta and if you’re playing on last-gen consoles, more of an alpha.For a game in development for eight years, I was expecting it to be on par with Grand Theft Auto 5, at the very least. To me, GTA V set the benchmark for what an open-world action game with the freedom to forge your own path and do whatever you want should be.In-fact, I find GTA V to this day to still be a quality game. Rockstar has done a tremendous job at keeping the game fresh and adding in new content (a whole island is coming to GTA V: Online this month).As a developer myself, my hunch is that Cyberpunk 2077 started off as a very different game and because of scope-creep and perhaps parts being rewritten, the development timeline blew out astronomically. This is an ambitious game and perhaps, a little too ambitious. Some allege that Keanu Reeves involvement made CDPR rewrite the story and parts of the game to give him more of a role in the game.Visually, Cyberpunk 2077 feels true to what many would consider Cyberpunk. The quirky outfits, excessive use of Neon, dystopian-style fonts and a world of hackers/underground militant groups and greedy corporate interests all at war with one another. A gritty and sinister world where it feels like anything can happen.But, there are flaws. I apologise in advance for the GTA V comparisons when I make them, I am referring to the gameplay and world, not the genre nor story writing. They are both different games but have enough similarities they are worth comparing in some aspects.Do this. Do that. Rinse Repeat.Do this. Do that. Rinse Repeat.I am only 10 hours into the game so far, so my experience might not be the same as yours. However, up until this point, things feel linear and not fluid. I shoot some bad guys, my chip/tech starts playing up, people shoot at me, I shoot back. I start to feel unwell, I see a RipperDoc.The dialogue is also same-same. I have limited choices in what I can respond with, it’s either A, B or C. Seemingly, no matter what I choose, the outcome seems to be the same every time. Sometimes I can talk my way out of a situation turning violent, but that is about it.A lot of the people on the street while you can try and talk to them when the option comes up, more often than not, they will just tell you to fuck off and not engage in conversation. Everyone in this city seems to be tightly wound and aggressive.In one of my first experiences, I tried talking to a police officer and they told me to beat it. I kept spamming the F key in hopes something would happen and something did happen: the police turned on me. A swarm of police spawned and started firing on me, just for trying to have a conversation and being non-threatening. It seemed heavy-handed, but in the current police climate in the US, not completely unrealistic.And yes, there are bugs. And yes, there are bugs. At the time of writing this, there is a litany of bugs ranging from broken pathfinding to instant spawning police and genitals clipping through clothing. Like the Witcher 3 at launch, I won’t focus too much on the bugs, because they are expected and will be fixed in the coming months. But, there are more bugs than a cheap motel right now.For the most part, whenever I have encountered a bug, I just exit the game and reload for it to fix itself. I couldn’t exit All Foods in one mission and in another where I had to search the body of an Arasaka agent, he fell through the elevator and I couldn’t search him.Who thought it was a good idea to map the crouch button to the skip dialogue button? For some interactions, I have to skip over a tonne of boring dialogue just to get to the point, sometimes it causes me to crouch as a result. It’s a minor annoyance, but it’s still damn annoying.Other funny bugs I have noticed include getting into a Delamain self-driving AI vehicle and seeing it struggle through the streets (maybe a satire of the current unfinished state of self-driving vehicles), taking rough turns and at times, driving through other vehicles (with no impact). I did laugh as I watched my Delamain vehicle drive through another, seeing the exterior shell disappear and seeing everyone sitting in mid-air.The comedic value of the bugs in this game gets old really fast though. At first, they were funny and more I played, the more they become an inconvenience (especially when I had to restart). As I said above, I won’t be focusing on them or marking the game down because of the bugs.Bugs aside, Cyberpunk 2077 is a hollow shell right now.Bugs aside, Cyberpunk 2077 is a hollow shell right now.Night City as beautiful as it is, feels very static and more of an aesthetic backstory accompaniment more than anything. The NPC’s are stupid, they’ll go about their day when an enemy attacks me, but if I take one swing at someone, they scream and panic. Can’t a Corpo swing his fists in public without inciting panic?It’s the little things that make the game feel less like a sandbox and more like a staged pre-scripted environment. Go into a store and take random objects that you find, nobody else reacts. The game is very dialogue-heavy, but the dialogue is more or less the same each time. Maybe GTA V has spoiled me in the living world AI.It is evident that there are many issues in Cyberpunk 2077 that go well beyond needing a simple patch, the game is very incomplete and feels like they rushed to get it out of the door. But considering CDPR received death threats for the previous delay, I can’t say I completely fault them.CDPR received death threats for the previous delayThis meme floating around Twitter feels a little too on the nose. Even CD Projekt Red has been forced to issue a statement regarding the atrocious performance on last-gen consoles. And the crazy thing is, an anonymous CDPR developer revealed months ago that game needed at least 4-5 months more work before being ready for release.has been forced to issue a statementan anonymous CDPR developer revealed months agoNight City does indeed feel big and buzzing, but it is sorely lacking in inner-details that pad the city out and make it live up to the scope of itself. There are numerous side-quests and you will get fatigued constantly being messaged and called all of the time. The world seems to be severely lacking in things to do.I am going to make another GTA V comparison. But, I would have expected that I could go and play a game of pool, being able to go into an arcade and play a game, go to the cinema and watch some dystopian thriller, maybe even go into a club and catch a show. I just want things to do that are not quests.I would have loved to been able to own property, have garages of dystopian Cyberpunk-esque vehicles and bikes I could choose to drive, maybe even own a nightclub or two.Maybe CDPR always intended to have these things, but evident by the mandated six-day working weeks and crunch, they were left out for the moment to just get the game done.Expect modders to plug holesExpect modders to plug holesEven if CDPR doesn’t fix all of the issues in this game or add in crucial missing features to make this game long-term and engaging, one thing is for certain: the modding community will step up. This game is a modders paradise waiting to be explored.Expect over the coming months and year to see numerous mods that not only plug in missing features/functionality, but go even further and fix some of the more broken aspects like the potato combat and driving mechanics.Echoes of No Man’s SkyEchoes of No Man’s SkyWhile the launch has been less than ideal, it’s not a complete unsalvageable trainwreck. If there is one game that CDPR can learn from, it is No Man’s Sky. When No Man’s Sky launched, it released in a buggy state and was missing a lot of features. Over the years, Hello Games continued to work on the game and it has become a masterpiece in its own right. Even Hello Games received death threats from some gamers as well.I can see the potential of Cyberpunk 2077 and I will continue to play it, even in its current flawed state. Is Cyberpunk 2077 a GOTY contender for 2021? Right now, I do not think it is. However, the base is there and if CDPR can continue to build this game up, the end result could be the greatest open-world game ever made (until GTA 6, that is).",2285
Fixing The Cyberpunk 2077 AMD Ryzen CPU Core Issue,"I have been absolutely enjoying Cyberpunk 2077, despite only finding the time to clock a few hours in the game. As an owner of an AMD Ryzen 3900x, like many Ryzen owners, there are known issues with core utilisation on Ryzen CPU’s.I’ll be honest, on my 3900x I had not noticed really many issues. I have experienced frame rate dips in the game but assumed it was optimisation related to the game itself and not core utilisation. So, naturally, I was curious about this fix that someone on Reddit discovered.someone on Reddit discoveredIf you are using a lower CPU like the 3600, admittedly, you are probably going to notice if your CPU is not being utilised properly more-so than you would on the 3900x for comparison sakes.The solution involves editing your cyberpunk executable file in a hex editor and changing some values. If you’re not technical, this might be a bit much for you, but the process is quite straightforward if you’re brave enough to do it. If you backup your .exe file first, there is no harm if you mess things up.Download a hex editor, like the free one HxD from here.Open up the cyberpunk executable file (depending on where you installed the game) and make sure it’s not the launcher for the game.Look for 75 30 33 C9 B8 01 00 00 00 0F A2 8B C8 C1 F9 08 Change it to EB 30 33 C9 B8 01 00 00 00 0F A2 8B C8 C1 F9 08Save the changes and open up the gameDownload a hex editor, like the free one HxD from here.hereOpen up the cyberpunk executable file (depending on where you installed the game) and make sure it’s not the launcher for the game.Look for 75 30 33 C9 B8 01 00 00 00 0F A2 8B C8 C1 F9 08 75 30 33 C9 B8 01 00 00 00 0F A2 8B C8 C1 F9 08Change it to EB 30 33 C9 B8 01 00 00 00 0F A2 8B C8 C1 F9 08EB 30 33 C9 B8 01 00 00 00 0F A2 8B C8 C1 F9 08Save the changes and open up the gameAs for the “why,” this fixes the issue or what this even does, admittedly, I have no idea. All I know is after making this change, even though the game was playable for me on my 3900x and RTX 3070, the game was noticeably more performant.",513
Gigabyte RTX 3070 Vision OC 8GB Graphics Card — Review,"I’ve had my eyes on an RTX 3070 card since they were announced, so too, have the scalpers. I was notified recently this card was in stock with my favourite retailer and I jumped on the opportunity.There isn’t a lot of information out there about this card, so I thought I would share my experience. I didn’t seek this card out, and it was first come, first serve more than anything else. I just wanted a card and bought the only card my retailer had in stock. Before we proceed, full disclosure: this isn’t a technical review, and it’s more of an everyday layperson review. How easy it was to install, the packaging, any gotchas. I won’t be running benchmarks or other things that reviewers do when talking about a card, as an everyday consumer I just want to know what to expect and if it can run popular next-gen games.Given the scarce availability of these cards, I know I paid more than I should have for this card. Retail price here in Australia should have been a couple of hundred cheaper. But, I wanted a card to play Cyberpunk 2077 and Microsoft Flight Simulator, so I pulled the trigger.First ImpressionsThe packaging is minimalist. A black box that opens up with a foam-cushioned card inside of it, standard graphics card packaging to be honest. There were no cables or any additional inclusions in the box, just the card and warranty information. This card is longer than any other card I have owned, and it takes up two slots worth of space. I have a full ATX case, so space was never an issue.This specific card has a 6 pin and 8 pin socket on it for power. So, make sure your power supply has two PCI-E power connectors. The card itself is beautiful. A nice silvery-white, adorned with a logo on the top of the card and three fans spanning the face of the card. The GIGABYTE logo is also illuminated, so if you’re all about lighting, you might appreciate that.Installation & SetupLike any other GPU, find a spare PCI Express slot with space large enough for the card and slot it in. Finally, plug in your six and eight-pin power connector cables.It’s that simple, in theory. For me, I had a brief moment of panic when my power cables would not go into the card whatsoever. I never thought to check the pins before doing anything else. This almost proved to be a fatal mistake for my card.In my research, I would discover that Gigabyte cards have a notorious reputation for shipping with bent pins on their cards. I had two options: either send it back and wait a month or two for a replacement as stock is limited or bend the pins back myself.I opted for the second option. Which honestly, I don’t recommend. I took a fine flat head screwdriver and carefully bent the pins back into place. On graphics cards, they make the pins and slots able to sustain a bit of force, so it worked out. But considering the price of this card, I won’t lie, I was angry. I shouldn’t be performing street surgery on my new RTX 30xx series graphics card.Once I got the card installed, everything went fine. I did, however, get a weird 4 beep thing happening during bios. Testing suggests the system is fine, but the new beeps were concerning. It turns out disabling CSM in my bios was the solution for this issue. This appears to be an issue with 30xx series cards as highlighted here.hereCyberpunk 2077The popular game right now is arguably Cyberpunk 2077. My RTX 2060 was obviously not going to cut the mustard with this visually taxing game. To complicate things further, I have a Samsung 34″ ultrawide monitor. The resolution on this requires a solid GPU to drive it in games at its native resolution of 3440 x 1440. It’s not quite 4k, but it’s a lot of pixels.In Cyberpunk 2077, at the Raytracing: Ultra setting the game is very playable. I did experience brief little moments of lag, but this might not have been a card limitation, and the game is still somewhat un-optimised. The janky moments were far few and between with this card, surprising given it’s a 3070, not a 3080.As I tour Night City on foot, in a car or on a bike, everything stands out. The reflections in puddles on the ground, the people walking by, the flashing neon signs and random city events. I didn’t go out of my way to adjust anything, and the 3070 handles Cyberpunk beautifully.I do not doubt if I toned down the raytracing to one level lower, maybe the game might fare better or adjusted my resolution not to be as big. But, I find it very playable on ultra with this card, combat and other intensive scenes are fine.Microsoft Flight Simulator 2020Once again, the 3070 performed really well in MSFS 2020 which is a notorious GPU pushing game that also requires a decent CPU as well. I could get decent frames with my 2060, but I did have to work hard to get it to work. Fortunately for MSFS 2020, 30 fps is more than acceptable for a sim.I found with this 3070, I was able to push up my settings higher and see the game in all of its beautiful real-time beauty. Undoubtedly, I would probably need to go for a 3080 to go even higher and get slightly higher frames, but anything over 30 is good enough for MSFS 2020.Coil Whine, Noise & TemperaturesAs with any modern GPU, this card has 0-decibel fans. Which means they won’t come on until they need too and it’s only really when you’re gaming do they come on. As they are running, you can’t even hear them either. The temperatures at idle or normal desktop usage were a lot lower than my 2060 this card replaced.Temperature stability is excellent. I never saw this card push 60 degrees, for me it seemed to idle around 40 celsius, but I live in Australia, and it is super hot here at the moment, and my study is not exactly cool once the sun hits the front of the house in the afternoon. I have yet to try and push the clock speeds; I am still in the honeymoon phase where I am too afraid to experiment with my card just yet.And unlike my 2060, my 3070 does not suffer from any coil whine or other issues whatsoever. I have had mixed experiences with various Nvidia cards (Gigabyte ones in particular) and coil whine, it’s nice not to have a card that doesn’t have it for once because the sound of coil whine will slowly eat at your sanity.ConclusionDo I recommend this card? Bent pins issue aside, yes. If you buy it, make sure you check your pins are not bent too. If you’re patient, I do recommend waiting until this card is cheaper, but if you don’t want to wait, I still recommend it. Just keep in mind this card is massive, you’ll need a bit of space for it.You can find the card being sold by numerous sellers on Amazon here.here",1641
"GitHub Gets Dark More (finally), Here Is How You Turn It On","A large majority of developers love dark mode. And for years, GitHub (the developer platform of choice for source control) has noticeably been devoid of dark mode. Which is kind of ironic, considering it’s the popular choice.There are third party scripts and ways of making GitHub dark, but a native dark mode is always best. Admittedly, I’m not motivated enough to go installing some third party creation (especially if the slightest change can break it).During GitHub Universal, they announced a lot of cool things, but one thing they announced has overshadowed everything else for me: native dark mode support. To me, that is all that matters.GitHub UniversalFinally, I don’t have to have my retinas seared like a steak on a molten lava hot grill late at night when I find myself on GitHub (which is frequently).If you visit this link to your appearance settings, you have the option of the light scheme (yuck), choosing to use the system default (if dark mode is enabled on an OS level, it will turn dark) and finally, always enable dark mode (which you should).visit this link to your appearance settingsSeriously, look how beautiful GitHub is now.And they are still not done yet. If you find the contrast too polarising, GitHub have plans to ship more colour options for those with accessibility issues.
After we are done with dark mode, we will ship accessible color modes for people with color blindness.— Nat Friedman (@natfriedman) November 23, 2020
After we are done with dark mode, we will ship accessible color modes for people with color blindness.— Nat Friedman (@natfriedman) November 23, 2020After we are done with dark mode, we will ship accessible color modes for people with color blindness.November 23, 2020",432
A Fix For Issues Calling The Yelp API Using wp_remote_get (and other WordPress request methods),"This is quite a specific issue you will only encounter if you are attempting to call the Yelp Fusion API using any of WordPress’ request methods such as wp_remote_get (that’s probably the one you are using). In attempting to do so, you will find yourself faced with a 403 error.wp_remote_getYou’re passing up your API key or valid token, but still, the request won’t work.This issue goes back a few years, but the bottom-line is Yelp block requests made from user agents featuring “WordPress”. The default user-agent for wp_remote_get and other methods just so happens to be this.wp_remote_getThis issue has a sort of explanation from Yelp. They don’t go into specifics and say why, but the workaround is simple. Set the user-agent to empty or something else.This issueIn my case, I ended up with the following:In my case, I ended up with the following:$args = array(
  'headers'     => array(
    'user-agent' => '',
    'Content-Type' => 'application/json',
    'Authorization' => 'Bearer ' . $token,
  ),
);

$response = wp_remote_get($endpoint, $args);
$response_code = wp_remote_retrieve_response_code( $response );
$response_body = wp_remote_retrieve_body( $response );$args = array(
  'headers'     => array(
    'user-agent' => '',
    'Content-Type' => 'application/json',
    'Authorization' => 'Bearer ' . $token,
  ),
);

$response = wp_remote_get($endpoint, $args);
$response_code = wp_remote_retrieve_response_code( $response );
$response_body = wp_remote_retrieve_body( $response );Like I said, a specific problem only to WordPress users who are using the in-built request methods. Curl and other ways you can make a request like file_get_contents do not experience this issue.file_get_contents",427
Using The Elgato Stream Deck XL With Microsoft Flight Simulator 2020,"The Black Friday deals were a bit so-so this year, but one thing I did get at a way cheaper price than RRP is the Elgato Stream Deck XL. I have had my eyes on this for a while now, but the almost $500 AUD price tag was a deterrent. I managed to get it for $336, a substantial discount from the $460 price it usually is.Despite the name “Stream Deck” the Elgato Stream Deck is not just for streamers. I do believe that was their initial focus and probably a large part of their customer base are streamers, but this thing can work with your entire system.The biggest benefit of the Elgato Stream Deck is that it can be used to hotkey your workflows. I use it with Adobe Premiere Pro to make my workflow a little less mouse heavy. I use it in Dota 2 for certain things I would have used my keyboard for. And specifically (it is in the title after all), Microsoft Flight Simulator 2020.Now, this is where the Stream Deck truly shines. Being a simulator that allows you to fly some complicated jet liners, the sheer number of keyboard shortcuts and procedures you have to remember to even take off, its exorbitant.I wanted the Elgato Stream Deck and the XL variant to make flying a little easier. Being able to use it to raise and lower the landing gear, disengage the parking brake and other operational aspects of flying a commercial jetliner in a simulator.If you have the non-XL variant of the Stream Deck, you can still use it. The XL just has a few more buttons in each row. While there are a few choices now for using a deck with MSFS 2020, really, the one I recommend is SPAD.neXt. Unlike other interfaces, SPAD has dynamic value support, meaning it can display values on the interface in real-time.If needing support for real-time values is not important to you, another option is FSXFollow which seems to be more static, but just as powerful in the way of workflow support and profiles.FSXFollowIf paying for an addon to get Stream Deck support does not sit right with you (maybe 2020 drained your wallet and bank accounts) you can do it the old fashioned way. It’s slightly more tedious, but this video below shows you how it can be done.

You can either use a third-party plugin/software to add in support, or you can do it manually. Either way, the Elgato Stream Deck can be used to control your MSFS 2020 and it makes life so much easier for a game that is so workflow/hotkey dependent.",599
"NBN Box Installed Inside of Garage, Where Do You Put The Modem?","My wife and I just sold our house, and for the next seven or so months, while we build a new home, we are renting. The rental property has NBN installed, and annoyingly the NBN box is located inside of the garage.Googling seems to suggest that this is quite common, especially for NBN fibre connections. The problem with the NBN box being located in the garage is obvious: the reception sucks, and in larger homes, you get blackspots.You have to plug a modem into the NBN NTD device, and because the NBN NTD is inside the garage, it means the modem has to be close to plug into the appropriate port.There were a few options to consider ranging from expensive to affordable:There were a few options to consider ranging from expensive to affordableRunning a large ethernet cable and putting the modem inside of the house somewhere (affordable).Purchasing a wifi range extender (affordable, but unreliable)Creating a mesh network (Google Nest Wifi or Netgear Orbi) which is expensiveRunning a large ethernet cable and putting the modem inside of the house somewhere (affordable).Purchasing a wifi range extender (affordable, but unreliable)Creating a mesh network (Google Nest Wifi or Netgear Orbi) which is expensiveIn most circumstances, you would wire up ethernet ports throughout the house and have them done by a registered cabler. Being a rental, this is out of the equation.The solution was just to run a large ethernet cable, neatly tuck it away against the edges of the walls and place the modem inside the house. The cable itself was only 20 or so metres, so nothing overly long. I’ve done many speed tests and haven’t noticed a drop in speed using this solution.I am considering adding a Google Nest Wifi into the mix, but this is an acceptable (not ideal) solution for now. My wife isn’t a fan of visible cables (neither am I these days), but you’re limited to what you can do (stopping short of putting holes in the walls).",483
"Dear Greys Anatomy, Stop Doing Crossover Episodes With Station 19","My wife loves Grey’s Anatomy and while I do find it somewhat cheesy at times, it’s our thing that we watch together and another couple of good friends. Sometimes it can be entertaining television.But, we need to address the big fat elephant in the room: Station 19. The show is associated with famed Grey’s Anatomy creator Shonda Rhimes set in the same universe as Grey’s Anatomy. It is produced by Shonda’s production company Shondaland. Now, given the success of Grey’s Anatomy, it’s understandable ABC would give her another show.These crossover events appear to be becoming a regular thing. Season 16 at the start of 2020 debuted with a crossover with Station 19 after a car crashed into Joe’s Bar. They have done a few of them now, seemingly timing them with the season debut tricking their fans into watching them.Well, the November 2020 Season 17 debut of Grey’s Anatomy has done it again and this time, taken things to an extreme level with a three-part crossover event of a TV series Grey’s Anatomy viewers don’t even like.taken things to an extreme level with a three-part crossover eventGrey’s Anatomy viewers don’t even likeIf Station 19 was a good TV show, there would be no problem. But the cast is unlikeable, the storylines predictable and boring, the writing is terrible for a show associated with Grey’s Anatomy. The biggest issue with Station 19 is the dialogue and acting of the actors and actresses is bad. I’ve seen better acting in hostage videos.We have suffered enough in 2020. Please stop forcing us to watch Station 19.",387
"Galaxycove Are At It Again, Running a Fake 50% Off Sale","I’m getting sick of blogging about Galaxycove, but they just can’t help but continually be dishonest and attempt to deceive people.I received an email from Megan at Galaxycove today claiming they are having a 50% off sale. Instead of $159 USD, they’re claiming to sell them at $79 USD instead.Here is the thing. They are always being sold at $79 USD, even before this alleged sale. That’s the price that I paid months ago.Don’t fall for the lies. Galaxycove has shown that it is a dishonest and unethical company who uses fake images, fake reviews and attempts to get any negativity abkht them scrubbed from the internet.There is no sale. I assure you that these projectors are not worth $20 let alone the alleged $159 USD price they claim they usually are.",189
This is Why People Hate Banks: Deutsche Bank Proposes 5% Work from Home Tax,"In what can only be described as the world’s worst idea, some economists at Deutsche Bank have proposed taxing workers 5% of their salary if they choose to work from home.proposed taxing workers 5% of their salaryIn the linked story it goes on to sayBy working from home, people aren’t paying for public transport or eating out at restaurants near their places of work, while expensive offices remain virtually empty.By working from home, people aren’t paying for public transport or eating out at restaurants near their places of work, while expensive offices remain virtually empty.Since when am I or any other contributing member of society obligated to spend their money? Why should people be taxed because they are not using public transport (many of which only use out of necessity, despite how poor it is in many countries).The message they are trying to put out is, people working from home are not consuming and not consuming is bad for those who rely on people consuming. If anything, working from home has bettered our society.Working from home means you’re contributing less carbon and greenhouse gas emissions, roads require less maintenance, pollution is reduced, waste and landfill is reduced as people are eating out and drinking takeaway coffee a lot less.Some research suggests Deutsche Bank are heavily invested in commercial real estate, so that might give some insight into why they are advocating for such a tax, they are panicking and desperate. Sad.This is why people hate banks and have serious trust issues. It’s all about money and this latest idea is just stupidity.",399
Marak Takes An Open Source Stand,"If you are not familiar with Marak on GitHub, you might be familiar with some of their work, most notably Faker.js which is a Node.js package for generating fake data.Faker.jsWell, Marak is taking a stand and honestly, it’s commendable and I 100% support it. In a Github issue on the Faker repository titled No more free work from Marak – Pay Me or Fork This, while it might appear that this was prompted by a Fortune 500 company asking for things, the following Tweet might provide some insight.No more free work from Marak – Pay Me or Fork This
I lost all my stuff in an apartment fire and am barely staying unhomeless. Lost access to most of my accounts. All precious metal is missing. If anyone could bless paypal@marak.com with a little cash it would help me from freezing on the street. lol.— marak🗿🧊⏳ (@marak) October 25, 2020
I lost all my stuff in an apartment fire and am barely staying unhomeless. Lost access to most of my accounts. All precious metal is missing. If anyone could bless paypal@marak.com with a little cash it would help me from freezing on the street. lol.— marak🗿🧊⏳ (@marak) October 25, 2020I lost all my stuff in an apartment fire and am barely staying unhomeless. Lost access to most of my accounts. All precious metal is missing. If anyone could bless paypal@marak.com with a little cash it would help me from freezing on the street. lol.October 25, 2020Sadly, it appears Marak lost everything in an apartment fire as well as access to accounts. Understandably, pouring your own free time into a free library that large companies are using to save in development costs, it’s salt being rubbed into your eyes.
There has been a data leak somewhere in FAANG and I can tell you with certainty billions of dollars are being made off our work and there are multi-millionaires literally laughing at us. #robotstrike cc @substack pic.twitter.com/GUH4ymDWxM— marak🗿🧊⏳ (@marak) November 9, 2020
There has been a data leak somewhere in FAANG and I can tell you with certainty billions of dollars are being made off our work and there are multi-millionaires literally laughing at us. #robotstrike cc @substack pic.twitter.com/GUH4ymDWxM— marak🗿🧊⏳ (@marak) November 9, 2020There has been a data leak somewhere in FAANG and I can tell you with certainty billions of dollars are being made off our work and there are multi-millionaires literally laughing at us. #robotstrike cc @substack pic.twitter.com/GUH4ymDWxM#robotstrike@substackpic.twitter.com/GUH4ymDWxMNovember 9, 2020It might be the headspace they are in right now, but there is a larger point here. For years large profitable companies have been operating and built on the backs of open source, many giving nothing back.For a good laugh, check out the Facebook Jest repository, they are asking for donations. Facebook, a company worths tens of billions is asking for donations on their own open-source project, if that isn’t messed up, then what is? Facebook Jest repositoryOpen source is broken and nobody seems to have the solution.",755
Are GalaxyCove Posting Fake TrustPilot Reviews?,"Another day, another possible controversy marring the tainted Galaxycove brand that is prevalent across social media. In case you missed my review and experience buying a Galaxycove projector, you can read about that here. You can also read about Galaxycove flagging legitimate negative reviews on their TrustPilot page here.herehereAfter having my negative and legitimate review flagged, I got it reinstated by providing proof that I actually purchased a projector from Galaxycove, my review now has the “verified” label on it over at TrustPilot.This didn’t stop Coco from Galaxycove attempting to do some damage control and in the process, attempt to discredit my review.This really ticked me off. My review was flagged in the first place (possibly by Coco), and it was only by providing proof of my purchase that it was reinstated, but they then go on to say, “I cannot locate your account.”Enough about my experience, let’s talk about the influx of suspicious activity on Galaxycove’ TrustPilot account. Fortunately, TrustPilot provide a transparency report of sorts with some useful data showing you when reviews were posted and the source of them.In the space of 10 hours, two people conveniently posted 5-star reviews raving about their experience with Galaxycove. If positive reviews were common for Galaxycove, this would not be suspicious at all. But, let’s take a closer look.And then another positive review, posted around the exact same time…Of these two people, this was their first review and they also have no profile image.And then again the day before, another positive 5-star review from someone with no existing reviews or profile image.The suspicious thing about all of these reviews is they end in exclamation marks or use them more than a couple of times throughout. The language also feels way too upbeat and over the top. These reviews seem to echo the overly positive reviews on their own website.A majority of these positive reviews have also been added in the last week or two, within a short time of one another. This graph on the TrustPilot transparency profile shows the influx of reviews in October.You can see that a majority of the reviews received in October were overwhelmingly negative. Despite Galaxycove attempting to flag these reviewsIf you read the reviews in sequential order, you begin to spot a bit of a pattern. Seemingly every time someone posts a negative (or in one case, an average review), a 5-star review is conveniently posted to attempt to offset the negative review, a game of cat and mouse. The way it is being done appears to not be one right after the other.What are the odds three five-star reviews were posted on Galaxycove’s Trustpilot profile in the span of 24 hours? Especially when the reviews are overwhelmingly negative and complaining about many of the things that these positive reviews are praising.All of this raises the question: Are Galaxycove intentionally posting fake reviews on their profile? The answer it would seem is obvious. But without proof, all we can do is speculate and use what information we have at hand.Update #1 – 5 November, 2020Update #1 – 5 November, 2020It appears that quite a few more positive reviews have shown up on the TrustPilot page for Galaxycove. They are doing their best to fight the negative reviews, but once again, all of the 5-star reviews are from new accounts and they are written with excessive exclamation marks again.Other people are starting to notice. The latest negative review is asking for TrustPilot to review the positive reviews.I can almost guarantee that Lisa’s review will be removed in the next few days. Interestingly, something else I noticed in the pattern of the reviews (despite the exclamation marks) is they are all quite small, roughly around the same length (a couple of sentences).",955
Quibi Proves Stupid Money Still Exists In Tech,"It seems that almost no one learned from the dot com crash of the early 2000s which saw numerous internet companies valued at clinically insane valuations and overfunded fail causing widespread layoffs and market turmoil. Quibi is the latest high-profile casualty that has announced it is shutting down.that has announced it is shutting downThe shutdown was honestly, all but inevitable, Quibi was never going to work. Which is why it might surprise some to learn they raised almost $2 billion US dollars, securing $1.75 billion in funding. Who in their right mind thought this was a good idea?securing $1.75 billion in fundingI can’t help but wonder, if instead of throwing this kind of money at a singular company, that $1.75 billion was instead given to a handful of startups (maybe a few million each)? At that amount, investors could have given 500 startups $3.5 million each in funding or 250 startups $7 million each.Think about it. Investors threw away money by pumping it into one company with an idea that was going to compete with other short-form video services like TikTok, YouTube and even Instagram (which do not charge for access).And think about this entire situation even more. Quibi managed to fail in six months, after securing $1.75 billion in funding, which is burning approximately $7000 per minute.The issue here is that the investors thought it was a safe bet investing in big business names over common sense. While Jeffrey Katzenberg and Meg Whitman might have amassed decent fortunes elsewhere and made a name for themselves in business.This is hot on the heels of the disaster that was WeWork, it seems we have returned to the early days of stupid investments and I would not be surprised if we eventually see a dot com boom fuelled by the pandemic in 2021/2022.One thing is for sure, Katzenberg and Whitman are going to be fine. They were billionaires before Quibi and will be billionaires after Quibi. The investors who stupidly parted with their money deserve no sympathy for failing to see that this was never going to work, pandemic or not.",519
Google Is Finally Being Taken To Court Over Antitrust Violations,"The US Justice Department has announced they are suing Google for antitrust violations. The crux of the legal matter stems from this line in the filing, “unlawfully maintaining monopolies in the markets for general search services, search advertising, and general search text advertising in the United States.”has announced they are suing Google for antitrust violationsIf you don’t mind a bit of dry legal jargon and extensive reading, the full filing is available here and it is quite interesting the information they have collected during their investigation.the full filing is available hereWhile other companies (some negatively affected by Google’s anti-competitive practices) begin weighing in, even Mozilla has weighed in on the case. It’s no secret that Mozilla relies heavily on its agreement with Google and search (also cited in the legal filing) so its opinion should be taken with a grain of salt.even Mozilla has weighed in on the caseIf anything, I am disappointed by Mozilla’s response. The fact they basically feel the need to say, “Hey, we need Google’s money, please be careful” goes against their image of being pro-privacy, anti-monopoly. Google probably only funds Mozilla so they can proclaim to not be a monopoly, a shell corporation of sorts to make Google look fair.Whether or not that case will have the same wide sweeping impact that the Microsoft case did, remains to be seen. It is ironic that the case brought against Microsoft in the ’90s paved the way for companies like Google to exist and grow, only for Google to itself become an eventual target.",396
Apple Has Lost Its Damn Mind,"Chances are, you and everyone you know or love has heard about the new iPhone 12 that Apple just announced at one of its cult-like keynote events. Now, Apple releasing a new iPhone is not surprising, it’s about as expected as the sun rising tomorrow.

While Apple has run out of ways to surprise and delight their customers as competitors and Apple find themselves on equal footing in terms of features and power, sometimes Apple finding themselves behind, they have managed to shock consumers once again and it’s not a new feature that has everyone talking.Apple just removed the charger and headphones from its products. Meaning, all new iPhones from the iPhone 12 going forward will come without a charger or headphones, which is being touted as an environmental move.Now, this is not a total surprise. Rumours have been out there for months that Apple was going to do this and now we know they are no longer rumours. And surprisingly, people were on board with the idea of Apple doing it.And look, Apple has a valid point here. A lot of people have one or more orphaned chargers, cables and headphones in a random kitchen drawer. But the wider point here people are missing is: a charger is a crucial accompaniment for a phone.What about first time iPhone users. those upgrading from older devices which will not have compatible chargers for the iPhone? The same thing for headphones, the included headphones with iPhone’s (and phones in general) don’t last very long.Issues with people who need a charger aside, the price of the iPhone is remaining the same. How is it Apple can ship a device with less overheads (because of lack of inclusions) and yet, the price of the iPhone remains the same?To Apple’s credit, they did drop the cost of EarPods and iPhone charger prices by $10. When you consider that Apple is probably saving more than the cost of buying a charger and EarPods outright, evidently the addition of 5G has driven up the price of the new iPhone’s (not that 5G is even widely supported in most parts of the world anyway), they’ve removed a couple of additions and kept the price of the iPhone the same.they did drop the cost of EarPods and iPhone charger prices by $10While some will have adapters around to use, many will undoubtedly buy a new adapter to go alone with their new shiny iDevice. If you go into the Apple store, you best believe they’ll be driving you to purchase the new MagSafe charger at $65 Australian dollars.Or you can buy yourself a USB-C adapter for $29 AUD from the Apple store to go along with the included cable.
Apple is pitching us on the environmental value of not including a charger or headphones–with a $1,200 iPhoneI see what you did there @Apple!— jason@calacanis.com (@Jason) October 13, 2020
Apple is pitching us on the environmental value of not including a charger or headphones–with a $1,200 iPhoneI see what you did there @Apple!— jason@calacanis.com (@Jason) October 13, 2020Apple is pitching us on the environmental value of not including a charger or headphones–with a $1,200 iPhoneI see what you did there @Apple!@AppleOctober 13, 2020Once again, the issue here is not necessarily that Apple has unbundled inclusions that many have come to expect when they fork out $1000+ for a new shiny aluminum phone, it’s the fact they have done it in a way that intentionally forces people to buy USB-C adapters.We know they have done this because when you buy the new iPhone 12, it comes with a USB-C to Lightning cable. Now, a large majority of iPhone owners will have USB-A chargers. This gives them one of two options, use their old charger and cable (the cable with that trademark Apple fraying), or buy a USB-C adapter and use the included cable.
So,There's millions of USB-A power bricks out in the world right nowSo Apple won't include one w/ iPhoneB/c you likely have a USB-A power adapterSo it's ""for the environment""But now they ship iPhone with lightning to USB-C cableWhich doesn't plug into those old bricks— Steggy (Jeff Stegner) (@Steggy) October 13, 2020
So,There's millions of USB-A power bricks out in the world right nowSo Apple won't include one w/ iPhoneB/c you likely have a USB-A power adapterSo it's ""for the environment""But now they ship iPhone with lightning to USB-C cableWhich doesn't plug into those old bricks— Steggy (Jeff Stegner) (@Steggy) October 13, 2020So,There's millions of USB-A power bricks out in the world right nowSo Apple won't include one w/ iPhoneB/c you likely have a USB-A power adapterSo it's ""for the environment""But now they ship iPhone with lightning to USB-C cableWhich doesn't plug into those old bricksOctober 13, 2020For a private company that is beholden to shareholders, pardon me for being sceptical of a company who has been marred in controversy ever since they announced the first iPhone. From poor working conditions in its factories to child labor in cobalt mines.child labor in cobalt minesThe irony here is honestly outstanding.",1237
WordPress Gutenberg Is Still An Unmitigated Disaster,"It is difficult to believe that WordPress’ Gutenberg editor is almost two years old (released in WordPress 5.0 in December 2018). Since then, Gutenberg has seen a plethora of development and resources thrown at it, promising a revolutionary writing experience driven by blocks.So, two years on, has Gutenberg become the beloved new writing experience in WordPress? Have the wrinkles been ironed out? Nope.Despite the amount of work developers have put into polishing Gutenberg and trying to make it a decent writing experience, many continue to avoid it at all costs. The outrage was overwhelming that an official plugin restoring the traditional writing experience was released called: Classic Editor plugin. It will allegedly be supported until 2022, but who honestly knows if it will be extended beyond that.Classic Editor pluginTo truly assess community sentiment, you only have to look at the star ratings for the Classic Editor plugin versus the Gutenberg plugin. The classic plugin has an average rating of 4.9 out of 5 with overwhelmingly positive reviews. It’s a stark contrast.overwhelmingly positive reviewsComparatively, the Gutenberg plugin has overwhelmingly negative reviews and an average star rating of 2 out of 5 stars.overwhelmingly negative reviewsIt still breaks GrammarlyI know not everyone uses Grammarly, but I do. For me, Grammarly is an essential part of my writing process. The Gutenberg writing experience completely breaks Grammarly because Gutenberg thought it would be good to turn the editor into a block-based writing experience (every new line is a block).I have to click into each separate paragraph (inside its own HTML div) and wait for Grammarly to check it. This is frustrating compared to the classic writing experience, which does not break the flow of text into blocks.I wanted to like GutenbergAdmittedly, when I first started using Gutenberg after a decade of using WordPress as a blogging platform, a CMS and building things with it (including web applications), it was a shock to the system. As a developer myself, I know the frustration of making things better and your users being stuck in the past.I persisted with Gutenberg because I thought it would get better almost two years on. I still prefer the classic editor and find Gutenberg incredibly complex, from its codebase to its horrendous user interface and performance issues.The thing is, the team focused so hard on producing a flexible, customizable and configurable editing experience that they forget that a majority of WordPress users only care about what their posts look like on the front-end, not when they are editing them in the admin panel.The biggest issues of all are the Gutenberg editor is confusing for WordPress newcomers. I have had clients with no WordPress experience who have gone in and experienced Gutenberg for the first time. I wonder if they even tested this editor with groups of people first to get feedback? Because many of my clients have struggled with it (to the point where I have reverted it for a couple of them).If the Gutenberg team could only cast aside their egos and listen to the community, they would see that forcing this block writing experience on people who don’t want it (especially developers) is the wrong way to go about things.One positive…One positive…While I’m not too fond of Gutenberg for writing blog posts, there is one area where Gutenberg makes sense: pages. Although behind the scenes pages and posts are the same in the DB, they serve different purposes. To me, Gutenberg makes sense if you view it as an officially supported page/site builder.But, forcing blog posts into a DIV soup block writing experience feels unnatural, and it causes endless headaches. I still encounter problems from time to time selecting all content inside of a Gutenberg block editor. Sometimes it won’t select everything, and copying/pasting results in sporadic formatting issues.The right thing would have been for Gutenberg to be optional. It has almost been two years, and people are still unhappy with Gutenberg. It’s time it became optional and not the default. Keep working on it, but let us choose if we want to use Gutenberg or the more simplistic and easier to write and develop with the classic editor.",1064
Galaxycove Continues Its Dishonesty and Attempted To Have My TrustPilot Review Removed (amongst others),"I recently published a review here on this blog about my experience buying the Galaxycove projector. In the review, I talked about how the projector was a poorly made dropshipped projector, how it is unbranded and the company has an intentionally misleading website. published a review hereIt appears that Galaxycove is desperately attempting to control the narrative by scrubbing any negativity of their projector from the Internet. Although, the Galaxycove website might have glowing endorsements from alleged customers and their Facebook and Instagram, over on Trustpilot the story is different.over on TrustpilotI also left a review on Trustpilot, providing a valid experience I had spending over $100 Australian dollars for this less than impressive projector being sold by a company intentionally misleading its customers.A couple of days ago, I received a notification from Trustpilot asking me for more information. It appeared as though Galaxycove were attempting to report every single negative review about them on Trustpilot. Here is the email.I could see what Galaxycove were attempting to do, so I complied. I provided my name, email and other details proving I was a customer. Which is why I was surprised when I received this email a couple of days later.These crooks reported my review, even though they would have been able to look up my order using my provided details and see I was a valid customer.Not content with laying down and letting this company continue to deceive people out of their money for their very overpriced products, I provided supporting evidence of my order and sent it to Trustpilot. Curious, I decided to see if the other negative reviews were also flagged and sure enough, they were.This isn’t all of them. There are a total of 18 reviews, 17 of those are negative and one positive (which I am sceptical about). Somehow, one of the negative reviews is visible (perhaps restored after providing evidence).This glowing review from “Connor” which is a five-star review is clearly fake. Considering it took weeks for me to receive my projector I somehow doubt Connor got their projector in 4 days, after seeing the number of hops my package took.As the Trustpilot page currently stands, that leaves a total of 16 negative reviews currently flagged by Galaxycove. They went and flagged every bad review they could find on the site. Go take a look for yourself here, I have never seen anything like this before.take a look for yourself hereUpdate October 14, 2020: It looks like Trustpilot has started reviewing some of the flags that Galaxycove placed on all of the negative reviews. As you can see on the business transparency page here, the Trustpilot team are reinstating a lot of the flagged reviews. So far, the Trustpilot statistics state that, “55% of the flagged reviews were flagged for invalid reasons.” this means Galaxycove flagged these reviews for the wrong reasons.Update October 14, 2020hereIs it any wonder they are desperately trying to salvage the sinking ship that is their company? Their Trustpilot rating is 1.8… The narrative is slipping from their hands.I assume my review will be back momentarily (I will make sure of it), but this experience goes to show that Galaxycove is trying to cover up how terrible their products are.",822
AMD Zen 3 Strikes a Final Blow Against Intel,"As great as AMD CPU’s are, Intel has had a distinct advantage even though the Zen 2 architecture was stellar and I upgraded to a Ryzen 3900x not too long ago and absolutely love it, it’s a performance and core monster.Intel’s only advantage against AMD was single-core performance. All of the benchmarks for 1080p gaming had AMD trailing behind Intel, with AMD beating Intel in multicore benchmarks.Thanks to a different architecture in Zen 3, the amount of latency between the CCX’s which resulted in some reduced performance on Zen 2 is now gone. If you’re wanting to know what core complex is and how it works, Tom’s Hardware has a great explainer here. The performance is claimed to offer a 2.4x performance per watt increase and 19% higher instructions per clock.hereOne of the heaviest punches thrown at Intel was when they showed Ryzen 9 5900X and Intel’s Core i9-10900K running on Cinebench, where AMD show they beat Intel in single-core processing speed by a boast-worthy 87 points. We don’t know the specifics of how they tested, but 87 points is a mammoth edge over Intel.Launching November 5, 2020, AMD is dropping 4 new Zen 3 CPU’s:Launching November 5, 2020, AMD is dropping 4 new Zen 3 CPU’s:Ryzen 9 5950X: 16-cores/32-threads, 3.4 GHz base clock (4.9 GHz boost), 105W TDP ($US800 ($1,118))Ryzen 9 5900X: 12-cores/24-threads, 3.7 GHz base clock (4.8 GHz boost) 105W TDP ($US550 ($768))Ryzen 7 5800X: 8-cores/16-threads, 3.8 GHz base clock (4.7 GHz boost) 105W TDP ($US450 ($629))Ryzen 5 5600X: 6-cores/12-threads, 3.7 GHz base clock (4.6 GHz boost) 65W TDP ($US300 ($419))Ryzen 9 5950X: 16-cores/32-threads, 3.4 GHz base clock (4.9 GHz boost), 105W TDP ($US800 ($1,118))Ryzen 9 5950XRyzen 9 5900X: 12-cores/24-threads, 3.7 GHz base clock (4.8 GHz boost) 105W TDP ($US550 ($768))Ryzen 9 5900XRyzen 7 5800X: 8-cores/16-threads, 3.8 GHz base clock (4.7 GHz boost) 105W TDP ($US450 ($629))Ryzen 7 5800XRyzen 5 5600X: 6-cores/12-threads, 3.7 GHz base clock (4.6 GHz boost) 65W TDP ($US300 ($419))Ryzen 5 5600XThe Zen 3 architecture is the final nail in Intel’s coffin if third-party benchmarks see the same dramatic increases as we were shown in AMD’s press conference.",545
DirectX 12 Support Is Coming To Microsoft Flight Simulator 2020,"Recently in a developer Q&A with the head of the Microsoft Flight Simulator 2020 team and CEO of Asobo, some key community questions were sort of addressed by the team.One of the questions was about DirectX 12 support. In the following timestamped embed, Sebastian Wloch says they have a big team working on Direct X 12 support.

If you’re time scarce or don’t want to watch the video, basically DirectX 12 support is summarised:There is a big team working on DirectX 12 support and they have been working on it for a whileIt’s not going to be for performance reasons mostlyDx12 support is being added for things like ray tracing and new effectsRendering work is already handled in a separate process/coreSebastian goes to say that it won’t provide big performance increases but quickly corrects himselfNo release date is given for Dx12 supportThere is a big team working on DirectX 12 support and they have been working on it for a whileIt’s not going to be for performance reasons mostlyDx12 support is being added for things like ray tracing and new effectsRendering work is already handled in a separate process/coreSebastian goes to say that it won’t provide big performance increases but quickly corrects himselfNo release date is given for Dx12 supportTo understand why DirectX 12 support is such a big deal, you have to understand two of its big features. One of which is support for multi-CPU and multi-GPU setups. Traditionally, games on DirectX 11 and below could only support a maximum of 4 cores (as evident in the current Dx11 version of MSFS 2020).The reason for the seemingly large amount of resources and time it is taking Asobo to support Dx12 is a classic case of, “easier said than done” DirectX 12 will potentially open up some new performance gains for multi-CPU users (able to spread rendering work across multiple cores) but it requires a lot of work from Asobo to support it.Coupled with another highly anticipated feature in DirectX 12 asynchronous compute, it allows compatible cards to perform multiple operations at once on the card itself. Effectively, by supporting asynchronous compute, latency is reduced resulting in better graphical performance.What kind of performance gains we will get from Dx12 remain to be seen in MSFS 2020.",566
In Defence of Hacktoberfest,"I am sure you might have seen this post doing the rounds recently, titled DigitalOcean’s Hacktoberfest is Hurting Open Source. Despite the spicy post title an exaggerated claim of Hacktoberfest being a corporate distributed denial of service attack, it does make some valid points about Hacktoberfest.DigitalOcean’s Hacktoberfest is Hurting Open SourceI don’t operate many popular repos and I’ve admittedly only seen one PR come through on one of my repos which was sort-of spammish, but not spammy to the point where it was low-quality, it was just a low hanging fruit pull request.The spam and troubles for Hacktoberfest this year can be attributed to a YouTuber called CodeWithHarry who encouraged his 600k+ subscribers to spam repos to get a free shirt. While CodeWithHarry seems to have been the catalyst, Harry alone is not to blame for what is a flawed scheme.encouraged his 600k+ subscribers to spam repos to get a free shirtWhile there is a lot of blame being directed at DigitalOcean, they are not the supervillain here. Just because a few bad apples decided to try and game the system, doesn’t mean Hacktoberfest itself should be cancelled. Many have been incentivised to contribute and as a result, will most likely continue to contribute after Hacktoberfest is finished.I like Hacktoberfest from the perspective that it incentivises people to contribute to open source. For all of the spam it has caused, it still has resulted in some good. One of the popular ideas floated around was making Hacktoberfest opt-in and quite quickly, DigitalOcean has done that.Going forward from October 3, 2020, repositories with the topic hacktoberfest will only be counted. Pull requests will also need to be merged, approved by a maintainer, or labelled as ‘hacktoberfest-accepted’ in order to qualifyWell, it turns out DigitalOcean has responded to the controversy in this announcement post. Not only did they implement a favourable solution, but they did it quite quickly and also issued an apology. I don’t think there was ever any ill intent on their side, people are just letting emotion dictate their response.DigitalOcean has responded to the controversy in this announcement post",547
Globally Configuring The Aurelia Fetch Client In Aurelia 2,"If you’re like me, in Aurelia 1 you configured the Fetch client with some defaults, usually inside of your main bootstrap code (main.js/main.ts) where you might have added an authorization header, set up a default base URL and other configuration options you wanted to be global for the fetch client.While there are other ways you can approach configuring the fetch client globally, I wanted to approach it in the same way you would if you were creating an Aurelia 2 plugin. This approach will also allow us to test it easier (testing is beyond the scope of this post).To do this, we will take a middleware approach. Inside of your source directory, create a new folder called middleware and a new file called fetch-client.ts — I am using TypeScript, so change the applicable parts to work with Javascript.middlewarefetch-client.tsimport { HttpClient, IContainer } from 'aurelia';
import { StartTask } from '@aurelia/runtime';

export const FetchClientMiddleware = {
    register(container: IContainer): IContainer {
        container.register(StartTask.with(HttpClient).beforeCreate().call(FetchConfiguration));

        return container;
    }
};

const FetchConfiguration = (http: HttpClient): void => {
    http.configure(c => {
        c.useStandardConfiguration();
        c.withBaseUrl('http://localhost:3000/api/');
        c.withInterceptor({
            request: (request) => {
                const bearerToken = sessionStorage.getItem('_loginTokenString');
                
                if (bearerToken) {
                    request.headers.set('Authorization', `Bearer ${bearerToken}`);
                }
                return request;
            }
        })
      
        return c;
    });
};import { HttpClient, IContainer } from 'aurelia';
import { StartTask } from '@aurelia/runtime';

export const FetchClientMiddleware = {
    register(container: IContainer): IContainer {
        container.register(StartTask.with(HttpClient).beforeCreate().call(FetchConfiguration));

        return container;
    }
};

const FetchConfiguration = (http: HttpClient): void => {
    http.configure(c => {
        c.useStandardConfiguration();
        c.withBaseUrl('http://localhost:3000/api/');
        c.withInterceptor({
            request: (request) => {
                const bearerToken = sessionStorage.getItem('_loginTokenString');
                
                if (bearerToken) {
                    request.headers.set('Authorization', `Bearer ${bearerToken}`);
                }
                return request;
            }
        })
      
        return c;
    });
};Because this will be passed into the register method inside of our main.ts file, we can create an object and define a register method which the container itself will call.registermain.tsregisterYou might have noticed we are using a StartTask which will allow us to tell Aurelia to run our code at certain points in the lifecycle. We then pass the dependency we want to instantiate to the with method, specify we want to call it before create and then pass in a callback which gets the instance on the call method.StartTaskwithcallWhen using this approach, it is important to note that it should be the first thing passed to the register method inside of your main.ts file.registermain.tsimport Aurelia, { RouterConfiguration } from 'aurelia';
import { MyApp } from './my-app';

import { FetchClientMiddleware } from './middleware/fetch-client';

Aurelia
  .register(FetchClientMiddleware, RouterConfiguration.customize({ useUrlFragmentHash: false }))
  .app(MyApp)
  .start();
import Aurelia, { RouterConfiguration } from 'aurelia';
import { MyApp } from './my-app';

import { FetchClientMiddleware } from './middleware/fetch-client';

Aurelia
  .register(FetchClientMiddleware, RouterConfiguration.customize({ useUrlFragmentHash: false }))
  .app(MyApp)
  .start();
We have just created a Fetch Client middleware which will allow us to override the global settings for each instance of the fetch client that gets injected into our components and services.",1013
"Time To Get The Mop & Bucket Out, StackOverflow Needs a Good Clean","I love StackOverflow and it has significantly contributed to my journey as a developer over the years. As the years have gone by, StackOverflow has experienced tremendous growth. More often than not, when you Google a development-related problem, a StackOverflow question (or two) will appear on the first page. However, the quality of the site has slipped a little.Legendary programmer John Carmack said it best in 2013 about StackOverflow
@chx @jzy @StackExchange @codinghorror SO has probably added billions of dollars of value to the world in increased programmer productivity.— John Carmack (@ID_AA_Carmack) September 17, 2013
@chx @jzy @StackExchange @codinghorror SO has probably added billions of dollars of value to the world in increased programmer productivity.— John Carmack (@ID_AA_Carmack) September 17, 2013@chx @jzy @StackExchange @codinghorror SO has probably added billions of dollars of value to the world in increased programmer productivity.@chx@jzy@StackExchange@codinghorrorSeptember 17, 2013My use of StackOverflow is mostly limited to the context of web development, Javascript and CSS. However, SO given its age (almost 13 years old) understandably has a stale content issue.In regards to Javascript especially, it’s not uncommon to come across questions from the early days of the site with answers that reference jQuery (a sign of the time). Given the web was a different place back then, that is to be expected. However, it leaves behind a lot of mess.Jeff Atwood voiced his concerns back in 2018 about this and pointed out that StackOverflow is less question and answer, and more collaboratively evolving resource (aka a wiki).
I wish more people understood that the goal of Stack Overflow is not ""answer my question"" but ""let's collaboratively build an artifact that will benefit future coders"". Perhaps SO could be doing more to educate people about this.— Jeff Atwood (@codinghorror) April 30, 2018
I wish more people understood that the goal of Stack Overflow is not ""answer my question"" but ""let's collaboratively build an artifact that will benefit future coders"". Perhaps SO could be doing more to educate people about this.— Jeff Atwood (@codinghorror) April 30, 2018I wish more people understood that the goal of Stack Overflow is not ""answer my question"" but ""let's collaboratively build an artifact that will benefit future coders"". Perhaps SO could be doing more to educate people about this.April 30, 2018One of StackOverflow’s biggest issues is that it makes the contribution process intimidating. While the moderation aspect seems a little less aggressive than it was a few short years ago, people are put off contributing to the site beyond mostly looking for answers to their short-term problems.There is a disparity between how the community sees StackOverflow and how StackOverflow sees itself. Many developers would class it as a question/answer website, not a Wikipedia for programming. Therefore, a large majority most likely never think of editing an answer, instead, opting to leave a comment.I am guilty of this despite having been given a few perks for my accumulated karma, passively using Stackoverflow and sometimes upvoting useful answer or comments.One of SO’s biggest flaws (perhaps now more than ever) is the onus is put back onto the author of the question to choose the most useful answer and give it that big green tick of approval. Which in my opinion is a flaw, the OP is perhaps the last person who should be deciding if an answer is useful or not.If the intention of StackOverflow is to be an evolving resource, why have the useful answer check at all? Especially considering over time, the answer might become irrelevant and later edits might make it more up-to-date and useful, yet the original author will still be there alongside the editors who come almost secondary.An easy solution is when you come across a question with an outdated selected answer, add a more up-to-date answer and hope it gets updated. However, considering the OP asked the question years ago and has possibly moved on, the chances of them selecting a new better answer are quite slim. So, you edit the selected answer and leave the old answer for historical purposes. What happens in instances where several versions have been released and there have been changes to a handful of those? Do you keep on editing an answer and furthermore, how do you ensure the quality of the edits is the same as the original answer? It’s not a secret versioning and stale answers are a huge problem.As for the green tick, sure it allows the original question author to state the answer was satisfactory to them, but isn’t that at odds with how Jeff and StackOverflow see the site? It means the answer was useful to them, but the site isn’t meant to be about one person, it’s meant to be a wiki.I have seen many instances where the accepted answer didn’t even answer the question properly, while other answers which are clearly “community accepted” and have more upvotes remain unselected. The confusion stems from many believing the selected answer is in fact the best answer when it only matters to the original author.I still love SO and I still use it, although, I feel less compelled to contribute these days. There is an influx of low-quality content from beginners and students, and only so many moderators who have the time to manage it all.If StackOverflow truly sees itself as more of a wiki and less of a Q&A platform, it needs to remove meaningless features like selected answer and leave the community to upvote and downvote the right and wrong answers to the question.",1403
"No, SurfShark Cannot Spoof Your GPS Location In Pokemon Go on Android Phones","Recently while browsing Tomsguide, I came across an article claiming that using a VPN provider on Android phones, you can spoof your GPS location in Pokemon Go. Considering Pokemon Go is tightly controlled and most known techniques have not worked in years, I thought I would investigate.I still play Pokemon Go, but the thought of being able to use a VPN to spoof my location and potentially catch some location-specific Pokemon, it was too tempting to not try.The article in question is here, titled How to spoof your location for Pokémon GO on Android.How to spoof your location for Pokémon GO on AndroidThe crux of the article is that by installing the application, enabling developer settings and then choosing SurfShark as being able to mock your GPS location miraculously gets you into other countries. On my Galaxy Note 10+, it didn’t work.I received an error in the game, “12: Failed to Detect Location” — it doesn’t take a genius to tell you that using SurfShark clearly does not work. Fortunately, I only used the trial and cancelled as soon as I realised this would not work.It looks to me like Tomsguide is engaging in trickery to get people to click their SurfShark referral link. I manually looked up the app myself and didn’t click on their link. I won’t reward dishonesty.",322
"No Man’s Sky Releases Its Most Significant Update, Ever: Origins","The Hello Games team who develop the game No Man’s Sky just continue to impress me. Not only did they rescue a highly hyped game that launched to a tonne of negativity for the content it was lacking, to the point where it was nicknamed Refund Simulator, but they have continued to work on it over the years and bring it back from the brink.Since launch, we’ve reviewed three major updates, each of which brings significant new changes to the game in the form of content, mechanic improvements, as well as fixes. Update 3.0 titled Origins is perhaps the biggest and best update so far that the team have released.Update 3.0 titled OriginsThe update comes in at a respectable size of 1.4gb, which for any other game would be considered tiny, but a game like No Man’s Sky which primarily leans on procedural generation for its in-game UI, this is a big update.One of the loudest criticisms from gamers is the lack of biodiversity and landscapes in the game. After a few hours, No Man’s Sky becomes very much a samey game where all of the planets look the same as the previous ones. For a game touted as a space exploration game, that’s obviously a problem.In the Origins update, they’ve added new planets into every solar system. Instead of changing the existing planets, they’ve just added new ones in. I honestly would have loved to see the NMS team just reset every planet, but I can understand that some people might have become attached to their home planets and built bases.They have added in volcanoes that actually erupt. They have also added in firestorms, meteor strikes, tornadoes, lightning strikes, better clouds (that react to the current weather conditions) and pretty much made planets feel like independent living things with variable flora/fauna and biomes.The biggest and most buried part of this update (you have to scroll down a bit to get to it) is the addition of sandworms. Right at the end of the patch notes is where they drop this bombshell addition.If you have been following NMS since pre-launch, then you might remember the trailers had sandworms, then the Hello Games team decided they were not “fun” and removed them. Many were disappointed they didn’t make it into the game.One of the first changes you’re introduced to is the revamped menu screen. They have made it considerably more visual and nicer to look at in this latest update. Going through the menus, they feel so much more polished and the game even feels faster.Although existing planets remain unchanged, loading into my current home planet resulted in a firestorm that was in progress. Not only was the ground burning, but the UI went hazy to signify the intense heat around me. It was confronting, but exciting considering the game has been mostly static for a long time.Admittedly, it has been a while since I’ve played No Man’s Sky, but this latest update is enough to bring me back into the game, curious and eager to uncover what new additions have been added into the game and to find complex unique planets.",752
Why Is Slack Still So Terrible?,"If you work in tech as a designer or developer, there is a high chance the company you work for uses Slack as its internal communication tool of choice. In March 2020, Slack released an update it dubbed “significant” which cleaned up the interface and introduced a few new shortcuts.March 2020When Slack landed on the scene, it was hot (about as hot as Zoom is now during the pandemic). Workplaces adopted it en masse, but pretty quickly Slack went from hot new thing to just another enterprise chore application that ends up overwhelming us.It also turns out that despite the grandiose vision, Slack did not kill email.And before we continue, I want to point out that despite the flaws that Slack has, Microsoft Teams is infinitely worse and growing in the enterprise space (companies already using Office 365). Many of the issues that Slack has can probably be traced to its use of Electron, but the UX issues go a little beyond that.Distance makes the virtual grow strongerNow more than ever, workplaces that might not have had a need for a communication tool like Slack thanks to the COVID-19 pandemic are finding themselves needing one to help keep remote teams together. Naturally, Slack is the first choice because it is one of the most well-known platforms out there.Look, Slack isn’t a complete failure. What they have achieved as a company is inspiring, nobody could have foreseen Slack becoming as popular as it did, especially when people were hedging their bets on Facebook taking the mantle (and never did).A recurring distractionThere is no disputing Slack has turned into an all-day meeting with no agenda, that never ends. Even some of the fun features like lunch and Gif sharing bots becoming distractions that many companies outright quell after a short period of time.Admittedly, the novelty of Slack bots have also worn off for me.Sometimes the noise gets a little much for me and I close Slack with the intention of opening it up shortly after. However, I sometimes forget to reopen it which can result in people not being able to contact me. Even though I have email and receive email notifications, people have conditioned themselves to only communicate through Slack (even when it is important).Search is brokenOne of the biggest annoyances with Slack is search. Sure, the ability to ctrl + f inside of conversations is nice, but the search is still terrible and doesn’t work properly. The global search is perhaps the worst of all, I struggle to find conversations I know I have had and usually end up resorting to scrolling through the history until I get what I need.Given Slack is often used to make important changes, it’s amazing how buried things get in the constant flow of discussion. Just this month alone, I’ve had to wade knee-deep through Slack finding conversations about specific features or decisions.Comparatively, search in email is really good. Outlook, Gmail and other email services have incredibly powerful search features. I never struggle to find something in an email, especially with modern email even allowing you to search inside of attachments. Still, Slack struggles with basic text string searches, often showing my irrelevant messages from years ago.Confusing, bolted-on interface and noisy sidebarDespite the interface improvements they have made, Slack is still one of the most frustrating applications to use and look at. Threads still feel tacked on and they’re frustrating to use. The main sidebar menu is spaghetti; channels, files, saved items, threads, direct messages and more. It looks like the menu for the Cheesecake Factory over there.One of the easiest changes that Slack can and should make is removing the people from the sidebar and into a separate window. Given Slack is really no different than messaging apps that came before it like MSN Messenger or AIM. Existing conversations and channels are the two primary things I find myself using in the sidebar.Group conversations need to be reworkedAnother confusing aspect is group conversations. Sometimes I have conversations with mostly the same people, except maybe a team member or two less. Navigating group conversations is a nightmare, especially when the participants get truncated with an ellipsis.  Further adding to the annoyance is the fact names are alphabetically ordered.Say you had 3 people named John at your company, someone named Aaron, a couple of people named Steve and then a mixture of other names. Alphabetically, Aaron is always going to come first, followed by the other names.Imagine you have had a group conversation with Aaron, one of the Johns, Steve and Janet. Then you have another group conversation with Aaron, one of the Johns, Steve and Janet is not in this one. From a UI perspective, they’ll show up almost similar to one another in the sidebar, truncated by an ellipsis if they’re too long.The problem here is obvious, I have to click on each conversation and see which one has the people I want to talk about. Let’s say I have information that Janet isn’t allowed to know, I accidentally click on the group chat and type in there, not realising I clicked the wrong one. I can delete the message, but Janet has already seen it.The simple fix to this solution is to provide the ability to label a group conversation. It’s a painfully obvious feature, the ability to name a group conversation. I realise that Slack has the ability to group chats and channels into groups now, but the ability to rename a group conversation just seems like the easiest solution.Notifications: noise. noise. noise. noise.And then you have the notifications… Ask anyone to name one thing they find annoying about Slack and they will probably tell you it’s the notifications. Despite being buggy for me from time-to-time (not clearing when I view a conversation/channel), it just feels like a constant barrage of noise, with Slack not providing adequate control over them.Depending on what I am working on, sometimes I need complete focus. And it’s a balancing act, working remotely I need to be available for emergencies and important things, but not too available that Gary sharing pictures of his weekend getaway are bothering me or our deployment bot which notifies when a build has failed or succeeded.You do get control over notifications, but it’s not as fine-grained as a tool built for business communication should be. Instead of just making me set myself as away or busy, give me the ability to set up “do not disturb” time periods where I can focus, aggregate my notifications into a panel of some kind and allow me to catch up on the import things later on.Multiple Sign-in Is Painful If you’re like me, you are possibly a member of more than one Slack organisation. The frustrating thing about having to join a new organisation is the need to create a new login, having to generate yet another password, enter your details again and tell Slack you don’t want the tour.Why can I just not have a single sign-on (SSO), one account and then join the organisations I am a part of? Why do I need to fill out an email, create a new password and enter my information repeatedly?The best example of single sign-on is Discord. I think Slack needs to really take a look at Discord, study what they have done and then go back to the drawing board. The way Discord handles multiple servers is also really nice and it is effortless to manage/organise them. Discord is ironically a tool built for the gaming community, but in many ways, it’s a lot better than Slack at basic communication and organisation features.In SummaryWhile Slack is not the worse option out there, it still surprises me that in 2020 now that Slack is worth billions that it still has some of these issues. The lack of single sign-on is probably one of the biggest omissions from the popular communication platform. Understandably, they have reached a point where they can’t move as fast as they could when they were still considered a startup.It does make me wonder, will Slack always be the top option or are we witnessing a MySpace 1.0 situation and a competitor that isn’t Microsoft Teams is just waiting around the corner to take the crown?",2039
What’s Going on With CSS Houdini?,"It has been years since CSS Houdini was revealed in 2016 and admittedly, it has been very slow going. Although, since the W3C task force was announced there have been a few things delivered. was revealed in 2016there have been a few things deliveredIf you’re not aware of CSS Houdini, it is a set of proposals and standards that allow access to lower-level aspects of the layout and CSS parser in web browser engines. It is basically an umbrella term used to describe a tonne of different API’s, some of which have already shipped and you might be familiar with.As someone who has been developing since Internet Explorer 5.5, I still remember what the web used to be like. We once upon a time in IE6 didn’t have support for transparent PNG’s, custom web fonts required using solutions such as SIFR which relied on Flash and JS, rounded corners with CSS were also not possible, requiring the use of carefully cut out transparent GIF’s.SIFRSince then, we have come a very long way. We can now do shadows, gradients, filter effects, custom fonts, rounded corners and advanced layouts all inside of CSS. It’s easy to forget how far we have come and to see where CSS is heading, CSS is not finished yet.Surprisingly, Firefox has been dragging the chain on implementation of numerous Houdini features. Mozilla has marked the Layout API as worth prototyping, the same thing for the Paint API, Properties & Values API and finally Typed OM. As for Safari, they’ve gone a little further than Mozilla (which is really surprising).Layout APIPaint APIProperties & ValuesTyped OMUnderstandably, CSS Houdini is ambitious and it has a lot of moving parts. Browser vendors and standards participants are very particular about the ways in which these API’s are implemented and exposed. There are numerous ideas in the issues section on the GitHub repository.the issues sectionThe last big piece of the puzzle before CSS Houdini goes to the next level is the Parser API. When you read up on what this API can do, you’ll understand why it hasn’t been implemented in any browser yet, it’s still in proposal form at the moment.Parser APIit’s still in proposal form at the moment",539
4 Somewhat Unknown CSS Layout Properties,"CSS just continues to grow and evolve, much like its cousin Javascript. I thought I would share a few CSS layout properties that are still relatively unknown at the moment with developers. The first 3 are variations of almost the same thing, with the fourth being the most obscure and unsupported of them all.place-itemsThis one blows a lot of the minds of developers I’ve shown this too or told about. The place-items property is shorthand for align-items and justify-items CSS properties.place-itemsalign-itemsalign-itemsjustify-itemsjustify-itemsIf you wanted a completely centered layout (on both the X and Y axis) you can just use place-items: center.place-items: center.layout {
    display: flex;
    place-items: center;
}.layout {
    display: flex;
    place-items: center;
}place-contentSimilarly to the place-items shorthand property, the place-content property is shorthand for  align-content and justify-content CSS properties.place-itemsplace-contentalign-contentalign-contentjustify-contentjustify-content.layout {
    display: flex;
    place-content: center;
}.layout {
    display: flex;
    place-content: center;
}place-selfAnother shorthand property, place-self is shorthand for the align-self and justify-self CSS properties.place-selfalign-selfalign-selfjustify-selfjustify-self.layout {
    display: flex;
}

.layout .item {
    place-self: center start;
}.layout {
    display: flex;
}

.layout .item {
    place-self: center start;
}aspect-ratioOne of the most exciting CSS properties which is not a shorthand property is aspect-ratio which allows you to control the calculated size of an element. The obvious use-cases here are video and images, but really, you can use this property for anything you want.aspect-ratioIn the following example, we create an element which only has a width specified of 250px, but using the aspect-ratio property will give us a perfectly square box of 250px both height and width.aspect-ratio.square {
  width: 250px;
  height: auto;
  aspect-ratio: 1/1;
}.square {
  width: 250px;
  height: auto;
  aspect-ratio: 1/1;
}Now, say you have a video and you want to maintain a 16/9 aspect ratio without resorting to Javascript or CSS padding hacks, you can just do this.video {
  width: 100%;
  height: auto;
  aspect-ratio: 16/9;
}video {
  width: 100%;
  height: auto;
  aspect-ratio: 16/9;
}Now, at the time of writing this, support for this is interesting. Many browsers support it in some capacity but have shipped support for internal mapping use only. Of all of the properties listed, this is the newest and most experimental. So, this property while cool is not directly useable yet. It is worth remembering we still have object-fit which is well-supported.interestingMany browsers support itobject-fit",691
How To Easily Convert A Javascript Array Into a Comma Separated String,"Some of you probably already knew this, but I only recently discovered this (accidentally) while building a wrapper around Postgresql queries in Node, and my mind is blown, to be honest.Calling toString() on an array will automatically return the values within as a comma-separated string. Best of all, there will be no hanging comma and all whitespace is removed. Implementations based on join will have a space between the items, toString() on the array itself will not.jointoString()I have always used .join(', ') to create comma-separated strings from array values, but this is so much nicer..join(', ')const arr = [ 'username', 'email', 'firstName', 'lastName', 'password' ];
arr.toString();const arr = [ 'username', 'email', 'firstName', 'lastName', 'password' ];
arr.toString();There are a million ways you can comma separate an array of values into a string, you can use reduce, a for loop, the classic join method and probably a few others not mentioned. Why overcomplicate?reduceforjoin",249
How To Paginate An Array In Javascript,"I was recently tasked with implementing pagination for an array of values (in some instances, over 2400 of them) and instead of using an existing library, I opted to write my own pagination logic and put it into an Aurelia value converter.There are a lot of ways you can achieve this, you can use slice, you can use filter and reduce as well. The solution I ended up coming up with uses slice because it produces the easiest to read and simple code. Performance-wise, I do believe that filter comes out on top, but that isn’t our concern here.slicefilterreducefilterconst paginate = (items, page = 1, perPage = 10) => items.slice(perPage * (page - 1), perPage * page);const paginate = (items, page = 1, perPage = 10) => items.slice(perPage * (page - 1), perPage * page);However, if you want something a little more descriptive, you can get a little more verbose and produce something that while not nearly as concise, gives you some more details.In my instance, I wanted to return the next and previous pages, as well as the total number of pages and items for each page that is being paginated. While you get a more verbose method, it becomes a lot more useful.const paginate = (items, page = 1, perPage = 10) => {
    const offset = perPage * (page - 1);
    const totalPages = Math.ceil(items.length / perPage);
    const paginatedItems = items.slice(offset, perPage * page);
  
    return {
        previousPage: page - 1 ? page - 1 : null,
        nextPage: (totalPages > page) ? page + 1 : null,
        total: items.length,
        totalPages: totalPages,
        items: paginatedItems
    };
};const paginate = (items, page = 1, perPage = 10) => {
    const offset = perPage * (page - 1);
    const totalPages = Math.ceil(items.length / perPage);
    const paginatedItems = items.slice(offset, perPage * page);
  
    return {
        previousPage: page - 1 ? page - 1 : null,
        nextPage: (totalPages > page) ? page + 1 : null,
        total: items.length,
        totalPages: totalPages,
        items: paginatedItems
    };
};If you’re using Aurelia, here is a value converter which will give you array pagination that can be used on a repeat.for or anywhere else you’re display items from an array. If you’re not using Aurelia, you can stop reading here, the above is what you’re after.repeat.forexport class PaginateValueConverter {
    toView(array, page = 1, perPage = 10) {
        if (!array) {
            return array;
        }

        const paginationObject = paginate(array, page, perPage);

        return paginationObject.items;
    }
}export class PaginateValueConverter {
    toView(array, page = 1, perPage = 10) {
        if (!array) {
            return array;
        }

        const paginationObject = paginate(array, page, perPage);

        return paginationObject.items;
    }
}You can then use this value converter in your Aurelia applications like this:<ul>
  <li repeat.for=""item of items | paginate"">${item.name}</li>
</ul><ul>
  <li repeat.for=""item of items | paginate"">${item.name}</li>
</ul>You would probably have a currentPage variable in your view-model, where you increment the value if a button or link is pressed, maybe a row of numbers.currentPage<ul>
  <li repeat.for=""item of items | paginate:currentPage"">${item.name}</li>
</ul><ul>
  <li repeat.for=""item of items | paginate:currentPage"">${item.name}</li>
</ul>",843
How To Filter Your Pokemon In Pokemon Go By Appraisal I.V Rating,"Yes, I still play Pokemon Go and as the years have gone by, the game has only gotten better. A somewhat recent discovery for me is the fact the in-game search supports search strings. You can type in specific values into the search bar to filter Pokemon by region, whether they can be mega evolved and more specifically, their appraisal rating.You know when you get your Pokemon appraised and you coloured bars, with 3 stars and filled bars meaning a 100% I.V rated Pokemon? Appraising one-by-one is frustrating and time-consuming, which is why I am delighted to discover that you can filter your Pokemon based on their appraisal rating.As you can see, I only have 6 100% Pokemon.My most recent acquisition from the Porygon Community Day is a Porygon-Z which is a 3 star 100% I.V rated Pokemon. Honestly, it was luck, but what a fantastic Pokemon and with a third added move, it’s pretty darn good in PVP as well.By typing a value between 0 and 4 followed by an asterisk, you can see your Pokemon by their rating. Have you ever wanted to just filter your Pokemon by their lowest rating and then bulk select followed by transfer to free up space instead of going through each one and appraising?The ratings appear to mostly equate to star ratings, with 0 being 0 stars, 1 being one star, 2 being two stars, 3 being three stars and 4 being 3 stars full Pokemon.As I discovered, I had a lot of Pokemon (ignoring the shiny ones) that were wasting space in my Pokedex that I could remove in bulk. I was able to free up considerable amounts of space by typing 0* into the search field.There are a lot more ways you can filter and query your Pokemon and surprisingly, a lot of Pokemon Go players don’t even know they exist. You can even combine them to query Pokemon from a specific region with a particular move and so on. You can find a better list over here on the Niantic site.over here on the Niantic site",476
Firebase Has Just Released a New Firestore Feature Which Will Save Firebase Users a Lot of Money,"The Firebase team has just released a new update for the Javascript SDK, version 7.21.0 in which a new feature was just added which will save anyone who uses Firestore possibly a lot of money.In the update, the feature introduces two new where compatible query operators for filtering out data: not-in and !=.In the updatewherenot-in!=not-in finds documents where a specified field’s value is not in a specified array.!= finds documents where a specified field’s value does not equal the specified value. Neither query operator will match documents where the specified field is not present.not-in finds documents where a specified field’s value is not in a specified array.not-in!= finds documents where a specified field’s value does not equal the specified value. Neither query operator will match documents where the specified field is not present.!=Filtering in Firestore and Realtime Database has been notoriously limiting. You have to either query exactly what you want using where (say you want to get all items that have an active status). The ability to perform NOT IN clauses inside of traditional databases has been a staple for years, but notably absent in Firebase.NOT INAs you can see in this StackOverflow question from 2014, it’s a feature that people have been wanting in Firebase for years. This is somewhat of a huge feature for Firestore users, one which can potentially save a lot of money and make your reads lighter.this StackOverflow question from 2014",369
Someone Is Selling a Poorly Drawn Picture of The NVIDIA RTX 3080,"This is actually brilliant. While looking to see if anyone reasonable had an NVIDIA RTX 3080 card on eBay at a reasonable price (nobody did), I came across this drawing that someone is selling on eBay and people are actually bidding on it. this drawingSadly, this is probably the closest that anyone is going to get to possessing an RTX 3080 for the next few months. It’s clear that it was mostly a paper launch for Nvidia to get press coverage and the scarce availability helps build more hype and result in more sales.Perhaps, even dodgier and less obvious is someone else selling a “PAPER EDITION” of the RTX 3080.someone else sellingThe listing kind of makes it seem ambiguous here. The image they have used looks like a real graphics card and people who don’t carefully read the descriptions could easily be fooled. I am pretty sure the bids are all fake on this and it’s more of a joke, but if it’s not, RIP.",229
How to Easily Add Bootstrap 5 Into an Aurelia 2 Application,"The Bootstrap 5 alpha was announced a few months ago and I instantly jumped on it and started using it. The lack of jQuery and more robust colour palettes as well as grid system were too hard to pass up.If you are reading this in the future and Bootstrap 5 has already been released, these instructions will still work for you. Admittedly, the approach I am taking here avoids the need for any package manager like Npm or Yarn, I’m going to show you a simple way.The easiest way is using the CDN version of Bootstrap 5 and adding it into the index.ejs file in your root directory. This approach also works for Aurelia v1 as well and avoids the need of bundling it, gives you browser caching and other benefits.index.ejsIf you generated your Aurelia 2 application using the CLI, all you need to do is grab both the CSS and Javascript files from the Bootstrap v5 site here and add them into the head.Bootstrap v5 site herehead<!DOCTYPE html>
<html>
    <head>
        <meta charset=""utf-8"" />
        <title>Aurelia</title>
        <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />

        <link
            rel=""stylesheet""
            href=""https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/css/bootstrap.min.css""
            integrity=""sha384-r4NyP46KrjDleawBgD5tp8Y7UzmLA05oM1iAEQ17CSuDqnUK2+k9luXQOfXJCJ4I""
            crossorigin=""anonymous""
        />

        <script
            src=""https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js""
            integrity=""sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo""
            crossorigin=""anonymous""
        ></script>
        <script
            src=""https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/js/bootstrap.min.js""
            integrity=""sha384-oesi62hOLfzrys4LxRF63OJCXdXDipiYWBnvTl9Y9/TRlw5xlKIEHpNyvvDShgf/""
            crossorigin=""anonymous""
        ></script>
    </head>

    <body>
        <my-app></my-app>
    </body>
</html><!DOCTYPE html>
<html>
    <head>
        <meta charset=""utf-8"" />
        <title>Aurelia</title>
        <meta name=""viewport"" content=""width=device-width, initial-scale=1"" />

        <link
            rel=""stylesheet""
            href=""https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/css/bootstrap.min.css""
            integrity=""sha384-r4NyP46KrjDleawBgD5tp8Y7UzmLA05oM1iAEQ17CSuDqnUK2+k9luXQOfXJCJ4I""
            crossorigin=""anonymous""
        />

        <script
            src=""https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js""
            integrity=""sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo""
            crossorigin=""anonymous""
        ></script>
        <script
            src=""https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/js/bootstrap.min.js""
            integrity=""sha384-oesi62hOLfzrys4LxRF63OJCXdXDipiYWBnvTl9Y9/TRlw5xlKIEHpNyvvDShgf/""
            crossorigin=""anonymous""
        ></script>
    </head>

    <body>
        <my-app></my-app>
    </body>
</html>This is what my index.ejs file looks like in my Aurelia 2 application. Now, make sure you get the latest files from the Bootstrap site as the versions change (the above is referencing alpha1) and the integrity hashes also change as well.index.ejsWhile you can also install using Npm/Yarn and import it that way, Bootstrap will conflict with Shadow DOM as well as CSS Modules (if you use any of those options) so adding them into the index.ejs file avoids this issue entirely and globalises your CSS.index.ejs",883
Vue 3 is Finally Released,"After an endless stream of betas and release candidates, Vue 3 is finally here. Vue 3 is finally hereCongratulations to the Vue team and all of the contributors who made the release happen. As many of you know, I’m on the Aurelia core team and I’ve seen how hard it is shipping a new major release. Aurelia 2 itself has been in development for quite a while now.

I haven’t made my criticisms of React a secret and I am glad that there are frameworks and libraries still around challenging React/keeping it honest, even if it is the king of the hill at present. In Vue 3, there are new composition features, support for TypeScript (finally) and other features which are more React-esque.Largely, it appears migrating from Vue 2 to Vue 3 is a breeze, I can’t really see any overly big breaking changes that would hinder migration to the new shiny version for users, however, plugin maintainers appear to have more work cut out for them.it appears migrating from Vue 2 to Vue 3 is a breezeIt’s a mammoth effort and I plan on having a play with Vue 3 shortly to try out the new features that it brings.",275
Can The Nvidia RTX 3080 Run Microsoft Flight Simulator 2020 at 4k 60 fps?,"Goodbye, “Can it run Crysis” and hello, “Can it run Microsoft Flight Simulator 2020” — it’s not a secret that MSFS 2020 is an intensive game that seemingly has been unable to run smoothly on high-end hardware since its late August 2020 release.The game released and while it ran okay, it wasn’t smooth at higher settings and resolutions, the Nvidia 2080 ti owners were dismayed their powerhouse card couldn’t achieve what we now know is impossible.As the RTX 3080 benchmarks and reviews roll in, some of the reviewers are throwing MSFS 2020 at the 3080 to see how it handles. The Guru3d benchmarks are some of the first that I saw.Guru3d benchmarksNow, the interesting thing about the RTX 3080, is that for most of the benchmarked resolutions, the frame rates were essentially the same as the RTX 2080 Ti. For 1920×1080, the 3080 achieves 50fps and the 2080 Ti achieves the same. Similarly, for the 2560×1440,  the 3080 achieves 48fps and the 2080 Ti achieves 47 fps.It is not until we get into ultrawide territory where we see the 3080 pull ahead. For 3840×2160, we see the 3080 achieve 42 fps and the 2080 Ti achieve 35 fps. A noticeable difference, but not as dramatic as the hype surrounding the 3080 would have led you to believe.There are some things that stand out for the 3080, looking at other benchmarks and games. The RTX 3080 is only worth upgrading too if you own a much older card (ie, not a 2080 or 2080 Ti) and you’re playing at a 4k (or more) resolution.YouTuber JayZTwoCents also did some benchmarking of his own and the results are somewhat similar.In this comparison video, we see some of the problems with MSFS 2020 are highlighted.

One thing that is quite clear, if you own the 2080 (non-Ti variant) or something older, the 3080 is a worthy upgrade based on price and performance alone. But, if you already own the 2080 Ti, it is clear that you’re not going to gain much from upgrading and you should wait (not that you can get a card at the moment anyway).Let’s be honest, most of us are still gaming at 1080p, 4k gaming is still in its infancy, even though Nvidia seems to be thinking forward to 8k gaming (evident by those 3090 benchmarks) it’ll be a few more years before consumer 8k gaming is a thing anyway.One of the most known issues with Microsoft Flight Simulator 2020 is that it is CPU bottlenecked. Those of you who have more than four cores have probably already noticed that this game won’t use more than four of them. Furthermore, only supporting DirectX 11, means that the game cannot take advantage of new DirectX 12 features which could help for multi-core processors. Although, the jury is out on whether or not DirectX 12 would fix MSFS 2020 issues or not.While the latest patch #2 certainly helped fix some of the performance issues, it didn’t exactly solve the bottlenecking problems that plague this game and honestly, I think it’ll be a few more patches and possibly upwards of a year before they can solve them. MSFS 2020 was released probably 6-8 months too early (evident by some of the things fixed in patch #2).If you’re thinking of buying a 3080 to play MSFS 2020 at ultra 4k, you should wait. I do believe this card is capable of 60fps on ultra in the game evident by the frame rates in other benchmarks for 4k games, but the optimisations and Dx12 support will be needed to achieve that.",836
Mozilla Turbulence,"The last few months of Mozilla news hasn’t all been exactly positive. The once beloved company is fighting to keep its head above water as well as retain its open and anti-corporate identity.You either die a hero, or you live long enough to see yourself become the villainYou either die a hero, or you live long enough to see yourself become the villainJust over a month ago, Mozilla laid off 250 employees. Understandably, 2020 has been a challenging time for many companies. However, the layoffs were not due to the pandemic or any other reason, the poor executive leadership at Mozilla is to blame.Mozilla laid off 250 employeesThe CEO of Mozilla Corporation Mitchell Baker continues to be compensated quite substantially, despite market share and dominance dropping. Over the last 10 or so years, Mozilla has been a mishandled and misguided mess (remember FirefoxOS).
Firefox market share vs Mozilla Foundation chair salary (2.5 million/year in 2018) pic.twitter.com/htEKDNFMDH— Saoirse Shipwreckt (@withoutboats) January 15, 2020
Firefox market share vs Mozilla Foundation chair salary (2.5 million/year in 2018) pic.twitter.com/htEKDNFMDH— Saoirse Shipwreckt (@withoutboats) January 15, 2020Firefox market share vs Mozilla Foundation chair salary (2.5 million/year in 2018) pic.twitter.com/htEKDNFMDHpic.twitter.com/htEKDNFMDHJanuary 15, 2020Now, Mozilla has announced Firefox Send and Firefox Notes are being binned.  Look, I get they are trying to diversify and move away from their reliance on Google search agreement money (which constitutes a large majority of Mozilla’s funding), but it looks like Mozilla is becoming a VPN company and less about being a browser and resource for the development community.has announced Firefox Send and Firefox Notes are being binned.One thing we do have to acknowledge is Mozilla is held to quite a high, some would say, impossible standard from the community. Mozilla champions itself as the open-source warrior for privacy and safety, against advertisements and trackers. So, when the topic of Mozilla wanting to make money pops up, the community gets a little cagey (especially the Hacker News crowd).People seem to forget that Mozilla is very reliant on Google, a company that goes against a majority of things that Mozilla stands for. But, there is no alternative. It is an interesting situation, given another not-for profit WikiMedia which runs the Wikipedia site, seems to always manage to cover its costs (admittedly, a lot less overheads). Why are people more willing to support Wikipedia than Mozilla?Many agree that Mozilla is important and that it needs to survive. However, you’re probably reading this on Google Chrome (hey, it’s okay, I am using Chrome too) and you’ve probably never donated to Mozilla.Now, the inexcusably high salary of Mozilla Corp Mitchell Baker is worth reiterating. For a company trying to find new revenue streams and survive in the face of Chrome’s dominance, $2.5 million US dollars is way too high. I think a good CEO is worth being compensated and Baker is arguably earning above her value.Mozilla is a company worth saving, but first it has to save itself.",787
Joe Rogan Is Already Dividing Spotify Internally,"The writing has been on the wall with this one since Joe Rogan announced his $100 million-dollar multi-year Spotify exclusive deal in May 2020. The Joe Rogan catalogue if you look back far enough is all but certain to have some content that people would find offensive.Some staff are internally are fighting against content in Joe’s back catalogue which they deem to be transphobic. There is a particular episode where Joe interviews an author who wrote a book comparing transitioning to eating disorders and self-harm, as well as comments made by Joe himself which many have deemed offensive.internally are fighting against content in Joe’s back catalogue which they deem to be transphobicIt isn’t a secret that Rogan interviews some divisive and many would say, hateful guests with opinions and perspectives that go outside the norms of personal opinion. Perhaps most notably, is Spotify removing episodes featuring Alex Jones, despite them being the most popular on YouTube.While Spotify have said they reviewed the episode in question and have deemed it doesn’t violate their content guidelines, it seems many internally are not happy. I wouldn’t be surprised if you see protesting or an exodus from Spotify over the decision.Ironically, when Joe Rogan announced the move back in May he said, “Spotify want me to just continue doing it the way I’m doing it right now. It’s just a licensing deal, so Spotify won’t have any creative control over the show.” – which seems funny given that quite a few episodes are missing from Spotify.The thing is, this is not going to be the first or last incident in which Joe Rogan causes controversy not only with numerous communities and people, but internally with Spotify staff. We will truly see just how much creative control Spotify are willing to give Rogan, especially if his comments about trans people keep being negative.Don’t be surprised if down the track in a few months, a year or whatever, you hear of Joe Rogan fighting to get out of his contract with Spotify because of the stranglehold they have and content policies. I don’t think it’s a matter of if, but when.",530
The Nvidia RTX 3080 Release Is a Botched Mess,"Well, the day has finally come and gone. The Nvidia RTX 3080 graphics card has gone on sale and all but sold out at every retailer in a matter of minutes. The limited quantities of the card, have created a frenzy.Firstly, if you thought about getting one of these new cards, you’re going to have to wait a long time.
Those wanting RTX 3080 GPUs, here's some info:This morning we experienced more traffic than the morning of Black FridayLimited inventory sold out in 5 minsWe'll release more as we get moreBot protection was in place, orders were humanTurn on Auto Notify & check back— Newegg (@Newegg) September 17, 2020
Those wanting RTX 3080 GPUs, here's some info:This morning we experienced more traffic than the morning of Black FridayLimited inventory sold out in 5 minsWe'll release more as we get moreBot protection was in place, orders were humanTurn on Auto Notify & check back— Newegg (@Newegg) September 17, 2020Those wanting RTX 3080 GPUs, here's some info:This morning we experienced more traffic than the morning of Black FridayLimited inventory sold out in 5 minsWe'll release more as we get moreBot protection was in place, orders were humanTurn on Auto Notify & check backSeptember 17, 2020Newegg Tweeted they received more traffic than the morning of Black Friday, which is insane for a graphics card. This is a launch for a new graphics card which is a generational leap and was hyped intensely before its release.Allegedly, some retailers only got as many as 10 cards, which is obviously not enough. The hype surrounding this card in the lead up to launch was an indication a lot of gamers were going to buy one. Understandably, people are really pissed off.Check out the replies to Nvidia’s celebratory Tweet about the RTX 3080
The GeForce RTX 3080 is out, and the rave reviews are in!  Learn more about the “staggering performance upgrade” (PCWorld) and more: https://t.co/BilhUvJNuf pic.twitter.com/KD9nxsTLqo— NVIDIA GeForce (@NVIDIAGeForce) September 17, 2020
The GeForce RTX 3080 is out, and the rave reviews are in!  Learn more about the “staggering performance upgrade” (PCWorld) and more: https://t.co/BilhUvJNuf pic.twitter.com/KD9nxsTLqo— NVIDIA GeForce (@NVIDIAGeForce) September 17, 2020The GeForce RTX 3080 is out, and the rave reviews are in!  Learn more about the “staggering performance upgrade” (PCWorld) and more: https://t.co/BilhUvJNuf pic.twitter.com/KD9nxsTLqohttps://t.co/BilhUvJNufpic.twitter.com/KD9nxsTLqoSeptember 17, 2020It turns out for every one who did not get a card, some scalpers did and the eBay listings are beginning to pour in. It’s clear that maybe bots were responsible for some of the purchases, I know some retailers held lotteries to get your chance to buy a card, but many did not.To put things into perspective, $2500 AUD is about $1800 USD, more than double the US price. The RTX 3080 itself bought through a proper retailer at the MSRP is $1139. Sellers are basically seller these cards more than double the retail price.I did also come across what appears to be a fraudulent listing for $10,000 AUD which I reported to eBay.The bidding activity for the above listing pretty much proves its illegal bidding activity, possibly a scam of some kind.It will be interesting to see what happens when the RTX 3090 and RTX 3070 go on sale. I cannot foresee the 3090 selling out too quickly given its expensive price tag and appeal towards more non-gaming users, but the RTX 3070 will undoubtedly cause another frenzy.I do wonder why the quantities are so low, maybe the new Samsung 7nm process has yet to provide high yields for these cards, although, the lower price would suggest that yields are maybe not the problem and perhaps it’s just the ongoing pandemic slowing things down (manufacturing and shipping). Which seems likely given Sony is experiencing the same issues with its PS5 (reportedly).After the incredible amounts of backlash, Nvidia has responded and all but confirmed bots and scalpers bought a majority of the cards in an official statement.in an official statementThis morning we saw unprecedented demand for the GeForce RTX 3080 at global retailers, including the NVIDIA online store. At 6 a.m. pacific we attempted to push the NVIDIA store live. Despite preparation, the NVIDIA store was inundated with traffic and encountered an error. We were able to resolve the issues and sales began registering normally.To stop bots and scalpers on the NVIDIA store, we’re doing everything humanly possible, including manually reviewing orders, to get these cards in the hands of legitimate customers.Over 50 major global retailers had inventory at 6 a.m. pacific. Our NVIDIA team and partners are shipping more RTX 3080 cards every day to retailers.We apologize to our customers for this morning’s experience.This morning we saw unprecedented demand for the GeForce RTX 3080 at global retailers, including the NVIDIA online store. At 6 a.m. pacific we attempted to push the NVIDIA store live. Despite preparation, the NVIDIA store was inundated with traffic and encountered an error. We were able to resolve the issues and sales began registering normally.To stop bots and scalpers on the NVIDIA store, we’re doing everything humanly possible, including manually reviewing orders, to get these cards in the hands of legitimate customers.Over 50 major global retailers had inventory at 6 a.m. pacific. Our NVIDIA team and partners are shipping more RTX 3080 cards every day to retailers.We apologize to our customers for this morning’s experience.I kind of accepted that I wouldn’t be able to get one of these 3080 cards anyway, so I didn’t even try ordering one or wasting my time refresher retailer websites trying to get one. I’ll try again in a month or so and see how I go then. Or maybe, AMD will handle their launch better and I’ll get a Big Navi card instead.",1460
How To Horizontally and Vertically Center DIV’s With CSS,"Back in the early 2000’s when the modern web was still in its infancy and jQuery reigned supreme, many of the things we can do now did not exist and required sometimes obscene hacks and solutions to get working. Believe it or not, centring a div used to be a complicated task.Fortunately, in 2020 with the advent of modern CSS, we have multiple ways to centre a DIV (or any content) with just a few lines of CSS. Using Flexbox affords us the ability to centre things both horizontally and vertically (even both at the same time).Horizontally Centre ItemsTo horizontally centre a DIV, we will justify the content using justify-content: center; which will centre the elements on the x-axis (horizontally).justify-content: center;

Vertically Centre ItemsTo vertically center items on the y-axis, we can use the align-items: center; property to vertically align our items. Pay attention to the CSS here, as the body is our parent container, we ensure that it extends a height of 100% and then make the section tag 100% in height as well.align-items: center;bodysection

Horizontally & Vertically Centre ItemsSometimes you want to perfectly centre items both horizontally and vertically along both the x and y-axis. Doing this used to require using position: fixed; and position: absolute in combination with negative margins and transforms to achieve the same result, now it’s just a few lines of CSS.position: fixed;position: absolute

It absolutely boggles my mind how easy this stuff is now. You no longer need to use Javascript, negative margins or transformations with weird side-effects. Through the power of CSS, you can do almost anything layout wise now.",415
Microsoft Flight Simulator 2020 Releases a Huge New Patch With Noticeable Performance Fixes,"Thank you, Asobo. Since the launch of MSFS 2020, the game has been plagued with some teething issues. Many of which have centred around excessive CPU usage, bottlenecking graphics cards and poor frame rates (especially in cities). I have been playing the game and it’s largely a success, but there have been some issues.We all knew they would eventually fix them and in patch 1.8.3.0, we finally have one of the first meaningful patches that take a step towards fixing the issues. The patch download initially in the Microsoft Store is 676mb, but in the game, you’ll see it’s an additional 13.4gb.patch 1.8.3.0Performance improvements include:Performance improvements include:Tweaked CPU thread priorities to reduce interruption of frame critical threads.Optimized loading system to reduce overall loading times.Optimized heavy airport scenes impact on CPU.Reduced the amount of GPU overdraw to improve GPU performance.Memory optimizations to reduce software memory footprint and improve performance on memory limited computers.Overall performance optimizations.Tweaked CPU thread priorities to reduce interruption of frame critical threads.Optimized loading system to reduce overall loading times.Optimized heavy airport scenes impact on CPU.Reduced the amount of GPU overdraw to improve GPU performance.Memory optimizations to reduce software memory footprint and improve performance on memory limited computers.Overall performance optimizations.And one of the most requested fixes is the wind bug fixed in, “Fixed 225°/3kt issue with the inconsistent wind in-game.” — the patch itself carries a lot of other updates from visuals, cockpit fixes for various hardware and a lot of fixes for avionics in the planes.One thing that immediately stood out for me is even before downloading the additional 13gb patch assets, the game itself already felt faster. The patch installation process also appears to be a lot smoother.As you can see, CPU usage through the installer has come down quite a lot. When I originally went through the installation process, my CPU was being bashed into the wall by the installer. However, the installer appears to still be hammering the GPU quite substantially, which is surprising given it’s a static image in the background of the installer.The real proof in the pudding is how the game handles after installing the 1.8.3.0 patch. Is there a noticeable performance improvement or was it mostly just aesthetic changes?But, before we fly, just navigating the menus feels really nice. For settings, I’ve chosen high-end which for my hardware the RTX 2060 and Ryzen 3900x, isn’t the best setting, but let’s roll with it anyway and see what happens on my widescreen monitor.For my first post-patch flight, I decided to take off from Heathrow, London, UK.The loading times are definitely a lot quicker in comparison to what they were. I don’t have any detailed statistics on how much faster, but speed is partly perception and I perceive a much larger increase. The reason for choosing Heathrow is that it one of the more taxing airports in MSFS 2020 from the airport itself to the scenery of the surrounding area (London is quite dense).The frame rate whilst taxied at the airport is quite dysmal, but I am running on the high-end setting on a relatively entry-level RTX card. If this were a 2080ti, I would imagine I would be getting substantially more frames then I am here.But, here is an interesting thing I noticed. I wasn’t getting 30fps, but the game didn’t feel stuttery or laggy, it felt smoother. Now, I’ve attempted this route before prior to the patch and it was definitely laggy. It appears that Asobo have optimised the game for instances where the frames dip below 30fps.However, once I lifted off the runway, I was constantly staying above 30 frames per second. And 30 frames is more than enough for a flight simulator game.I know the frame rate purists out there will roll their eyes seeing these frame rates, but once again, remember my GPU is anything but high-end or current-gen. These frames on my RTX 2060 rendering at 3400 x 1440 resolution are impressive for me.Looking in the resource panel, it appears once again, MSFS 2020 isn’t overly CPU intensive, but GPU intensive.One thing that is immediately obvious, they have optimised MSFS 2020 for lower-end hardware. I mean, my CPU is decent, but MSFS 2020 does not take advantage of more than 4 cores (hopefully a future update does), but it’s clear that my graphics is the limiting factor here.I let the co-pilot drive the plane while testing and look what happened?It dropped me in a park somewhere, the coolest thing is I noticed there are cars driving on a road behind me (you can kind of see to the right hand of the above photo). The attention to detail in this game is amazing.",1196
"Netflix Australia Raises Its Prices, Again — But Does It Matter?","After raising the prices of its premium tier last year, Netflix is now raising the prices for its basic and standard plans over the coming weeks for existing members and effective immediately for new signups. The basic plan is going up by $1 to $10.99 and the standard plan increasing by $2 to $15.99.Netflix is now raising the prices for its basic and standard plansI do find it quite strange that Netflix would increase the prices of its streaming service during a pandemic, and furthermore, at a time when Australia officially entered recession not too long ago where the economy contracted seven per cent.Australia officially entered recessionI get it, Netflix is a business and has undoubtedly seen an increased demand for its services which means an increase on service resources, but I would have would thought would have been a good thing for them. The charts are also painting a great picture for Netflix.In Q1 2020, Netflix recorded $5.76 billion in revenue. In Q2 2020 it did even better, recording $6.14 billion in revenue. Keep in mind, these are US figures and representative of the whole company, not the Australian division. I realise that Netflix invests a lot in its original content and has produced some great shows (even if they have a tendency to cancel them after two seasons).A spokesperson had this to say about the price increasesMembers tell us how much they value the breadth and variety of the catalogue, and we’re updating our prices so that we can continue to invest in more shows and films.Members tell us how much they value the breadth and variety of the catalogue, and we’re updating our prices so that we can continue to invest in more shows and films.This pretty much confirms they’re increasing the prices to throw more money at its recently grown subscriber base. Netflix wants to retain its new subscribers before they experience catalogue fatigue and look elsewhere for content once they have watched everything good (we all eventually reach that point).If Netflix had very little competition, I could maybe forgive the price increases, but Netflix isn’t the only player at the streaming poker table. You have Stan, Amazon Prime Video, Disney+, Apple TV, Binge, BBC Britbox is also launching soon and there have been rumours that Hulu is eyeing off Australia after the success of Disney+.As a Netflix premium subscriber, I took last years price increase on the chin. We have enough consumers of Netflix in our household that the multiple streams are worth paying for (my wife and I, as well as two kids with different tastes) the fights over what to watch with premium are worth paying for alone.The timing for a price increase could not be any worse. Despite the widespread unemployment, recession and ongoing pandemic with on/off lockdowns between the states, Netflix is also fighting a war after the PR disaster that is the movie Cuties which has been accused of being paedophilia, resulting in cancel Netflix to trend online.But, at the end of the day, does it really matter? The increase is at most $2 per month and people spend more on a cup of single coffee here in Australia (unless you’re getting a 711 coffee). Not to mention, you do not need to have Netflix, you’re not locked into a contract and can cancel at anytime.Whether you realise it or not, never forget that as a consumer, Netflix is at your mercy and you can vote with your wallet if you do not like the price increase or how they handle bad PR. A lot of people these days cancel and resume their Netflix subscriptions on a regular basis as content comes and goes (new Stranger Things season and so forth).If money is a little tight, I highly recommend Amazon Prime Video, which is one of the cheapest streaming services in Australia at around $6 per month and has some serious great original content.",954
Convert a HTML Dropdown To an Object,"I had a rather interesting use-case recently where I needed to take the contents of an HTML dropdown on a website which had timezones and then convert it into an object. But, I’ve been there before. How many times have you wanted a country dropdown or age dropdown and just wanted to copy one from an existing website?I just opted for a simple for..loop in this case.// Query the dom for all option elements inside of the select dropdown called timezone
const options = document.querySelectorAll('#timezone option');
const obj = {};

for (const option of options) {
    obj.label = option.textContent.trim();
    obj.value = option.value;
}// Query the dom for all option elements inside of the select dropdown called timezone
const options = document.querySelectorAll('#timezone option');
const obj = {};

for (const option of options) {
    obj.label = option.textContent.trim();
    obj.value = option.value;
}I then use textContent to get the label from the option, and then I call trim to ensure that any whitespace doesn’t get copied over to my object. I call option.value to get the value. But, here is the thing… If you’re taking this data with the intention of iterating over it later on, you probably want it to be an iterable type such as an array or even a map.textContenttrimoption.valueBy creating an object and pushing it into an array, it will give us an array of objects, with our dropdown values dictated by a label and value property which correspond to our dropdown.labelvalue// Query the dom for all option elements inside of the select dropdown called timezone
const options = document.querySelectorAll('#timezone option');
const values = [];

for (const option of options) {
	values.push({ label: option.textContent.trim(), value: option.value }); 
}// Query the dom for all option elements inside of the select dropdown called timezone
const options = document.querySelectorAll('#timezone option');
const values = [];

for (const option of options) {
	values.push({ label: option.textContent.trim(), value: option.value }); 
}If you’re not a fan of arrays, maybe you prefer maps instead, you can do pretty much the same thing. The first argument is the name and the second argument of the set method is the value.set// Query the dom for all option elements inside of the select dropdown called timezone
const options = document.querySelectorAll('#timezone option');
const values = new Map();

for (const option of options) {
	values.set(option.textContent.trim(), option.value); 
}// Query the dom for all option elements inside of the select dropdown called timezone
const options = document.querySelectorAll('#timezone option');
const values = new Map();

for (const option of options) {
	values.set(option.textContent.trim(), option.value); 
}In this instance, I just opened up Developer Tools in Chrome and ran it directly in the console of the website that I wanted to call it on. If you console log the final values, you can then copy them from within Chrome. Right-click the printed value, choose, “Store as global variable” and it should create a new variable called temp1 if this is the first time running this code and then you can write copy(temp1) to copy it to the clipboard.temp1copy(temp1)Understandably, this is all pretty specific and non-typical Javascript, but if you ever want to copy a dropdown and convert it into an array, this will all do the trick.",849
Fixing Distorted/Pixelated Buggy Sound In Windows 10,"This is a really strange issue I encounter sporadically in Windows 10. Even freshly installing Windows and the issue will still occur. I use a Presonus Audiobox for my audio needs, so it is possible that the device itself might be the culprit or the drivers I am using.The sound I hear is this distorted/pixelated sound which sounds like what Minecraft would sound like if it were in audio form. This fix I’ve discovered I can’t vouch it fixes it permanently, but it does work without having to restart your PC.Right-click on the audio icon in the taskbar and select, “Open sound settings” or you can open the start menu and type, “Sound settings” to open it that way as well.You should be greeted with a similar screen to this one. Select, “Device properties” for your broken audio output device. Then click on the right-hand link, “Additional device properties”On the speaker properties window, choose the, “Enhancements” tab and select, “Disable all enhancements”, then click apply and ok. Your audio might possibly work now. This seems to work for me, but if your audio is still broken, it’s possible the issue might be something else.",285
Create a Trello Kanban Layout Using CSS Grid,"Arguably, one of the best additions to CSS in a very long time is CSS Grid. If you have been a developer longer than a minute, there is a good chance you remember what the web was like before CSS Grid and even before Flexbox.CSS GridThe most popular implementation of a kanban style layout is Trello. A lot of companies have replicated the easy, Kanban layout of Trello (including GitHub Projects).This is the final layout you’ll be learning to build.This is the final layout you’ll be learning to build.Inspecting the width of the columns in Trello, we know that each column has a width of 272px. The width of Trello’s columns are fixed and always 272px, so we do not need to make the width responsive. We also can see inspecting Trello’s markup and CSS that it has a gap of 8px between each main column..boards-container {
  display: grid;
  grid-auto-columns: 272px;
  grid-auto-flow: column;
  grid-gap: 8px;
  height: 100vh;
  overflow: auto;
}.boards-container {
  display: grid;
  grid-auto-columns: 272px;
  grid-auto-flow: column;
  grid-gap: 8px;
  height: 100vh;
  overflow: auto;
}This CSS gives us a Trello style Kanban column layout. It gives us columns of 272px. We use grid-auto-columns to tell CSS Grid to automatically create new columns, this negates the need to specify the number of columns we want (because we can have infinite columns). Using grid-auto-flow tells our grid not to wrap and keep it as columns (expanding across the x-axis). Finally, we use overflow: auto to give us a scrollbar.grid-auto-columnsgrid-auto-flowoverflow: autoThe HTML looks like thisThe HTML looks like this<body>
  <div class=""boards-container"">
    <div class=""boards-container__board""></div>
    <div class=""boards-container__board""></div>
    <div class=""boards-container__board""></div>
    <div class=""boards-container__board""></div>
  </div>
</body><body>
  <div class=""boards-container"">
    <div class=""boards-container__board""></div>
    <div class=""boards-container__board""></div>
    <div class=""boards-container__board""></div>
    <div class=""boards-container__board""></div>
  </div>
</body>As you can see in our Codepen example, we have a Trello column layout perfectly spaced, with a scrollbar. It’s crazy to think how difficult stuff like this used to be a few years ago, now in a few lines of CSS, is easy.

Now, you’re probably wondering how we can get the cards inside of the columns to be like Trello’s layout. Well, turns out, the code needed for that is also only a few lines of CSS.For each column of our board, we’ll create a grid and then tell it to create rows (instead of columns) that are as big as their content. We add in a grid gap (instead of margin), some padding and background colour for each column. We also add a border-radius of 3px because that’s what Trello does as well..boards-container__board {
    background: #EBECF0;
    border-radius: 3px;
    display: grid;
    grid-auto-rows: max-content;
    grid-gap: 10px;
    padding: 10px;
}.boards-container__board {
    background: #EBECF0;
    border-radius: 3px;
    display: grid;
    grid-auto-rows: max-content;
    grid-gap: 10px;
    padding: 10px;
}Now, let’s do some quick styling for each of the cards. We add a background for each card, a box shadow, padding and border radius. We also need to style our heading by making it smaller and reducing the initial margin set on it by the browser..boards-container__board__card {
    background: #FFF;
    box-shadow: 0 1px 0 rgba(9,30,66,.25);
    border-radius: 3px;
    padding: 10px;
}

.boards-container__board h1 {
    font-size: 16px;
    margin: 0 0 12px 0;
}.boards-container__board__card {
    background: #FFF;
    box-shadow: 0 1px 0 rgba(9,30,66,.25);
    border-radius: 3px;
    padding: 10px;
}

.boards-container__board h1 {
    font-size: 16px;
    margin: 0 0 12px 0;
}This provides us with a final Trello-esque Kanban layout driven by CSS Grid. It’s simple and effective.

I know CSS Grid seems intimidating and honestly, it is. But, for certain layouts, CSS Grid is the right tool for the right job and can make your life so much easier. Flex is still great and can be used alongside CSS Grid, but when it comes to layout, CSS Grid is the new king.",1053
Firebase Is Great for Lazy Developers Like Myself Who Hate Setting up Backends,"I have been solely a front-end developer for six years now and while I can still find my way around on the backend (with Node and PHP) my interest in the backend has all but faded.This is where platforms like Firebase are a real asset for developers like me.It is no secret the front-end has become complicated, if you’re not fighting a framework or library for control, you’re debugging confusing TypeScript error messages or trying to get Webpack configured. While the backend is a little more straightforward from a lack of new things to learn every five minutes like the front-end, it’s just another thing to worry about.Part of setting up a backend isn’t just setting it up, it’s configuring file/folder permissions, it is ensuring your firewall rules are appropriately configured, that you’ve disabled password-based access on your server, that you’ve disabled external access to your MySQL database.The amount of work to properly secure a backend is quite extensive and tedious.This is why platforms like Firebase and hosting solutions like Heroku and Vercel are of interest to me and many other developers like me. Sure, there is a risk in trusting that the provider you’re using has taken the appropriate measures to secure your data, but nonetheless, they’re going to do a much better job than you.But, laziness comes at a cost. Firebase is notorious for getting really expensive, real quick. The lack of free plan means if you incorrectly setup your Firestore rules for example, you can be stung with a bill for thousands as a few other well-known examples point out.Still, as long as I am careful, I would much rather roll the dice than spend days setting up a backend that is going to have more holes in it than swiss cheese.",435
Moment.js Officially Becomes A Legacy Project In Maintenance Mode,"Well, here is an announcement that probably won’t surprise anyone who has used Moment in the last few years, especially trying to get the size of it down in your Webpack configuration. The Moment.js team has announced it is now a legacy project in maintenance mode.has announced it is now a legacy project in maintenance mode It is not dead, but it is indeed done. It is not dead, but it is indeed done. It is not dead, but it is indeed done.deaddoneThe Moment team poetically declares, “We now generally consider Moment to be a legacy project in maintenance mode. It is not dead, but it is indeed done.” Which means, we’ll fix any serious issues (security concerns, etc) but new features or changes are off the table.deaddoneI moved over to date-fns quite a long time ago which I equate to being the Lodash of date libraries. It’s small, tree shakeable and can work with the Intl API for internationalisation. Moment.js hasn’t really been a viable option for most circumstances for a few years now.date-fnsIntl APIMoment came at a time where the Javascript ecosystem definitely needed a date/time library, for quite a long time, Moment was as good as it was going to get. Fortunately, browsers and working groups have evolved to the point where dates and times (while still not easy) are a lot easier than they were. The lack of tree shaking is one of the biggest downsides to Moment.Just like jQuery greatly contributed to modern Javascript API’s, Moment has been instrumental in highlighting the importance and providing a stepping stone for developers to feel less pain working with dates and times, especially timezones. RIP Moment.js.",410
Microsoft Flight Simulator 2020 Is The New Crysis,"The old Internet meme which was rooted in reality, “But, can it run Crysis?” believe it or not, Crysis was released in 2007 and since then, obviously other more hardware pushing titles have been released since then. Crysis funnily enough still held the title for years, it took years before hardware caught up and could run Crysis on max settings.it took years before hardware caught upIf you told someone back in 2007 that one day the new king of hardware pushing game would be a flight simulator, you would have been laughed at.And yet, here we are.The requirements for Microsoft Flight Simulator 2020 are:MinimumMinimumCPU: Ryzen 3 1200/Intel i5-4460GPU: Radeon RX 570/NVIDIA GTX 770VRAM: 2GBRAM: 8GBHDD: 150 GBCPU: Ryzen 3 1200/Intel i5-4460GPU: Radeon RX 570/NVIDIA GTX 770VRAM: 2GBRAM: 8GBHDD: 150 GBRecommendedRecommendedCPU: Ryzen 5 1500K/Intel i5-8400GPU: Radeon RX 590/NVIDIA GTX 970VRAM: 4GBRAM: 16GBHDD: 150GBCPU: Ryzen 5 1500K/Intel i5-8400GPU: Radeon RX 590/NVIDIA GTX 970VRAM: 4GBRAM: 16GBHDD: 150GBIdealIdealCPU: Ryzen 7 Pro 2700X/Intel i7-5800XGPU: Radeon VII/NVIDIA RTX 2080VRAM: 8GBRAM: 32GBHDD: 150 GB SSDCPU: Ryzen 7 Pro 2700X/Intel i7-5800XGPU: Radeon VII/NVIDIA RTX 2080VRAM: 8GBRAM: 32GBHDD: 150 GB SSDThe minimum requirements are quite reasonable, recommended reasonable and ideal not exactly too crazy. The only thing some might not meet is 32gb of RAM, but that is an easy upgrade.The biggest problems of MSFS 2020 appear to be optimisation issues in the game itself. The game clearly has some issues in high-trafficked areas like New York and even Sydney, Australia. While MSFS 2020 is perfectly fine at 30 fps, there is something concerning about a game on high-end hardware running at such low fps comparatively to what others might deem more visually intensive titles.In numerous benchmarks which you can find online, you’ll notice that no current-gen CPU or GPU can run MSFS 2020 at high settings and achieve a smooth 60+ fps.

It will be interesting to see if the new Nvidia 30 series cards (including the coveted Nvidia 3090) can run MSFS 2020 at its highest settings and achieve 60+fps. Although, considering the game is not optimised for DirectX12, nor does it take advantage of multi-core CPU’s, the game itself could still be the bottleneck.Perhaps I am being a little too hard on MSFS 2020. I know it’s not completely optimised, but it’s playable. If you have a potato PC, I do not advise flying over New York, it will pin your hardware to the wall and make every fan you have spin up to maximum RPM.The question we will now be asking ourselves is, Can it run Flight Simulator? Sadly, for as long as the game is CPU bottlenecked, we’ll keep hearing that question being asked, perhaps even on high-end cards like the 3080 and 3090.",692
Elon Musk Is a Fancy Marketing Man Who Wants to Put an Implant Into Your Brain and Make You Boop,"It’s hard to deny that Elon Musk’s brand of futuristic dreamer type thinking is not contagious. I, for one, am excited about the future of electrical vehicles and the innovation brought about by Tesla is undeniable.But, I want to talk about one of Elon’s most bizarre product ideas: Neuralink.Brain implants, dude.Brain implants, dude.Allegedly, it’ll cure everything from neurological disorders to autism. And what futuristic brain-computer would be complete without the ability to use it to listen to music as well?Over the years Elon has marred himself in controversy, attracting the ire of the SEC for numerous violations and having to guarantee his own insurance because of how big of a risk he is to traditional insurance companies. A lot of his predictions have been way off the mark and overly optimistic. It’s clear Elon subscribes to the school of positive thinking.It’s not that uncommon you see Elon’s name in big lights alongside other revolutionaries of our time like Steve Jobs. To Elon’s credit, he has achieved more than many in his position would and taken on a lot of risks which have paid off like SpaceX. It’s clear Elon isn’t afraid to play the long game.After silence, speculation and little snippets from Joe Rogan podcast episodes and interviews, we were finally going to get some Neuralink information and a demo. Were we going to see someone have their eyesight returned or maybe Elon was going to cure COVID-19?In the lead up to the August 28 event, Elon made a bold promise to show us the matrix in the matrix.
Will show neurons firing in real-time on August 28th. The matrix in the matrix.— Elon Musk (@elonmusk) July 30, 2020
Will show neurons firing in real-time on August 28th. The matrix in the matrix.— Elon Musk (@elonmusk) July 30, 2020Will show neurons firing in real-time on August 28th. The matrix in the matrix.July 30, 2020We were going to get a demo of the elusive Neuralink working in real-time, “Will show neurons firing in real-time on August 28th. The matrix in the matrix.” — the matrix within the matrix, this sounds like high-level exciting stuff.In true Elon fashion, August 28 rolls around and it is revealed that these implants have been put into pigs. Every time the pig snuffles, it would trigger a beeping noise.

Damn, Elon… You click baited us. I would have given props to Elon if he owned the clearly absurd demo by showing us footage of Rick Astley’s internet meme hit, Never Gonna Give You Up. Overdone, but when done right, a Rick Roll is still appreciated.We were promised a look inside the matrix of the human mind, instead, we got snout boops. Do you remember those key finders where you would whistle and they would make a noise to help you locate your keys? That’s what the Neuralink feels like in this demo.In all fairness, this isn’t simple stuff. Elon and Neuralink’s intentions are noble. There is nothing wrong with wanting to cure brain diseases, help people get back the ability to talk/walk or see again. But on the other hand, there is something cruel about getting the hopes up of people living with these issues only to show the extent of your product can do what medical technology has been able to do for at least a decade and a bit.Can you imagine being paralysed and hearing Elon promise a revolutionary new brain-computer that will possibly help you walk again? I am not doubting that it can’t or won’t happen, but there is obviously a reason nobody else is doing this stuff to this extent: it’s difficult, ambitious and is at least a few decades away from being a reality (once you factor in testing and approvals).It even turns out there is a website dedicated to tracking Elon Musk’s broken promises, ouch. It seems the patience of some has run out and Elon’s brand of futurism is getting old. Still, I’ll find myself being click baited by Elon for the foreseeable future.Elon Musk’s broken promises",971
No time for gaming.,"I made a realisation the other day, I own a lot of games. Not just on PC, but on my Nintendo Switch as well. Of all of the games I own, I’ve played maybe a few hours of a couple in 2020, but for the most part, haven’t really spent much time gaming whatsoever.And it isn’t for lack of trying.I recently bought the Tony Hawk’s Pro Skater 1 & 2 remastered version and I was instantly taken back to my youth, the days where I could game all day and then all night, get up and go to school the next day and somehow function.Playing the game made me realise that I don’t game as much anymore, with work, family and being knee-deep in renovations taking up the majority of my time.It’s quite strange to think that in 2020, as we are all at home more than we ever have been, not having to commute into the office, we’ve been given back quite a few hours we would normally lose during an average normal year day. And yet, I still can’t get any solid gaming time.I actually sold my Playstation 4 to buy my Nintendo Switch. I was annoyed that every time I went to play a game, I would spend most of my time waiting for a 30gb patch to download. Even with auto updates, I would seemingly always have to wait for a Playstation update to finish. The Switch appealed to me because it was pick up and play.And in a huge bout of irony, I recently built a new PC which is quite a beasty gaming machine. I bought Microsoft Flight Simulator 2020 and I have maybe played 4 hours of it so far, which is more time spent gaming than I’ve probably spent on average the last two years.The Switch definitely gets a workout, though. My five-year-old son loves the Switch and plays it more than I ever have. He’s managed to unlock all of the characters in Super Smash Bros (which I love him for) and I occasionally battle him in it. When he started I had unlocked about 8 characters.On my bucket list, I still have The Legend of Zelda to complete and there is a new Zelda game coming soon: Breath of The Wild 2. No release date as of yet, but it was revealed back in 2019, so suffice to say, it’ll probably be here early 2021.

By the time Breath of The Wild 2 is released, I might have got a further 5% into the game. I can’t even remember where I am up to.And then there are all of the PC games I own. I used to hit Rocket League on PC quite hard until I got it on Switch and then I played a bit on that. I’ve got Microsoft Flight Simulator 2020, I still have love for Dota 2 and one I’ve been meaning to sink my teeth into is Red Dead Redemption 2.I have no time for gaming, but I want it. As soon as our renovations are done, I’ll be able to spend a little more time having fun and less time precariously drilling into walls to find the drill bit breaking hardwood studs beneath.",689
Firebase vs Supabase,"For such a long time, Firebase has been in a league of its own. Not only does Firebase offer hosting, cloud functions (AWS Lambda serverless style functions), but it also offers authentication, two different types of database and a plethora of other features.Honestly, nothing else has ever really come close to Firebase… Until now.Supabase is a promising open-source alternative that eventually aims to be a close replacement for Firebase. And if you’re thinking Supabase is some scrappy upstart, they were in the Y Combinator Summer 2020 batch and received $125,000 USD in seed funding from Y Combinator.Despite the fact that Supabase is still in alpha (at the time of writing this), already, it features quite a strong feature-set that takes on some of Firebase’ most appealing features like the real-time database and simple SDK-based drop-in authentication.Supabase And the best thing of all: Supabase is merely a layer on-top of the versatile and battle-tested PostgreSQL database. Even the authentication functionality (non-oAuth) uses row-level security features in PostgreSQL.Although Supabase is currently in an early-ish alpha, it is already quite promising. I have yet to try out their hosted version, but I was able to get a nice Supabase test setup on Digitalocean using an image on the DigitalOcean marketplace quite easily.The beautiful thing about Supabase is where possible, they have opted to use existing libraries and databases. Instead of leveraging always cutting edge technology, they’re using PostgreSQL for quite a lot of their functionality and other libraries to implement other features. The lack of custom built functionality is a good thing from a lock-in perspective.As a long-term Firebase user, I certainly do feel locked in. Sure, Firebase gives you quite a decent authentication feature with support for all major oAuth providers as well as anonymous and username/password type authentication, but it definitely isn’t easy migrating away from an authentication perspective.As you can see, Supabase already has basic authentication support. However, many modern applications utilise logins via oAuth providers such as; Facebook, GitHub, Google, Twitter and so on. A feature which Supabase will support, but at the time of publishing this, does not.Supabase already has basic authentication supportAs we see the rise of no-code, I think it is clear that Firebase has had the right idea all along. Firebase has always been ahead of its time, which explains the lack of competitors. However, with the rise of Vercel, Netlify and now, Supabase, it’s clear that the concept is starting to become more popular.If Amazon ever devotes proper resources, AWS Amplify could also be another viable alternative to Firebase. Although, as things currently stand, Amplify is an incredibly limited and crippled version of Firebase that doesn’t come anywhere near close to its functionality and ease-of-use.AWS Amplify could also be another viable alternative to FirebaseThe TL;DR right now is that Supabase is still in its infancy, but is already quite promising. While it will not replace every single feature that Firebase offers, it will offer the important features (database and authentication) with authentication being one of the most popular and loved parts of Firebase (at least to me, anyway).Let’s check back in 2021 and see where Supabase is at. If Y Combinator sees value in what they are doing, then people should stand up and take notice.",868
Are Phone Makers Just Trying to Outweird One Another Now? The Alleged LG Wing Is Strange,"Footage of an alleged device from LG called the LG Wing have leaked out onto YouTube ahead of a planned September 14 event.

It appears as though the smartphone race has taken a drastic turn. It started out with phone makers battling each other for the best camera, the biggest battery, the highest resolution screens. Now that all major phone makers are on an almost equal playing field, companies like Samsung and LG are taking it to the extreme.While Samsung tries to bring back flip phones like it’s 2005 again with the Galaxy Z Flip, LG seems to be exploring some weird swivel phone which can really only be described as bizarre if the above video is legitimate and that is indeed a new phone LG is going to be announcing.Galaxy Z FlipIt makes me wonder, who is the target market for a device like this? Is it people who want futuristic phones that look like preschool arts and crafts creations? Maybe I am missing out on something here, but the LG Wing (if it is a real device) is one of the weirdest things I’ve seen in 2020 and it has been a strange year.",266
Could Grand Theft Auto 6 Copy Microsoft Flight Simulator 2020’s Realism?,"There has been plenty of talk and speculation for years about what a Grand Theft Auto 6 game could look like, what world it would be set in and how it might work. It’s not an industry secret that GTA V is a money-making machine, still making a tidy profit in the seven years it has been out.Microsoft Flight Simulator 2020 was recently released and it is a next-gen marvel. The game uses a combination of real-life data (Bing Maps run through a 3D algorithm) combined with real weather and air traffic, you’re flying in a realistic living world.Now, imagine if that same real-life meets game technology in MSFS 2020 made its way into GTA 6? There have been plenty of rumours and alleged leaks detailing an expansive game that will not be set in one era and transition through different decades as the world changes.However, there are some key differences between GTA and MSFS 2020. The biggest difference being you can’t get out of your plane in Microsoft Flight Simulator 2020 and Grand Theft Auto is based on travelling by foot, car, bike, plane, boat and other means of transport. A living city built using real map data would require a more substantial algorithm to generate something of that scale.However, there is the possibility that GTA 6 could focus on main cities in the game like Los Santos, Vice City, Liberty City (detailed cities with interiors and interaction like GTA V) and so on, filling in the blanks with algorithmically generated terrain using map data.There is also the possibility that real-life data is irrelevant in a controversial game like GTA. It has always blended arcade with realism, real, but not too real. Even in GTA V, you had realism mixed with arcade nonsense like outlandish weapons such as the Atomizer or Peyote plants that allowed you to turn into different animals.If some of the leaks that have come out around GTA VI have an ounce of truth to them, we could be getting weather events (hurricanes, storms, fires, tsunamis, earthquakes) and I think adding in weather in itself would be a nice dose of realism to the GTA world.I fired up GTA V and played for a couple of hours before I wrote this post and honestly, the game has held up. The casino DLC and other improvements have made this living game stand the test of time and graphically, it still looks impressive for a 7 year old game.It is clear that Rockstar Games are in no rush, with GTA V being announced for next-gen consoles, clearly the cash cow GTA V is still lucrative and popular enough to release on yet another gaming console. This means GTA V will have been on 3 different generations of gaming console, which has to be some kind of record.",663
A Tested (and working) Streamlabs OBS Display Capture Black Screen Fix,"I do a bit of live streaming over on Twitch (give me a follow if you’re interested in coding and this post helped you) and while I primarily use my desktop PC, I was travelling recently and wanted to use my gaming laptop to do a little code streaming (scroll down to the “fix” section if you’re in a hurry).give me a followLike many, I use Streamlabs OBS for my streaming needs. While I attempted to enable display capture for my live scene, my webcam was working fine, but none of my laptop display was being captured. All I got was this black screen and no explanation.I tried everything inside of Streamlabs OBS, but it turns out it is related to the graphics in the laptop being in use, requiring OBS to be configured to use integrated graphics. While we are probably not guaranteed to have the same laptops, this fix should work for anyone experiencing the same issue. I have an integrated Nvidia 1060 GPU.After a lot of dead ends reading forum posts, blog posts and anything else I could find, nothing seemed to work. I knuckled down and was able to fix the solution.The FixOpen up the Nvidia Control Panel (right-click on your desktop and choose, Nvidia Control Panel).Inside the control panel on the left-hand side, select, “Manage 3d settings” and on the right-hand side, choose the program settings tab. Don’t worry about the dropdown, Streamlabs OBS will not be detected there. Click the, “add” button instead.If you have recently opened Streamlabs OBS you’ll see it at the top of the list, but ignore that. The real file for Streamlabs OBS is obs64.exe select it and click, “Add Selected Program”.Click the, “Select the preferred graphics processor for this program” dropdown and choose, “Integrated Graphics” and then click the “Apply” button.Now, open up Streamlabs OBS (and make sure you’re running it as an administrator). You should now see your display capture working, so additional trickery or configuration needed.Really, it is that simple. A lot of the solutions I found focused on enabling irrelevant things like compatibility mode, updating drivers for your graphics card and even changing the performance mode for the app to high performance (all of which, do nothing). I hope this helped you.",555
Will WebAssembly Replace Javascript?,"Depending on who you speak to, WebAssembly will kill Javascript, WebAssembly will enhance Javascript, and WebAssembly will merely appeal to a small subset of die-hard developers working on problems that require lower-level bare metal solutions written in a compiled language.A lot of work has gone into reducing the performance issues that have plagued WebAssembly regarding making JS calls (especially when working with the DOM). Efforts seem to be focused on making WebAssembly more than a memory-siloed thing, but rather a viable alternative for anyone wanting to work mainly within the confines of WebAssembly.The thing with accessing the DOM in WebAssembly applications that need to do so is you still have to go through Javascript. To my knowledge, there are no plans to allow for direct-DOM access via WebAssembly, but who honestly knows.Since its early days of development as a specification, WebAssembly has been designed to complement and work alongside Javascript, not replacing it. There are no important aspects of WebAssembly that wall themselves off from Javascript. The two can communicate with one another.An ongoing feature built into WebAssembly is garbage collection which will then share these objects with Javascript and allow them to be accessed. But, as always with these proposals, who knows if it’ll be accepted or the final implementation will resemble that of the initial specification.garbage collectionWhile Javascript is great for building UI’s such as web applications and websites, you can fall into performance traps when you’re building GPU/CPU intensive applications that go beyond the limited capabilities of modern browsers. An exciting application area is lower-level graphics, allowing things like Virtual Reality in the browser to achieve the required low-latency and high-framerate performance necessary for a smooth experience.One of the areas that WebAssembly has the potential to be used in is gaming. A port of Doom 3 into WebAssembly has proven to be a huge success. Figma famously made the early move to use WebAssembly in 2017 and saw substantial performance benefits.port of Doom 3 into WebAssemblymade the early move to use WebAssembly in 2017As we advance, you’ll not only see games take advantage of WebAssembly, but another promising area where I believe you’ll see some WebAssembly innovation is blockchain. Imagine a blockchain running inside your web browser, which could be used to build decentralised applications or as a stand-in for traditional databases.I also envision web frameworks and libraries using WebAssembly for their virtual DOM implementations such as ReactJS. Imagine if the dom-diffing algorithm used for the Virtual DOM was moved inside WebAssembly, freeing up the main thread for crucial UI tasks and ensuring smoother frame rates?As you can see, many of the possible use-cases for WebAssembly will still have some UI component, and that’s where CSS, HTML and Javascript will still be needed to create rich interfaces. Still, WebAssembly will allow developers to move intensive UI-blocking work outside of the browser. It will result in better performing web applications that run on newer and older hardware more smoothly.So, to answer the question (if the answer wasn’t obvious enough), WebAssembly will not replace Javascript. If anything, WebAssembly makes Javascript better.",839
Does @types/node have to be the same version as Node?,"You’ll notice from time-to-time that Node.js will release a new minor version and sometimes a major version. If you’re working with TypeScript, you’ll want to install the @types/node typings into your project.@types/nodeIf you’re running Node.js version 12.14.1 confusion might set in because there is no matching version for the Node types package.Node is known to introduce new features into minor releases, so ensuring that your typings are up-to-date within the latest major release band will ensure that your typings are not too old or too far forward (LTS vs current release).If you attempt to use typings for Node that are not for your appropriate version of installed Node.js, you could run into issues as major releases, in particular, introduce breaking changes and new features.",197
Git: Keeping A Fork In-sync With The Source Repository,"You fork a repository on somewhere like GitHub or GitLab and you make some changes. Maybe you want to contribute to an open-source repository. You work on your fork, but in the interim, the repository you forked has had a new release and quite a few new commits.How do you get those commits from the repository you forked and merge them into your fork? This is where setting an upstream from the forked repository comes in handy.1. Add an upstream remoteFirst thing you should do after forking a repository, is adding the source as an upstream remote.git remote add upstream git@github.com:aurelia/aurelia.git
git remote add upstream git@github.com:aurelia/aurelia.git
2. Keep the upstream up-to-dateThis will fetch changes from our upstream (the parent repository that we forked from) — if there are any new changes, it will update our upstream.git fetch upstreamgit fetch upstream3. Then to update your current repository, you can fetch and rebaseThe reason you should use rebase is if you have some local changes, your commit history will be kept clean. If you have ever pulled and merged changes in without rebasing, then you might have seen how messy your history can get.git rebase upstream/mastergit rebase upstream/master",307
How to Add Dark Mode to Your Website Using Only CSS,"If you’re a developer, chances are you have a penchant for dark mode. Staring at a screen all day and possibly part of the night, dark mode is easier on the eyes and it just looks awesome.It might surprise some of you to know that CSS has native support for dark mode and styling specifics using a property called prefers-color-scheme.prefers-color-scheme@media (prefers-color-scheme: dark) {
    /* paint it black */
}@media (prefers-color-scheme: dark) {
    /* paint it black */
}The best thing about prefers-color-scheme is that it is well-supported. If you do not have to support Internet Explorer, then you can use it and not have to worry about any polyfills or Javascript fallbacks.prefers-color-schemeit is well-supportedTo enable native Dark Mode if you’re using Windows 10, go to the personalization screen and choose default app mode to be dark. The latest version of macOS also supports OS-level dark mode as well.It might surprise you to know (if you don’t already have dark mode enabled) that this blog has a dark mode theme and it is just a few lines of CSS to pull it off.@media (prefers-color-scheme: dark) {
	body {
		background: #0D1219;
		color: #FFF;
	}
	
	.site-title a,
	.genesis-nav-menu a,
	.entry-title a {
		color: #FFF;
	}
}@media (prefers-color-scheme: dark) {
	body {
		background: #0D1219;
		color: #FFF;
	}
	
	.site-title a,
	.genesis-nav-menu a,
	.entry-title a {
		color: #FFF;
	}
}That is the dark theme for this site. Simple and effective. And in case you still haven’t enabled native dark mode just yet this is what this blog post looks like with dark mode enabled.No Javascript or fancy tricks needed, just good ol’ fashioned CSS.",417
Building A New PC For Gaming & Development,"I have had my current Core i5 6600k PC for a few years now and it has served me well and if I wasn’t upgrading, would continue for years to come. However, now I am working from home more and find myself branching out into streaming, video editing and trying to play Microsoft Flight Simulator, I needed something stronger and more future proof.Five years for a PC is an incredible run. I never turn my PC off, not to mention, I overclocked it and it never skipped a beat or reduced its lifespan. I have always been Team Blue (Intel), but the price and compelling benchmarks of the Amd Ryzen chips had me intrigued.After some research, I was sold… I am going to switch teams and move on over to Team Red (sorry, Intel). Sure, Intel might have better single-core performance and games are mostly not taking advantage of multiple cores beyond 4 (yet), but that will soon change and the Ryzen will rise to the challenge.I envision that this build will last me another 5 years, at least. The sheer number of cores in this thing means that the only aspect that will become outdated is the graphics card.The Parts ListAMD Ryzen 9 3900X with Wraith Prism coolerGigabyte B550 AORUS MASTER AM4 ATX MotherboardKingston HyperX FURY RGB 16GB (2x 8GB) DDR4 3200MHz MemoryThermaltake View 51 TG ARGB Full Tower E-ATX Case (black)AMD Ryzen 9 3900X with Wraith Prism coolerAMD Ryzen 9 3900X with Wraith Prism coolerGigabyte B550 AORUS MASTER AM4 ATX MotherboardGigabyte B550 AORUS MASTER AM4 ATX MotherboardKingston HyperX FURY RGB 16GB (2x 8GB) DDR4 3200MHz MemoryKingston HyperX FURY RGB 16GB (2x 8GB) DDR4 3200MHz MemoryThermaltake View 51 TG ARGB Full Tower E-ATX Case (black)Thermaltake View 51 TG ARGB Full Tower E-ATX Case (black)I already had a relatively new power supply, a Geforce RTX 2060 graphics card and a 1tb Samsung Evo 970 solid-state drive (SSD), so I really only needed a CPU/motherboard, memory and a case. Technically, I didn’t need the case, but I wanted something new to compliment the build and promote good airflow.Geforce RTX 2060 graphics card1tb Samsung Evo 970 solid-state driveThat is definitely something I encourage. If you’re going to do a new PC build, motherboard and CPU are essential as it is rare you’ll just upgrade one or the other. But, using your existing power supply (if it meets the requirements for your chosen hardware) prevents unnecessary landfill and it makes things cheaper. I would say also the existing case, but who am I to say?I plan on adding another 16gb of ram shortly, but to keep costs down, I went for 16gb upfront.The curious case of the big caseThe curious case of the big caseI am usually a mid-size ATX case kind of guy, but this time I was won over by the Thermaltake View 51 TG case. My wife was shocked when she saw it, it looks like a small refrigerator.I am not going to lie, adjusting to full tower life after being used to mid-tower life for most of my home building PC career required a slight adjustment and a half. But, the added space and more future proofability of the case is really what you’re buying into. If you want liquid cooling, you want a full-size case.AssemblyThis is always my favourite part, putting it together. Find yourself a nice spot that isn’t overly dusty, has plenty of space and most importantly: not on carpet. I have only ever fried a chip once (years ago when I was young and naive) and it was an expensive lesson.One of the easiest steps in the build process is installing the motherboard. I like to keep my motherboard in the box and install my CPU then and there, followed by your air cooler (unless you’re going liquid). I touch a metal part of the case before I touch any electrical components just in-case there is any static. You don’t need an anti-static wrist strap, just make sure you’re discharging into metal or touch something else.I then install my memory sticks into the board and it’s ready to be transplanted into the case. Ensure you are affixing the screws to the risers correctly in the case, don’t do them up too tight.As you can see, an ATX motherboard inside of this case is almost comical. It’s a standard size motherboard, but this case makes it look like a micro-ATX board because it’s a monster.Always the second step for me after the motherboard is in: the power. In this particular case, it goes behind and the wires feed in through. This is a stark difference to other builds I’ve done where everything all lives within the one confined area of the case. But, we’re building a PC inside of a small house, so there is more space.Your motherboard is in, your CPU and power supply are installed, the last step is to add in a graphics card and affix it to the PCI slots in your case. Then, finally run all of your cables to the motherboard (power and case switches/lights).Sorry for the poor photo quality, the tempered glass causes a lot of reflections. But, this is the final result. I’m still working on cable management and I plan on going and tidying this all up, but I was excited and wanted to see it all turn on.",1258
Native TypeScript Support In The Web Browser,"I’ve had this thought one and off over the last few years since I started using TypeScript back in 2015 actually. What if Web browsers natively supported TypeScript?If Web browsers could natively support TypeScript without needing a build step first, would it be possible and secondly, would it be performant? As in, you just build your site and instead of .js files you reference .ts files..js.tsIt turns out, someone else had the same idea back in 2016 when they created an issue on the Chakra GitHub repository requesting native TypeScript support. The team had discussed the possibility but they were against it because TypeScript isn’t a web standard and they believe WebAssembly is a better option.We now know based on Deno experimenting with native TypeScript Support in its Node.js successor that it might be easier said than done. As it currently stands, the TypeScript compiler has some performance issues due to the overhead of type checking.someone else had the same idea back in 2016 when they created an issueit might be easier said than doneWhile Deno has done a fantastic job attempting to get native TypeScript support in its Node successor, they still experienced issues in regards to type checking incuring a performance hit. The idea was floated in the above linked issue, but rewriting the type checker in Rust while undoubtedly providing performance improvements would open up a can of problems.TypeScript is being constantly iterated on and improved, any type checker written outside of the TS core would need to be updated and patched almost constantly. That’s a lot of work just to have your own individual and more performance type checker.Secondly and most importantly, the amount of work to rewrite the TypeScript type checker into Rust would be an astronomically huge, expensive and time-consuming endeavour.We also can’t forget that Google already attempted to do something similar with Dart several years ago. Google Chrome shipped with a Dart VM and you could write C-like Dart code which supported generics, static typing, and SIMD. Although, where Google went wrong was attempting to replace Javascript, not enhance it.Seeing first-hand the problems that Deno has had in regards to performance and native TypeScript support, the dream of browsers natively supporting TypeScript will probably always remain a dream, unless the TypeScript team themselves move TypeScript to Rust (which is doubtful).",608
I Upgraded My Aurelia 2 Projects To Yarn v2 and It Worked,"When I last tried Yarn v2, it was a nightmare. Besides the fact it was a completely new major version that fundamentally changed how Yarn worked (from its architecture to how dependencies were handled).After seeing news about the latest release for Yarn v2, I got to thinking and the idea I would migrate one of my Aurelia 2 Projects from Yarn v1 go Yarn v2 was born.This isn’t going to be a tutorial or how-to for upgrading. I didn’t actually encounter any issues upgrading to v2. In fact, this is quite a non-eventful post.I followed all of the steps in the official Yarn documentation which details installing and configuring Yarn to work with v2. They’ve handled the legacy problem quite nicely, you install v1 of Yarn as a global Node package and then you configure on a per-project basis to switch over to v2.which details installing and configuring Yarn to work with v2As a precaution, I initially enabled legacy support for Node modules because I wasn’t sure how things would go without a physical node_modules directory. But, it worked without the need for legacy mode. Not all libraries are compatible with the new version of Yarn, but many are finally starting to catch up or fix the issues.enabled legacy support for Node modulesnode_modules",313
You Should Wait for AMD to Announce Its Big Navi Cards Before Buying a New Nvidia 30 Series,"Nvidia unexpectedly dropped three new graphics cards in its recent announcement. The 3070, 3080 and 3090 at price points which are just as impressive as the cards themselves.The 30 series Ampere cards are a new generational leap for graphics cards which have a lot of people excited. Noticeably quiet, but imminently expected to launch its Big Navi architecture cards is AMD.And it seems AMD is getting ready, given they’re doing publicity stunts in Fortnite and seemingly, staging them as well. 
Hey @AMDGaming @AMDRyzen @AMD @sherkelman. Found this on Fortnite. What does it mean? pic.twitter.com/C3cpOTx8DU— Gina (@MissGinaDarling) September 4, 2020
Hey @AMDGaming @AMDRyzen @AMD @sherkelman. Found this on Fortnite. What does it mean? pic.twitter.com/C3cpOTx8DU— Gina (@MissGinaDarling) September 4, 2020Hey @AMDGaming @AMDRyzen @AMD @sherkelman. Found this on Fortnite. What does it mean? pic.twitter.com/C3cpOTx8DU@AMDGaming@AMDRyzen@AMD@sherkelmanpic.twitter.com/C3cpOTx8DUSeptember 4, 2020Here is the thing, the Nvidia 30xx series cards are not only competently priced and affordable, except maybe the 3090 (Titan equivalent), but they’ll all be launched by October 2020.I don’t think anyone (including AMD) expected these new Nvidia cards to be so cheap. Which has put AMD into an interesting position, because if they can’t match Nvidia in performance, they’ll have to reduce their price to make them a more attractive option.There are rumours that AMD have had to readjust their pricing for their new cards
""They [AMD] want to release the 16GB at $599 and the 8GB at $499 but after the Ampere announcement I'm expecting $549 for the 16GB and $499 for the 8GB. We're receiving the ASIC (GPU+MEM) this month [September]"" – AMD partner pic.twitter.com/tLd6Fe36Or— coreteks (@coreteks) September 4, 2020
""They [AMD] want to release the 16GB at $599 and the 8GB at $499 but after the Ampere announcement I'm expecting $549 for the 16GB and $499 for the 8GB. We're receiving the ASIC (GPU+MEM) this month [September]"" – AMD partner pic.twitter.com/tLd6Fe36Or— coreteks (@coreteks) September 4, 2020""They [AMD] want to release the 16GB at $599 and the 8GB at $499 but after the Ampere announcement I'm expecting $549 for the 16GB and $499 for the 8GB. We're receiving the ASIC (GPU+MEM) this month [September]"" – AMD partner pic.twitter.com/tLd6Fe36Orpic.twitter.com/tLd6Fe36OrSeptember 4, 2020So, the situation AMD currently finds itself in is: they either have to beat Nvidia on performance for each newly announced card or, they have to beat them on price (perhaps both). Until we get the third-party benchmarks, we’ll have to take Nvidia for their word that they appear to be the king of the hill.This new arms race between AMD, Nvidia and Intel has really heated up during the last few years, especially with AMD Ryzen taking the gaming world by storm. The winner in any hardware race is ultimately, you, the consumer.Wait for AMD to announce Big Navi and see if they have any tricks go their sleeves.",753
The Underrated Benefits of Working From Home,"When it comes to working from home, the same benefits are often touted like saved money, reduced commute times, lack of in-office distractions. But, there are benefits which don’t often get mentioned as much as they should.Only one car neededOnly one car neededPrior to the pandemic like many working families, we had two cars. When the pandemic set in and I went from 3 days remote to 5 days remote, our second car sat unused collecting dust (and bird droppings). Going forward I knew going into the office would be a rare thing, so we sold our second car and put it towards our renovations.Never miss packagesNever miss packagesPrior to only sporadically being at home, I would often miss packages. Because I wasn’t always in the office either, getting them delivered to the office wasn’t exactly an option either. Since working from home full-time, I’ve never missed a delivery.No after work drinksNo after work drinksDepending on the industry you work in, you might be familiar with the concept of after work drinks where everyone goes out usually on a Friday and has a few drinks together. I love a good beer or single malt as much as the next drinker, but not everyone drinks and it can get expensive (especially here in Australia).You get sick lessYou get sick lessIf you work in an open plan office, chances are you’ve experienced at least one wave of office sickness. Sadly, not everyone thinks to stay home when they are sick and in open offices especially, sickness runs rampant like colds and flus. I’ve been working full-time remote and haven’t been sick once in that time (and I have kids in kindergarten).More time for exerciseMore time for exerciseWhen you go into an office, unless you wake up super early, working out can often be impossible and by the end of the day, you’re usually exhausted and easily excuse yourself from exercising. Working from home means no commute, so more time to exercise before work or even on your lunch break.More emphasis on important communicationMore emphasis on important communicationAs a developer, I have experienced my fair share of distractions and they’re usually in the form of meetings which are mostly pointless or do not really benefit me in any way. Being remote means that communication goes from being this always-on thing to only being a scheduled and crucial thing, so people can think about who needs to be involved and in what.Reduced expendituresReduced expendituresWorking in an office (unless you have free meals and an in-house barista) you’re more inclined to buy takeaway coffee and eat out at lunch. Here in Australia, you can pay $5 for a takeaway coffee and $20 for a cheap takeout meal from a Thai place.More time with your familyMore time with your familyThe pandemic struck at a crucial time in the life of my family, my daughter who is now 1.5 years old was growing up and I have been fortunate to not only see her first steps working from home, but seeing her play and interact. I also get to speak with my wife and watch my kids play, it’s a nice change and the moments I usually would miss, I don’t.Pants are optionalPants are optionalWhile I am a big proponent of getting ready like you’re going into the office, some days you just want to be comfortable. I don’t mean sit at your computer completely naked (unless you want too), but you can wear shorts or even sit in boxers if you want. Even if you do a video call, people can’t see below your bottom half (unless you show them and please don’t).",871
"Possible Fix For AMD PC Not Booting, Single White Line Showing (bios code 99)","I recently just built a new AMD Ryzen 3900x gaming PC and I moved my tower to a new location. Everything had been working for days and after moving, I reconnected everything and turned it on. The computer made a “beep” noise, but wouldn’t go into the post screen.Naturally, my first instinct was to freak out. Had the motherboard, CPU or power supply died? Maybe I got a dud board?The debug LCD display inside was showing a code of 99. If you Google what a bios error code of 99 is, you get a wide range of probably causes for this error. It’s a code that is telling you that the computer is having a problem booting (well, thanks for stating the obvious).The code 99 is not technically an error code. It’s just showing you at what point of the boot process your computer is at. It just so happens if the computer doesn’t go beyond 99, it’s having an issue booting.After I accepted the fact my new board might be a dud, I decided to try the tried and tested debug process, disconnecting every USB device. Well, that worked.Slowly, I started plugging everything back into my PC and it continued to work. After some investigation, it turns out my Nintendo Switch Pro Controller was the culprit. I have no idea why my Switch controller was stopping my PC booting, but disconnecting the cable fixed it.Funnily enough, I only had my controller plugged in to charge. I usually use my controller via Bluetooth. So, if you encounter this issue, don’t freak out, disconnect every USB device and see if that helps.",376
How To Create Your Own Microsoft Flight Simulator 2020 Scenery & Landmarks,"While Microsoft Flight Simulator 2020 has been about barely a couple of weeks now, the community are already fast at work on making MSFS 2020 so much better. One area where the community and modders have really stepped up is fixing scenery and landmarks in the game.It turns out that Bing Maps is not as comprehensive or as detailed as Google Maps, so some landmarks are missing and existing buildings improperly converted into 3D models using Blackshark.ai’s 2D to 3D generation artificial intelligence algorithm.In fact, at the time of writing this, prominent and well-known landmarks are completely missing from MSFS 2020, including the Sydney Harbour Bridge in Australia. Fortunately, someone in the community added it in and smashed it out of the park. the Sydney Harbour Bridge in Australiasomeone in the community added it inYou can find many completely additions from the community here in the /r/FS2020Creation subreddit and some of the work done here will blow you away.here in the /r/FS2020Creation subredditFortunately, someone in the community has shared how they go about creating new scenery and landmarks in Microsoft Flight Simulator 2020, but it is time intensive, so make a coffee first.

In the above video from Flying Theston, you’re guided through the process of exporting photogrammetry model data from Google Maps and importing it into Microsoft Flight Simulator 2020. Of course, it’s not a simple export and then import.The amazing thing about MSFS 2020 is the SDK allows this level of customisation. The ability to replace segments of the game itself with more realistic scenery and buildings, the community are filling in the blanks and quite fast as well.Birmingham, United Kingdom scenery from: https://www.reddit.com/r/FS2020Creation/comments/im0fw8/birmingham_uk_scenery_initial_release_google_maps/When I get a moment, I want to try my hand at exporting some data from Google Maps and creating my own scenery in MSFS 2020. For now, I’m just trying to remember all of the keyboard shortcuts to fly a plane using a keyboard and mouse.",516
Tony Hawk’s Pro Skater 1 + 2 PC review: The perfect remaster,"When it comes to remastered games, they can be hit and miss cash grabs designed to get gamers to part with their hard-earned cash from their wallets by tapping into the part of your brain that longs for nostalgia.Tony Hawk’s Pro Skater is one of those iconic game series that thirty-somethings like myself played as kids. My friends and I used to stay up all night playing this on the Playstation. When I heard they were remastering this game for its 20th anniversary (jeez, when did I get so old) I knew it was an instant purchase.The first thing that stands out is how modern the game feels. THPS 1 and 2 are old games by gaming standards, but it looks like it was made in 2020, albeit without some of the bells and whistles that companies put into their games which just waste your CPU and GPU (err hmm, Flight Simulator 2020).On your first load you’ll be asked to calibrate your screen, similar to what GTA V asks of you to ensure light and dark colours are properly contrasted in the game. You will also be asked to accept privacy and terms of use on separate screens (something you don’t see much these days).Choosing, “Skate Tours” will take you to a screen where you can choose from THPS 1, THPS 2 or Ranked & Free Skate.Naturally, if you’re long for nostalgia you’ll want to start off at the iconic warehouse level in THPS 1. If you played this game for as long as I did as a child, every memory of this place will come flooding back.Unlike other remasters I have seen, things have been taken to the extreme in THPS 1 & THPS 2 remasters. They didn’t just take the old game and redraw some of the assets, it looks like they completely redesigned it and retained the original levels as well as 80-90% of the original soundtrack (with some new modern tracks from the likes of Billy Talent and Machine Gun Kelly) into it.Fidelity wise, the developers have taken the essence of the original games, what made them fun and great and modernised them without making the game feel sterile or too modern. The layouts are almost identical to what they were in the original games, to the point where muscle memory kicks in. There are subtle differences, but nothing that is immediately obvious.tooOne of the coolest features of the remaster is the updates they’ve made to the characters in the game, they look like the characters they’re modelled on. All of the original skaters are there, except they’ve been remodelled and reflect their current age. The young Tony Hawk from the original games is now middle-aged to reflect his current age.Instead of being a direct 1:1 remaster, they not only added new music into the game to compliment the iconic soundtrack of the originals (Superman by Goldfinger is a must-have that is in the remaster), but a bunch of new younger skaters have also been added into the game including non-binary skater Leo Baker. Of course, Tony Hawk’s son Riley Hawk is also in the game.Not only is everything from the originals present, but reimagined for modern gaming, it feels just as accessible as it did all of those years ago. Even players who didn’t experience the originals will find the remaster somewhat easy to pickup, but hard to master.Furthermore, they also added in a multiplayer mode which will pit you against other players and you can choose from competitive and jams mode which will change who you’re up against.The multiplayer experience is seamless and fast. Once you find a game, you’re loaded straight in. Unlike original multiplayer modes, you can continue to skate while you wait for other players to finish after the timer hits zero and waiting for a new game mode to begin.A multiplayer game without a constant lobby in 2020 is surprisingly refreshing. You can just keep on skating as the multiplayer modes cycle through on a playlist. I had a blast playing graffiti mode, although, I was focusing on writing this post and less on winning.Games appear to be capped at eight players and as far as I could see, you cannot choose the game mode you want to play. The modes appear to be random, but admittedly, it does make it more fun not knowing what is coming up next.After the disastrous release of Tony Hawk’s Pro Skater 5 in 2015 which saw it get smashed by reviewers and gamers alike, the remaster is almost a redemption that makes up for the terrible game they released almost six years ago that saw no follow-up.which saw it get smashed by reviewers and gamers alikeAfter seeing what they have achieved with this remaster, it makes me wonder if he’ll see any additional remasters for Tony Hawk’s Pro Skater 3, Tony Hawk’s Pro Skater 4 and the underground series THPS games as well? Given the remaster has seemingly received widespread praise from reviewers and gamers alike, it seems like a no-brainer.This is arguably the best remaster I have ever seen and I say that as someone who was a huge fan of the Command & Conquer Remastered Collection released a few months ago. While 2020 has been a terrible year, it has been a great year for remastered games.",1252
Using PowerShell In Visual Studio Code,"I recently built a new PC and did a clean Windows 10 install. As a result, I had to reinstall VSCode and everything else. I use PowerShell as my prefered shell in VSCode and encountered an error I sort of remember from when I did my last fresh install.The error in my case reads File C:\Users\dwayn\AppData\Roaming\npm\yarn.ps1 cannot be loaded because running scripts is disabled on this system. For more information, see about_Execution_Policies at https:/go.microsoft.com/fwlink/?LinkID=135170.File C:\Users\dwayn\AppData\Roaming\npm\yarn.ps1 cannot be loaded because running scripts is disabled on this system. For more information, see about_Execution_Policies at https:/go.microsoft.com/fwlink/?LinkID=135170Your error might differ from mine, but I was trying to globally install Yarn via Npm and then run it. To fix the issue, it’s quite simple.Open up a PowerShell terminal and run it as an administrator (right-click, run as administrator) and run the following command:Set-ExecutionPolicy –ExecutionPolicy RemoteSignedSet-ExecutionPolicy –ExecutionPolicy RemoteSignedIt’ll give you a warning and ask if you want to change the execution policy, enter A to answer yes to all. That’s it, that should now all you to run Node packages that have been installed globally and fix any other issues around the building of packages.",333
How Much Will the New Nvidia 30xx Series Cards Cost in Australia?,"Well, Nvidia just well and truly dropped the microphone with its announcement of the Nvidia RTX 30xx series cards. One of the biggest announcements of all is the lower tier of the new cards the RTX 3070 will allegedly have double the performance of the current-gen flagship 2080 Ti and is only $499 USD.In Australia, we pay more for technology in general. After the conversion, GST (government tax on goods and services), people are probably assuming the price won’t work out to be too nice for us Australians.Well, I was surprised when I saw that the new cards from Nvidia are not going to be that expensive. The Nvidia site states the 3070 will start from $809 Australian dollars. Of course, this price doesn’t factor in markup from the retailers themselves or any other price bolt-ons.  are not going to be that expensiveThe 3080 is being listed as starting from $1139 and finally, the flagship powerhouse 3090 is listed as starting from $2429. We will have to wait for the launch to see which retailers stick close to the MSRP. There are also third-party cards which could be cheaper options as well.Looking at the prices, it appears that maybe these Australian prices do factor in taxes. The RTX 3080 is $700 USD which at the current exchange rate is $963 AUD, which means almost $200 for taxes and whatnot.Considering the 3070 is meant to be twice as fast as the 2080 Ti, this would make it over half the price and an absolute steal of a deal if you’re not concerned with the higher tier cards. I would probably save your pennies for the 3080.It’ll be interesting to see what AMD reveals with its new Big Navi architecture, the ball is in their court now that Nvidia has well and truly dropped a bomb on them.",429
The Future of Passwords Is No Passwords,"Passwords are insecure. Even as password awareness has increased and password practices have shifted towards creating stronger passwords and using password managers such as 1Password, a password is only as good as its complexity, uniqueness and security of the application or website.You can enforce stronger passwords, but you can’t enforce uniqueness. Password rules just make people change their passwords from: password123 to password123! — I’ve educated my dad on passwords, but he was notorious for creating basic passwords as most of our parents are.And even if you’re a good conscious internet citizen and you generate a new lengthy password for each account you create, if the site gets hacked, it doesn’t matter how strong the password is if it is stored in plaintext.Two-factor authentication has started to become the new normal, with many companies and products now enforcing two-factor authentication as well as companies requiring their employees to use two-factor authentication for their email and other services.How does passwordless authentication work?Chances are you have already used a web service or app that used passwordless authentication. The most popular form of passwordless authentication is email, specifically, magic links.A magic link is emailed to you and the URL contains a special token which expires after a few minutes. Clicking this one-time link will log you straight into the service you’re using.Another implementation of passwordless authentication is done via SMS where you’re sent a code (akin to a one-time password), and you enter that as the password.Popular authentication provider Auth0 actually offers passwordless authentication, but you have to pay for it. There are other providers, but I haven’t used them before.Auth0Is passwordless authentication secure?Really, passwordless authentication is nothing more than an extension of the password reset model. The approach is the same, instead of resetting your password, the password is removed and instead of a password reset link, it’s a login link.Traditional password or not, you could argue even though the attack vector for magic-link based passwordless authentication is the user’s email address, if someone got their email hacked, someone could reset their password anyway (amongst other things).There is always the chance that the email server could be compromised on the application side because email is rarely encrypted and is sent as plaintext which can easily be sniffed if someone got in the middle. But, you could argue the same for password reset links and confirmation signup emails.Time and time again, as we hear about a new data breach every other week, a future in which passwords are taken out of the equation could mean hackers focus their efforts elsewhere instead of attempting to compromise databases looking for plaintext passwords or weak password hashing techniques.Going 100% password-free will obviously never be a viable option, your email account, for example, will always need a password. But, only having to remember one single strong password for your email is better than reusing one strong password for everything or weak passwords that are easy to hack and guess.Admittedly, I still haven’t committed to passwordless authentication in my personal projects yet because of the time and cost required to do so. But, eventually, it’s a priority for me to make the transition and move over to passwordless wherever possible.",865
How to Fast Launch Microsoft Flight Simulator 2020 (decrease game loading time),"One of the most frustrating and annoying features of Microsoft Flight Simulator 2020 is waiting for the game to load past the splash screen logos (which take forever), only for it to go to a pointless screen that tells you to, “Press Any Key to Start” which then makes you wait for the game to load again.It turns out that Microsoft Flight Simulator 2020 has a fast launch flag you can specify which will skip the publisher and partner splash logos at the start as well as remove the background video. The game takes seconds to load.Create a new shortcut on your desktop by right clicking, choosing create new shortcut and then pasting in the following value.C:\Windows\System32\cmd.exe /C start shell:AppsFolder\Microsoft.FlightSimulator_8wekyb3d8bbwe!App ""-FastLaunchThis is what the press any key screen looks like with fast launch enabled.Sadly, this will not bypass the, “Press Any Key to Start” screen, but it will remove the CPU-eating video in the background and reduce the time it takes for the game to load. Hopefully, Microsoft in a future patch allows us to bypass the press any key screen. I wonder if the SDK allows us to bypass this screen and a community mod can be created?",298
Nintendo posts a Q1 Profit of 427.7% (we should have bought Nintendo shares),"Recently announced Nintendo financials reveal that Nintendo has posted an operating profit of 427.7% for the quarter of April to June 2020. In that single quarter, they made 144.7 billion yen ($1.4 billion USD). To put that into perspective, the same quarter in 2019 they made a 10.2% profit. Nintendo has posted an operating profit of 427.7%It makes sense considering for most of 2020 thus far, many of us have been cooped up in our houses and gaming is a great source of entertainment, especially if you have children and looking for family-friendly games. Nintendo has become the soundtrack to the pandemic, especially in our household.It is a similar story for Sony who broke a bunch of records with profits and revenue for Q1 2020 as well. People are turning to games to escape the grim pandemic reality we all find ourselves living in.is a similar story for SonyAll I know is, I should have bought some damn Nintendo stock a few years ago when the Wii-U wasn’t the success they had hoped and their stock price wasn’t as high as it is now. As the saying goes, always bet on Nintendo.",272
How To Downgrade Windows 10,"Long story short, I needed to downgrade from Windows 10 Pro to Windows 10 Home and as I discovered, it’s a bit of a tricky situation that requires editing the registry. If you want to downgrade from within Windows using the Media Creation Tool and not have to create a USB key, this is what you will have to do.In my situation, attempting to run the Media Creation Tool resulted in some error about not being able to upgrade (even though I was trying to do a fresh install). The following steps worked on my machine and should work on yours as well.Obligatory disclaimer: Follow these steps closely and back up your registry. This is where the settings for Windows live and messing them up can result in unintended issues.Obligatory disclaimer:Open up regedit, go to the start menu and type regedit and hit enterNavigate to the path HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersionChange the CompositionEditionId to CoreChange the EditionID to coreChange ProductName to Windows 10 Home EditionOpen up regedit, go to the start menu and type regedit and hit enterregeditNavigate to the path HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersionHKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows NT\CurrentVersionChange the CompositionEditionId to CoreCompositionEditionIdCoreChange the EditionID to coreEditionIDChange ProductName to Windows 10 Home EditionProductNameWindows 10 Home EditionNow, you have to navigate to HKEY_LOCAL_MACHINE\SOFTWARE\WOW6432Node\Microsoft\Windows NT\CurrentVersion and make the same changes you made above by repeating the steps to ensure values here are the same.HKEY_LOCAL_MACHINE\SOFTWARE\WOW6432Node\Microsoft\Windows NT\CurrentVersionNow, you can run the Media Creation Tool and you’ll be able to do a new install of your desired Windows 10 version.",451
File Upload Progress With The Fetch API Is Coming,"The Fetch API might seem new to some, but it’s already five years old with browsers rolling out support in 2015. It was a replacement for the dated XMLHttpRequest which was tedious to work with and a relic of the yesteryear of internet.Still, all of these years one feature that is available in XMLHttpRequest is the ability to handle file upload progress, missing from the Fetch specification and we are now in the year 2020, almost 2021 and still, there is no across the board way to handle file upload progress. This is the last missing piece before Fetch can truly replace XMLHttpRequest.Coincidentally, shipping in Chrome 85 is support for Fetch upload streaming as an experiment. With Fetch upload streaming, you’ll be able to make a Fetch request with a ReadableStream body. You might have read Jake Archibald’s post on streaming in Javascript back in 2016, it’s a great read if you’re not familiar with streams. Jake actually works on the web standards team over at Google.Chrome 85Fetch upload streaming ReadableStreampost on streamingUnderstandably, some developers are still learning standards features that were part of ES2015 and subsequent releases, this is another lump of hardwood on the fire (so-to-speak).Explaining all of this, it sounds rather complicated and convoluted. Admittedly, streams are another notch in the complexity belt for Javascript developers. And look, I am not going to sugarcoat this, there is a lot to learn here if you plan on creating your own streams.The Google team have created a demo showcasing how you can stream chunks of text to a server here, showing you how Fetch upload streaming works, admittedly, this is one of the more simple examples. You need to have it enabled and using a compatible version of Chrome for it to work.hereYou can also reference the w3c-test site to see when Fetch supports ReadableStream objects and the different types of data being passed into them along with the request here.hereLike all complicated things on the web, frameworks and libraries will come to the party and make this easier to do. But, while this is all new and still very experimental, you’re going to have to roll up your sleeves and work with streams. Given how powerful they are, I recommend learning them.By default, Fetch upload streams will be HTTP/2 only and the Fetch request will be rejected if it’s not being done over a HTTP/2 connection. To support Fetch upload streams over HTTP/1 you have to explicitly enable this behaviour. However, be aware that allowHTTP1ForStreamingUpload is a Chrome only feature and not part of the standard.allowHTTP1ForStreamingUploadfetch(url, {
  method: 'POST',
  body: stream,
  allowHTTP1ForStreamingUpload: true
});fetch(url, {
  method: 'POST',
  body: stream,
  allowHTTP1ForStreamingUpload: true
});Sadly, it seems we are still a long way off all major browsers implementing support for Fetch upload streams, but the fact we are finally starting to see some traction in this area in itself is exciting. This has been a long time coming.",757
Nvidia Announces New Monster Gaming Graphics Cards (RTX 3080 and RTX 3090),"But, can they run Microsoft Flight Simulator 2020 at its highest settings?The Nvidia RTX 3080 starts at $699 and the performance is being promised as twice that of the prized gaming graphics card the Nvidia RTX 2080.Nvidia RTX 3080If that’s not good enough for the avid hungry gamer, Nvidia also announced a new monster RTX 3090 which is double the price of the RTX 3080. The difference being, the RTX 3090 promises to support 8k gaming, nevermind the fact most people are not gaming at 4k in 2020.RTX 3090The RTX 3090 is a massive card, not just in price, but also in physical size. It measures 313mm in length, compared to 285mm for the RTX 3080. It will also take up 3 slots worth of space, in comparison to the average of 2 slots. This means you’ll probably want to consider a full-size tower case instead of a mid-size tower.We are still a few years away from 8k gaming being a thing, in my opinion. We are only now just seeing gamers begin to embrace 4k gaming and even then, most people don’t have the machines or budgets capable of true 4k gaming.Can the RTX 3080 and RTX 3090 run Microsoft Flight Simulator 2020 at its highest settings?Can the RTX 3080 and RTX 3090 run Microsoft Flight Simulator 2020 at its highest settings?This is the one question I have. MSFS 2020 has shown in benchmarks that right now, you cannot achieve a smooth 60fps at ultra settings on hardware currently available on the market. Considering the performance is doubled compared to the 2080 and the 3090 looks like an excessive beast, maybe it’s possible now?Another issue with MSFS 2020 is that it has also shown it is CPU bottlenecked and can only utilise a maximum of 4 cores (until a future update and presumably Direct X 12 upgrade changes that).The RTX 3080 looks like a good buy, but unless you’re a die-hard GPU fanatic who wants to get the most frames possible out of your gaming experience or you’re doing GPU intensive activities, the RTX 3090 is probably beyond the budget for many, especially considering the price of the RTX 3090 is that of a decent gaming PC build.Even so, this, in theory, should see the price of the RTX 2080 and RTX 2080 Ti come down substantially in the coming months. You don’t need a 30XX Nvidia card, the 2080 is still a mighty fine option if you’re looking to save some money for more RGB or a better case.However, buried in the lede here is Nvidia announced an RTX 3070 card priced at $499. If you’re a 2080 Ti buyer, you would be understandably pissed off right about now, Nvidia is saying the RTX 3070 is faster than the RTX 2080 Ti which sells for 3 times the price.RTX 3070You can see eBay is being flooded with RTX 2080 and RTX 2080 Ti cards as we speak. Buyers are attempting to recoup some of their money before the market completely collapses and the prices plummet.While Nvidia can throw fancy graphs and performance metrics at us all day long, the real story will be told during benchmark testing. Have we reached a point in hardware where the CPU is the bottleneck again? We’ll have to wait until the cards are released to see the true numbers, but we appear to be witnessing a new generation of graphics cards being born.",790
The Ethics of Web Scraping,"Not that kind of scraping.If you’re a developer, chances are in your career you’ve written a web scraper before. You either did it for a personal project, learning exercise or you were asked by someone else to build a scraper. My first freelancer job was a scraper on Freelancer.com years and years ago.There is often confusion around the legality of scraping and it’s not illegal to scrape public data. This was further reinforced by the case of hiQ vs LinkedIn which ruled it is legal to scrape public data.hiQ vs LinkedInLegality aside (because we already established it’s not illegal), let’s talk about the ethics. As a scraper, you’re not obligated to abide any of these ethics of scraping, but it is the right thing to do.Furthermore, when it comes to scraping: it is a two way street. It’s unethical in my opinion for sites to intentionally hamper the ability for people to scrape their public data. Website owners need to accept that scraping can and will happen.If there is a publicly available API, you should use that instead. The only exception to this rule is when the provided API is lacking in features or places unfair limits on the data you can obtain.Adhere to the robots.txt file. Respect what it allows and disallows.Take only what you need. If you only need text on a recipe website, but not the imagery, just take that. Scraping data en mass and then throwing a lot of it away is wasteful and in poor taste.Don’t DDoS the site you’re scraping, rate limit your scraper and the number of connections (even if it means it’ll take longer). Hitting a site too hard could get you blacklisted as it might look like a DDoS attack.Provide attribution. If you’re using data from a website, don’t pass it off as your own.Identify yourself to make it easier for site owners to reach out if there are any problems and always be willing to talk and negotiate.Scrape outside of peak hours. This goes hand-in-hand with not DDoS attacking the site you’re scraping. Scraping late at night and early hours of the morning (where possible) will mean you won’t degrade the experience for others.If there is a publicly available API, you should use that instead. The only exception to this rule is when the provided API is lacking in features or places unfair limits on the data you can obtain.Adhere to the robots.txt file. Respect what it allows and disallows.robots.txtTake only what you need. If you only need text on a recipe website, but not the imagery, just take that. Scraping data en mass and then throwing a lot of it away is wasteful and in poor taste.Don’t DDoS the site you’re scraping, rate limit your scraper and the number of connections (even if it means it’ll take longer). Hitting a site too hard could get you blacklisted as it might look like a DDoS attack.Provide attribution. If you’re using data from a website, don’t pass it off as your own.Identify yourself to make it easier for site owners to reach out if there are any problems and always be willing to talk and negotiate.Scrape outside of peak hours. This goes hand-in-hand with not DDoS attacking the site you’re scraping. Scraping late at night and early hours of the morning (where possible) will mean you won’t degrade the experience for others.The important thing to note here is, the whole entire internet is billed on scraping. Google built its business on scraping, news aggregators built on scraping, ticketing and booking platforms (when they were starting out especially) used scraping. Have you noticed when you paste a link into a Facebook or LinkedIn post, it scraps the thumbnail image as well as title? That’s scraping.",904
"The Internet Is Complaining the Playstation 5 Won’t Be Backwards Compatible With PS1, PS2 and PS3 Games","File this one under, “peak 2020” Some Sony fans online are upset that the Playstation 5 console will allegedly not be backwards compatible with Playstation titles prior to Playstation 4. This means any games released for the Playstation 3 and older might not be playable.
Ubisoft has shared Sony's PlayStation 5 will not feature backwards compatibility for PS1, PS2 and PS3 titles from a recent FAQ page.We won't be playing GTA IV, May Payne or the first Red Dead Redemption on PlayStation 5 for another generation, I'm afraid. pic.twitter.com/469w9m5P8I— VideoTech (@VideoTech_) August 31, 2020
Ubisoft has shared Sony's PlayStation 5 will not feature backwards compatibility for PS1, PS2 and PS3 titles from a recent FAQ page.We won't be playing GTA IV, May Payne or the first Red Dead Redemption on PlayStation 5 for another generation, I'm afraid. pic.twitter.com/469w9m5P8I— VideoTech (@VideoTech_) August 31, 2020Ubisoft has shared Sony's PlayStation 5 will not feature backwards compatibility for PS1, PS2 and PS3 titles from a recent FAQ page.We won't be playing GTA IV, May Payne or the first Red Dead Redemption on PlayStation 5 for another generation, I'm afraid. pic.twitter.com/469w9m5P8Ipic.twitter.com/469w9m5P8IAugust 31, 2020The information comes from the next-gen support page on the Ubisoft website. The triggering line for Sony fans has since been deleted, quite possibly because of the backlash it seems to have generated and it now looks like this.next-gen support page on the Ubisoft websiteSony has yet to comment on backwards compatibility and it is possible they might have asked Ubisoft to remove the offending line. The silence from Sony is rather telling, it’s clear backwards compatibility was or still is not a priority for launch.In all honesty, does it matter?In all honesty, does it matter?A small subset of players most likely only play older titles and rendering them on a newer 4k console might pose some problems as older graphics were not designed for modern resolutions. Many older titles would need to be updated or the console itself limiting the resolution to prevent pixelation.The approach that Sony has traditionally taken is advancing forward with new consoles and generation-defining games. Backwards compatibility has never been high on the priority list for Sony, in comparison to the Xbox which has great backwards compatibility.If you want to play Playstation 1, Playstation 2 or Playstation 3 games so badly, go buy one of those consoles. They are incredibly cheap to buy. The Playstation 1 will set you back around $100 Australian dollars.",648
Has Spotify Censored Joe Rogan?,"Well, it has barely been a day with Joe Rogan starting his new $100 million dollars multi-year deal with Spotify and already the move is already generating controversy.Eagle-eyed fans have noticed that there are numerous existing Joe Rogan podcast episodes missing from Spotify, perhaps most notably are the infamous and highly entertaining Alex Jones episodes. A Reddit thread has compiled a list of missing names and there are some recurring guests that come up.Reddit threadThe Chris D’Elia episodes are missing, the Alex Jones episodes and more bizarrely episode #463 with Louis Theroux is missing. A search for the Louis Theroux episode on YouTube also turns up empty, the episode appears to have been pulled.I got curious about episode #463 and a search seems to suggest that the episode talked about Scientology quite a bit and well, The Church of Scientology are notorious for threatening legal action against anyone who dare say anything negative about them.It is clear either Joe himself has selectively self-censored, or executives at Spotify deemed these episodes a problem. The Alex Jones episodes from a censoring perspective at least make the most sense, given Spotify was instrumental in helping get Alex Jones deplatformed a couple of years back.This whole situation would be a lot less concerning for JRE fans, if Joe had not stated all episodes would be available on Spotify as can be seen in this clip below announcing the move.

Reading through Spotify’s FAQ page, you can see some of its content policies around hate speech could be used to censor episodes and possibly some of the ones listed in the above thread.Spotify’s FAQ pageAlthough, given how many episodes Joe has recorded, I am having a hard time agreeing with the possibility that Joe Rogan went through his entire backlog and removed some of the episodes he didn’t like, he could have done that when it was on YouTube.This is the thing, even though Joe Rogan said he would retain control and the podcast wouldn’t change, he is beholden to Spotify’s content policies and they own him now they’ve given him such a large chunk of cash. Unless he has a contract with terms that state he can do whatever he wants, he can’t.Interestingly, in this news story, Joe Rogan states it is just a licencing deal and he retains full creative control over the show.he retains full creative control over the showIt’s just a licensing deal, so Spotify won’t have any creative control over the show. They want me to just continue doing it the way I’m doing it right now.It’s just a licensing deal, so Spotify won’t have any creative control over the show. They want me to just continue doing it the way I’m doing it right now.It will be interesting to see if Joe sees out his contract with Spotify — because I get the feeling if Spotify is the one already censoring episodes, eventually, Joe will want out unless it was always only about the money.",729
Facebook Threatens to Block Australians From Sharing News on Facebook,"As some might be aware, there is currently a news war taking place in Australia. The Australian Government alongside the ACCC (Australian Competition and Consumer Commission) have drafted legislation which will compel companies like Google and social media platforms such as Facebook to pay for news on their respective apps and sites.Google recently weighed in on the issue with an open letter to Australians and now, Facebook has done the same issuing a straight-to-the-point press release about the legislation.open letter to Australiansissuing a straight-to-the-point press releaseAssuming this draft code becomes law, we will reluctantly stop allowing publishers and people in Australia from sharing local and international news on Facebook and Instagram. This is not our first choice – it is our last. But it is the only way to protect against an outcome that defies logic and will hurt, not help, the long-term vibrancy of Australia’s news and media sector.Assuming this draft code becomes law, we will reluctantly stop allowing publishers and people in Australia from sharing local and international news on Facebook and Instagram. This is not our first choice – it is our last. But it is the only way to protect against an outcome that defies logic and will hurt, not help, the long-term vibrancy of Australia’s news and media sector.TL;DR Facebook will stop allowing publishers and Facebook users in Australia from sharing news content on Facebook and Instagram if the proposed legislation is passed and introduced in its current form.While Facebook is a questionably non-transparent social media platform marred in controversy, they make some valid points about the value they bring to news organisations. Admittedly, a bit of my news exposure is through Facebook as I follow numerous news organisations and engage in conversation with other commenters.Over the first five months of 2020 we sent 2.3 billion clicks from Facebook’s News Feed back to Australian news websites at no charge – additional traffic worth an estimated $200 million AUD to Australian publishers. Over the first five months of 2020 we sent 2.3 billion clicks from Facebook’s News Feed back to Australian news websites at no charge – additional traffic worth an estimated $200 million AUD to Australian publishers. Not everyone is convinced that Facebook is doing this for noble reasons
They are so afraid of paying for news content that they’re prepared to damage their own business to avoid it.One third of Australians get their news from Facebook.If Facebook pulls news they will lose all that data harvesting, all that eyeball time. https://t.co/we0aF2UxU1— Belinda Barnet (@manjusrii) September 1, 2020
They are so afraid of paying for news content that they’re prepared to damage their own business to avoid it.One third of Australians get their news from Facebook.If Facebook pulls news they will lose all that data harvesting, all that eyeball time. https://t.co/we0aF2UxU1— Belinda Barnet (@manjusrii) September 1, 2020They are so afraid of paying for news content that they’re prepared to damage their own business to avoid it.One third of Australians get their news from Facebook.If Facebook pulls news they will lose all that data harvesting, all that eyeball time. https://t.co/we0aF2UxU1https://t.co/we0aF2UxU1September 1, 2020I disagree. Visit any of the major news websites (especially Murdoch media) and turn off your ad blocker. You will be greeted with so many advertisements and intrusive scripts you’ll hear the fan in your laptop spin up. This means traffic coming from Facebook and Google is providing these news sites impressions (and sometimes clicks).I distrust Facebook and Google as much as the next tech-oriented privacy-aware person, but if Facebook pulls support for news, this will only hurt publishers, not Facebook. This won’t damage Facebook’s business. If you are looking for a good read on the proposed draft legislation, Stratechery has a great write up here.hereI think quality news is worth paying for, but the reality is many of the Murdoch publications here in Australia are notoriously biased and right-wing. If it’s not biased political coverage, it’s trashy celebrity news or republications of news from other media partners (especially in the case of news.com.au).In my opinion, this legislation is blackmail. While I have zero sympathies for companies like Facebook and Google who could afford to take the hit and pay, it’s the ambiguities around publishers being allowed to make up their own numbers that are the concerning thing about all of this. Once again, this is the case of the government being asked to step in and legislate a dying industry because they can’t think of their own ways to make money. Ironically, the same government who refused to bail out the local car industry as it slowly died a horrible death and is now bailing out news organisations. It really echoes the early days of the NBN when the government built a sub-par network which benefitted Rupert Murdoch and slowed down the threat of internet streaming companies like Netflix to Foxtel as users stuck on slow internet connections for users.benefitted Rupert MurdochAustralian tech figurehead Mike Cannon-Brookes (of Atlassian fame) had a more levelheaded response to the news of Facebook pulling support for news.
The only logical move for Facebook to make. News doesn't make it much $ I'd assume. UGC is their business. No shortage of content. Media orgs will lose out of this in general. https://t.co/Zvt7sWXE9P— Mike Cannon-Brookes 👨🏼‍💻🧢 (@mcannonbrookes) September 1, 2020
The only logical move for Facebook to make. News doesn't make it much $ I'd assume. UGC is their business. No shortage of content. Media orgs will lose out of this in general. https://t.co/Zvt7sWXE9P— Mike Cannon-Brookes 👨🏼‍💻🧢 (@mcannonbrookes) September 1, 2020The only logical move for Facebook to make. News doesn't make it much $ I'd assume. UGC is their business. No shortage of content. Media orgs will lose out of this in general. https://t.co/Zvt7sWXE9Phttps://t.co/Zvt7sWXE9PSeptember 1, 2020Coincidentally I just finished watching Fear City: New York vs The Mafia on Netflix and I am seeing some familiar mob tactics in play here. The only reason the government is even getting involved here is because of the influence news organisations have in Australia, even as their numbers dwindle.I am sure the unofficial Prime Minister of Australia, Rupert Murdoch has a grin the size of a small island on his face right now.",1631
Is TikTok A National Security Threat Or Political Football?,"If you’ve been following any tech news for the last few weeks, you have probably heard about the TikTok situation unfolding in the US where Donald Trump via executive order is forcing a sale of TikTok in the US or banning it.To say that 2020 has been a wild and unprecedented year is an understatement. TikTok has been a source of entertainment for those staying at home as the COVID-19 pandemic closes down schools, states, countries and limits movement.It poses the question: is TikTok really a national security threat or just a political football the US can kick to punish China in its ongoing trade war as well as war of words over the origins of the COVID-19 virus which Donald Trump refers to as the “China virus”?The answer is as complex as you might expect.TikTok operates no different to that of other social media apps and platforms like Facebook and Instagram. In fact, independent research efforts seem to suggest that TikTok collects the same information as other apps do and might not even be the worse offender.Still, the association with the Chinese Communist Party (CCP) is a dark stain on the underpants of TikTok. Even as it attempts to distance itself from Beijing, it can’t shake the fact Bytedance (the company who owns TikTok) is a Chinese company and therefore, beholden to China’s laws.China’s National Intelligence Law 2017 is pretty clear in what it can compel individuals and companies to do including; access, cooperation or support for intelligence-gathering activities. While it is not known if the CCP has compelled ByteDance to hand over TikTok data which is stored in Singapore, the risk is very real.access, cooperation or support for intelligence-gathering activitiesIt does raise the question of what the situation would be like if a US company were being compelled by the Chinese government to sell off its assets or be banned in the country.In November 2019,  TikTok was forced to apologise to a 17-year-old American teen whose video about the detention of Uyghur Muslims in China went viral. Her account was deleted from the app which TikTok alleges was an accident. However, of all the content on its platform that could be removed, it is convenient it was this one viral video and account. whose video about the detention of Uyghur Muslims in China went viralThere has been talk of Microsoft being close to a deal to takeover TiTok’s US assets, with Walmart allegedly wanting a piece of the TikTok, because, why not? Allegedly, a deal being announced is imminent for TikTok’s US, New Zealand and Australian operations.Walmart allegedly wanting a piece is imminentHowever, there has been a spanner thrown into the works. As ByteDance reportedly concedes defeat and considers selling various TikTok assets, China has updated some of its export control rules which allow it to restrict the sale and export of Chinese technology. ByteDance has reportedly responded by saying they will “strictly” obey the laws (not that they have a say in the matter).restrict the sale and exportI am not sure if ByteDance nor any of the parties involved in the potential sale could have foreseen this coming. It does, however, go against the narrative that TikTok has been painting that it is not beholden to CCP interests and control. Let’s also not forget in 2019, the Guardian reported on leaked guidelines that allude to the CCP’s influence on certain topics such as Tiananmen Square and Tibetan independence.Guardian reportedI think it is evident that talk of banning TikTok has a strong whiff of politics. If the US were to go down this path, it would be a slippery slope seeing the US government compel a company to sell its assets and outright banning its presence, especially with the lack of strong evidence TikTok is anything more than a social media app.However, like all social media applications (including and not limited to TikTok) there needs to be some sweeping changes to hold social media companies to account, provide more transparency around data and how it is being used. TikTok is not alone in what it collects, but the waters are muddied by the China connection (which a sale would alleviate).",1035
Should We Be Concerned About Google’s Web Bundles Proposal?,"Google is well and truly on a roll lately. The Chrome 85 release is jam-packed with new features including the new content visibility property which I wrote about here. Something not many might realise is that Google has experimental support in Chrome 80 for Web Bundles.hereWeb BundlesWhat are Web Bundles? What are Web Bundles? Essentially, it is a new file format which allows resources in a web application to be compiled into a singular file. Think images, Javascript files, CSS and other resources you might find on a webpage. They can all be packaged into a singular file and work offline.In theory, it sounds cool, but in practice, you can see how this could be abused. When you load a .wbn file, you don’t see URL’s to resources but rather indexes which have no meaning and are not transparent..wbnRight now, they are hidden behind a flag you need to enable visiting chrome://flagschrome://flagsPrivacy researcher Peter Snyder recently published a scathing takedown of Google’s proposal and cited some valid concerns over the new Web Bundles proposal. Describing Web Bundles, Snyder uses a great analogy of a PDF file.published a scathing takedownWebBundles make Websites behave like PDFs (or Flash SWFs). A PDF includes all the images, videos, and scripts needed to render the PDF; you don’t download each item individually. WebBundles make Websites behave like PDFs (or Flash SWFs). A PDF includes all the images, videos, and scripts needed to render the PDF; you don’t download each item individually. Part of the concern here is being driven by how this new experimental proposed suite of features could be abused and specifically, used by advertising networks (Google itself is one) to make it impossible to block ads or tracking scripts which would essentially be obfuscated in the bundle.If you were thinking that Mozilla might also err on the side of caution here and consider this Google-championed proposal as potential harmful — well, it turns out that Mozilla is seemingly supportive of the standard, as made evident here.hereFortunately, there are people championing solutions to the problem of obscured/obfuscated resources inside of wbn bundles. Naming resources inside of bundles. Some of the concern is people could hide dangerous code like cryptocurrency miners and tracking scripts and they won’t show up in the network tab or be easy to block.wbnNaming resources inside of bundlesPrivacy implications aside, another concern is that web bundles will negate the one benefit that they provide besides blocking ads and trackers: saving bandwidth. The current proposal means bundles are shipped as a whole and do not allow you to ignore certain files or resources which might sap bandwidth (especially important on mobile).It is worth noting that some of these concerns are being taken into consideration and the use cases page for the emerging proposed standard does mention some use cases.use cases pageI see the value in the proposed set of standards, but the concerns surrounding them are very real. The scary thing is Google has already shipped experimental support behind a flag in Chrome, there is a very real possibility this could happen.As the standard currently stands, we should be concerned. Google has a conflict of interest as they stand to profit more than most if they can find a way to solve the adblocker problem which is a threat to one of the most profitable parts of their business. Where do we draw the line? This all smells of DRM-type thinking.While ads are just one part of the equation, we need to start holding Google to a higher standard than we currently do as they wield a lot of influence.",911
"Project Fugu, The Exciting Leap Forward For Web That You’ve Probably Never Heard About","Have you heard about Web Capabilities Project aka Project Fugu? Chances are, you probably haven’t heard about it (unless you’re reading this well into the future), but it is one of the most exciting initiatives in the web being undertaken right now. Spearheaded by Google, Microsoft, Samsung and Intel, there are some heavy hitters supporting this.Project FuguAt its core, Project Fugu is an initiative to bring native application features into the web to close the gap between web applications and native applications. This means that you do not need to use a wrapper to get access to native features, further blurring the line between native and web.As you can see on the Project Fugu status page, many features are already available in the latest version of Google Chrome. Some of the most exciting and already shipped features include:Project Fugu status pageShape Detection API – Quite possibly one of the most exciting shipped features, this API allows you to build web applications that can read QR codes and identify shapes and text in thingsWeb OTP API – One time passwords (OTP) are now quite common and this API makes it easier for web applications to implement OTP support using text messagesWeb Share API – Chances are, you have heard of this API. It allows web applications to show a sharing dialogue, which native users are already accustomed toContact Picker API – As the name implies, it allows web applications to access the contacts list, which has been a feature that native apps have always supportedPeriodic Background Sync API – Allow web apps to periodically sync data in the background like a native appShape Detection API – Quite possibly one of the most exciting shipped features, this API allows you to build web applications that can read QR codes and identify shapes and text in thingsShape Detection APIWeb OTP API – One time passwords (OTP) are now quite common and this API makes it easier for web applications to implement OTP support using text messagesWeb OTP APIWeb Share API – Chances are, you have heard of this API. It allows web applications to show a sharing dialogue, which native users are already accustomed toWeb Share APIContact Picker API – As the name implies, it allows web applications to access the contacts list, which has been a feature that native apps have always supportedContact Picker APIPeriodic Background Sync API – Allow web apps to periodically sync data in the background like a native appPeriodic Background Sync APIPerhaps one of the most exciting things about this project besides native features being made available in the browser is you (yes you) can suggest your own features you want to see in Project Fugu over on the tracker here. And the spreadsheet available here, you can get a visual progress update on what the status of certain features are.hereavailable hereWhile the ambitious project is still relatively new and browser support does not really extend that far beyond Google Chrome, things are looking promising for the future of the web and progressive web applications. Not having to use wrappers which can introduce performance problems is always a positive.",786
"Yarn 2.2 Update Released, But Is Anyone Even Using Yarn 2 Yet?","Browsing my various online sites for tech news, I came across an update for Yarn, a 2.2 release for the ill-fated Yarn 2 package manager which many will attest, has been a trainwreck of biblical proportions.a 2.2 releaseI know a couple of people who have attempted to migrate from Yarn v1 to Yarn v2 and given up in the process, opting to go back to Npm. Even though Npm might not be as cool, Npm has caught up on several fronts and really, the only reason developers chose Yarn in the first place was because of the performance improvements.Firstly, I just want to point out I am not taking a dump on the Yarn team nor am I saying that the people who have been working on Yarn 2 want it to fail or intentionally meant to make it so difficult to move to v2: it is what it is.I did get a good chuckle at the end of the update where the author states:We’ll try to make more regular minor releases from now on, shipping exactly one minor per month (eventually leading up to the release of Yarn 3 in January 2021).We’ll try to make more regular minor releases from now on, shipping exactly one minor per month (eventually leading up to the release of Yarn 3 in January 2021).Considering people have struggled to update to Yarn 2, the fact another major version is coming in what, four months? It does seem a little silly when you put that into context. But, it is good to have a release schedule and incrementally update your software.In defence of Yarn, they had the right idea, but the changes were so fundamentally different it echoes the mistakes the Angular team made when they announced Angular 2 and didn’t have a solid migration plan in place. People were not prepared and even though people have tried to upgrade, the failure rate seems high.There is a migration guide here, but it’s not a light read akin to someone shining a bright light in your face and screaming at you. There is also a plugin for enabling old style node_modules support detailed here.migration guide herenode_moduleshereInterestingly, Jest has migrated over to Yarn v2, but they had to enable the legacy node_modules support, because, clearly if they didn’t the migration would be a lot of pain for them. Maybe it is worth migrating and switching legacy support on, I’m not sure if a lot of developers even know you can do that.Jest has migratednode_modulesAfter the launch of Yarn v2, I gave up on it almost instantly. Now that v2.2 is out, I might see if I can upgrade an old Aurelia project I have lying around to Yarn v2 and switching on legacy mode to see if that clears any potential roadblocks.",645
An Honest Review of The Galaxycove Star Projector (read before you buy),"If you’re reading this, chances are you’ve seen the Facebook or Instagram ads for the Galaxycove Star Projector. The photos and videos are convincing, to the point where they lure you in. If you’re on the checkout screen or about to splurge that $80, keep reading first.This is a review for the Galaxy Cove projector purchased from their galaxycove.com website, not any knock-off site, the same site they advertise on social media and everywhere else.If you don’t want to read a novel, the TL;DR is RUN AWAY. Do not deal with Galaxycove, the projectors they sell are not worth the price they are asking. And if you want to know what kind of company you’re dealing with, read about how they handled genuine negative reviews on their Trustpilot page.read about how they handled genuine negative reviews on their Trustpilot pageBefore we continue, this review is for the Galaxycove v1 projector that comes in the unbranded box and looks like a cheap Chinese knock-off. They released an updated version a few weeks after I ordered mine and have changed the packaging and slapped a logo on the projector itself. Don’t fall for the updated branding, besides being able to choose some new colours, it’s the same projector.Even though I found reviews and videos praising the projector, part of me was sceptical that these reviews were not all completely honest. Information suggested that this company was selling cheaply made Chinese products at a premium price, but I didn’t look too much into it at the time. The marketing got me.It has been brought to my attention that Galaxycove are attempting to discredit reviews such as these. The claim is that people are buying from alleged “illegitimate” websites. Well, not that it matters much, but here is a screenshot of the transaction from my internet banking. Sure enough, I ended up buying one and used the excuse of buying it for my son. In reality, I was the one most excited about it because the marketing made this thing seem like it was amazing. It totalled just a little over $100 Australian Dollars after the conversion. This better be good, I said.It ArrivedWell, the first good sign this wasn’t a complete scam is the projector did arrive. The projector took a while to arrive in Australia, at about 2.5 weeks. The Galaxycove website states a week, but when you look at the tracking information, you see it comes from China and the pandemic has slowed everything down. The paths it took for delivery were unlike anything I had ever seen, it seemingly bounced a heap of places.Opening up the package, you’re greeted with an unbranded generic looking product. This looks like a product you would buy at a garage sale or find in your grandma’s basement beside the unused foot spa she got for mothers day 5 years ago.As you can see, nothing on this box suggests that it is a Galaxycove product whatsoever. The feeling that you’ve been “had” begins to set in. I was doubting if this was going to work. Sure, it might have arrived, but did they send me a projector slapped together wholesale for $3 and sold at a premium? We were about to find out.The obviously translated text from Chinese into English really instils confidence that you’ve just bought a carefully engineered product. You know when the packaging is bad, it’s a red flag, especially poor translations. I did have a good laugh at the line, “enpowder your life”Maybe there is branding inside of the box, you say? Nope. It just uses a generic name of 3 In 1 LED Starry Sky Projector Light on the instruction booklet. The booklet itself is quite useless, as is the case for cheaply made Chinese exports such as these. It is littered with typos and grammatical errors, indicative of poor and rushed translations.Putting the dots together, it was clear this was a wholesale product being sold at a heavily marked up price. I got curious, is anyone else selling this same projector? If it is being dropshipped, GalaxyCove can’t be the only ones doing this.If there is one thing in life I’ve learned, when it doubt, look on Wish. Sure enough, I found a projector that is a splitting image of the one I bought under a different name. Allegedly you can also find the same one on Aliexpress as well as other sites like eBay.a splitting imageAt this point, we haven’t even turned it on yet and there are red flags everywhere. The above Aliexpress image shows a projector that looks identical, right down to the placement of the USB charging port, compliance writing and where the projector dome sphere sits.In another image on the linked Aliexpress projector, even the colours it supports out of the box are the exact same as the Galaxycove colours. The moon and stars look the same, as does the placement of the buttons and their functionality.It is quite clear the Aliexpress projector is the same, at a fraction of the price. I might even buy it and then do a comparison between the two, even taking them apart and seeing how they differ from the electronics inside (they probably don’t).Weird Website BehaviourHere is one sign that the Galaxycove company is not who they say they are, the weird and dishonest tricks they attempted to pull on their own website shocked and surprised me. I have never seen any other site do what the Galaxycove site does.They have made all attempts to not only right-clicking, but they have also disabled any attempts to open up browser developer tools. If you are using Chrome, give it a try yourself. It’s clear they do not want anyone viewing their website source code, copying their images or text.Try right-clicking anywhere on the site, it doesn’t work. Then try selecting any of the text on the site such as the product page here, text selection has been disabled, which is just weird.hereI still managed to get developer tools open and then encountered another trap attempting to block you from using your browser developer tools. The Galaxycove website has a timer running every 1000 milliseconds (every second) with a debugger statement being called. A debugger statement signals to the developer tools to break on that particular line of code, it’s an attempt to disable the standard behaviour of the developer tools.setInterval(function(){ debugger; },1000)setInterval(function(){ debugger; },1000)The code above is an attempt to prevent people from viewing their code, which just made me even more curious, what are Galaxycove trying to hide on their website? There is nothing proprietary here, just some basic HTML, CSS and Javascript for what is a product website.It turns out that Galaxycove is hosted on Shopify and they are using a Shopify addon called Cozy AntiTheft. The thing is, the plugin doesn’t even do what it says it does. It prevents text selection and keyboard shortcuts, but you can still open developer tools.Cozy AntiTheftWhile I did not uncover any hidden secrets or tricks in the website itself, I did find it odd they installed such a plugin for their Shopify store.Fake product imagesThe website shows the projector itself should have a Galaxycove logo on it. Upon closer inspection, it appears the text was added after the fact.Taking the projector out of the box, one thing stands out immediately: no logo is present on the product itself.The website images were edited mock-ups, not uncommon even for reputable companies, but they literally Photoshopped their logo onto the images to make it look like a legitimate product. The projector obviously has no logo that comes in the box. This is intentionally misleading now.Another annoying thing is that it ships with a USB adapter with an American plug. Obviously, being in Australia, an American wall outlet is useless to me. Fortunately, I had a spare phone charger adapter lying about I could use.Mystery SC511Upon closer inspection, I noticed on the back of the projector there is a model number SC511 which once again seemed odd. What would happen if I Googled that? As it would turn out, a whole bunch of identical projectors show up using that model number, including a few on Amazon. This one here shows the same product.hereInterestingly, this listing details some specifics down the bottom.Galaxycove at every turn are trying to make themselves appear to be a company they are not. Not only do they claim they design and build their products themselves (calling themselves light experts, amongst other titles), but they portray themselves as the Apple of light products.More Fake ImagesThe Galaxycove hole digs deeper and once again, Galaxycove employs their favourite trick: fake images. This is the Galaxycove about us page where they show stock-looking photos of designer-looking people working on computers. They talk about being light experts and speaking as though they painstakingly designed and architected this themselves.the Galaxycove about us pageThat is because the images on their about page are stock photos. I took a snippet of the photos and then did a search on TinyEye and sure enough, those people do not work at Galaxycove. Look at this link, it’s the same picture being used as the one on the Galaxycove about us page.this linkThis is the image from the Galaxycove about pageAnd this is the one from the website where I found the same stock photo in use.If you look closely, you will also notice they put a weird blur over the mockups on the screen in their image. The reason they did this was most likely an attempt to cover up the fact they edited the image.Fake ReviewsThe clue in Galaxycove being a semi-legitimate drop-shipping operation is the botted reviews. Looking on the product page, there are way too many 5 star reviews, but there are cleverly some 3-star reviews as well (but they do not say anything critical about the product itself).Funnily enough, I left a review of my own and I waited for it to show up on the site. It never showed up. It’s clear they only show the highly-rated ratings, possibly delete anything critical of their drop shipped projector and portray the image of a loved company shipping a great product.The real reviews for this product are over at TrustPilot, where you can imagine the real story is being told. Allegedly whoever is running this company keeps changing their name, they were previously known as Luxavate.over at TrustPilotThe upside of all of this is that it wasn’t a complete and total scam. I mean, the projector arrived and when I queried them about the delay, they responded quite fast and apologised. They definitely try and make people perceive they are a legitimate business selling premium products, but the blatant fake reviews, weird return/refund policy and fake about page are red flags.Does it work?In my case, yes. We took it out of the cheap generic-looking box, we found a spare phone charger and plugged it in and waited for it to charge.The projector worked, it didn’t throw light as much as the smart misleading marketing would have you believe or all of the 5 star, “omg this is amazing”, “my cat loves this laser lol” reviews clearly left by fake/paid accounts. Is this worth $80 USD? Definitely not. For me, it ended up being over $100 and I have zero confidence this laser will last any more than a few months.If your projector does work, it won’t impress you. Once you realise the colour choices are limited, that the music sense mode will most likely not work and that the amount of light being projected isn’t nearly as much as the marketing materials depict, you realise this is a waste of money.My five-year-old-son seems to love it and is currently using it as a nightlight. That’s one damn expensive nightlight. And look, there is nothing wrong with dropshipping, but masquerading behind a business front and being dishonest as well as charging top prices for products worth half what they’re charging, it’s one of the many reasons to avoid Galaxycove.When I get a chance, I am going to open this projector up and see what the build and component quality is like on the inside. I am expecting some serious shortcuts lurking within, the inbuilt rechargeable battery is what scares me the most.Would I advise you to buy one? Definitely not. You can allegedly pay half the amount and get the same projector elsewhere, do that instead. If you buy the Galaxycove projector, you will be disappointed.Would I advise you to buy one?",3066
Increase Web Application Performance Using content-visibility,"The web continues to advance forward and like a few other convenient CSS properties such as native image lazy loading, Google have once against shipped a new feature which I am hopeful will make it into other browsers shortly called content-visibility which is part of the CSS Containment Specification.native image lazy loadingcontent-visibilityCSS Containment SpecificationThe crux of this new property is you can tell the browser to ignore rendering of certain elements that are off-screen. Traditionally, when you load a webpage, it will draw it from top to bottom (regardless of what is visible and what isn’t), this is why virtualisation techniques are popular (especially for long grids).As you can see in the handy reference image provided by Google, the browser will only draw the first few elements and then ignore the rest until it’s used. Rendering time is significantly reduced and therefore, increases your first paint as well as load time by a huge factor.A good use-case for content visibility is a feed. Think of a site like Pinterest or application like Instagram which shows a few images above-the-fold, but a bunch more below-the-fold. Why should the browser rendering everything, especially if the user doesn’t scroll beyond what is visible? (maybe they click the first item they see).Perhaps one of the coolest and most exciting features of this property besides speeding up initial render of your applications astronomically is being able to use it for hidden elements. Think of it as display: none; on steroids by being able to specify content-visibility: hidden which will hide the element and preserve its rendering state, if there are any changes that need to happen, they’ll only happen if the element is visible.display: none;content-visibility: hiddenYou’re probably excited, you want to rush out and make your websites and applications faster for your users; here is the kicker — This feature is only launched in Chromium 85, with Firefox prototyping the property but not committing to its implementation. This means Chromium-based browsers (Google Chrome, Microsoft Edge and Opera) will have support.When you see such useful and exciting features being shipped in Chromium, it makes you thankful that Chrome is the dominant browser with 65% of market share in July 2020 because it means you can use this feature today with great effect. Understandably, Mozilla has fewer resources than Google, so implementing new features like these into Firefox takes a little longer and requires more assessment before proceeding.65% of market share in July 2020The future of the web is bright.",653
Newsletters Are The New Startups,"As much as some people wanted to believe in Slack’s tagline that email was dead, it seems in 2020 that email is anything but dead. I observe trends in tech, I’m always looking for new ideas and startups to explore.One thing I have noticed these past few months in 2020 is newsletters are becoming big business.Instead of launching products, people are launching niche newsletters. And honestly, I’m sold. The past month alone, I’ve subscribed to more newsletters than I have my entire internet life.This newsletter called Patents Today sends you a newsletter with interesting patents. Another is called Mental Models, where they will send you an email with a new mental model and explain it to you in simple terms.Patents TodayMental ModelsAnd a shameful plug, I have even taken the plunge and started my own newsletter called The Ideas Digest.The Ideas DigestThere is a heap of these things. Product Hunt has a great collection of these newsletters here. Most of the newsletters I have joined were found on Product Hunt.What is most interesting about this new newsletter trend is that it is mostly being driven from Substack which is a genius platform that makes it easy for people to build communities built around newsletters. Think of Substack as a Tumblr meets Mailchimp type product where you can create newsletters and communities, even the ability to monetise with subscriptions.SubstackThe beautiful thing about newsletters is there is no barrier to entry, if you know something about a particular topic, you can create a newsletter and build a community for it. All that is required is knowledge and time, that’s it.Even if you don’t know anything about a particular topic, you can make it up as you go along and take your readers on the journey with you. Maybe you’re learning to code, perhaps you’re on a journey to become an authentic French cook. Journeys in themselves are interesting enough that people would want to follow them if they are consistent.To my knowledge, none of these Substack newsletters have made million-dollar exits nor had venture capital thrown at them. But, here is my prediction: newsletters are going to be big business going into 2021. The best thing is, you don’t need investment.Just like streamers on YouTube and Twitch can make a decent living, not-to-mention influencers on Instagram (even with small follower counts), newsletters with large followings can make a decent income and with very little outgoing costs.A couple of avenues that come to mind are advertisements, paid listings and referral links. Imagine building a newsletter audience to over one-million people, that kind of audience would give you access to businesses wanting to advertise. If you have built a niche audience, that is even more valuable and you don’t need one-million subscribers either.One of the easiest ways to monetise through a platform like SubStack is subscriptions where users can subscribe to your newsletters and access exclusive members only content.A niche newsletter with 25,000 readers in itself could be valuable. Targeted audiences are laser-focused on your niche. It’s not so much how many readers you have, but the quality of the readers you have. Are your open rates high? Do you have good engagement with your readers? Do people care about what you’re sending them?Say you are writing about home DIY and sharing tips, you could partner up with tool companies to offer discounts, companies like Home Depot might want to advertise to an audience that is very much their target audience.Not all ideas have to be products or built using code. Consider starting a newsletter, it might be a profitable side hustle for you.",915
Thoughts On The LG Battery-powered Face Mask,"It looks like a new hardware arms race is upon us. I am not talking about consumers rushing to buy GPU’s to mine cryptocurrencies in their parent’s basement: masks are the new hot tech (for obvious reasons).LG has announced a yet unavailable air purifying mask that you strap to your face. They are calling it the PuriCare™ Wearable Air Purifier.has announcedConsidering 2020 is anything but normal, it seems fitting that LG would release a face mask to help continue the 2020 dystopian theme we all find ourselves embedded in. This looks like something out of Dune or a movie about a highly-infectious virus turning people into zombies.Allegedly it will be able to last up to eight hours and two hours at its highest setting. I do wonder what the difference is between the two, is it less effective at the lower more efficient setting?The press photo that LG released being heavily Photoshopped does not inspired confidence in the product. If you look at not on the carefully groomed model with background bokeh, you notice it is two rubber loops that go around your ears.Maybe it works well like this, but I can’t see how this wouldn’t force your ears to protrude out from the side of your head and if the load is very front-heavy, wouldn’t it just have the same problems (possibly worse) as traditional masks worn for extended periods of time?The real question here is, why loops instead of straps and some kind of counterweight, similar to how VR headsets work? I know battery tech is getting good and all, but two filters, two fans and a battery in a plastic mask with ear loops? Sounds painful to me.Pondering the question as to why LG designed their mask this way and after a little research into face masks, I made a discovery. It appears that LG has possibly stolen this concept from an IndieGoGo project called ATOMOBLUE.ATOMOBLUEI mean, damn… It looks like LG took some serious liberties when they created their mask. They look somewhat identical. The difference between the two is subtle design differences and the strap (which ATMOBLUE says they have patented).Regardless of the potential IP theft here, inspiration or whatever you want to call it, I like the idea of a purifying face mask. Admittedly, I don’t think these two look that bad and if the price were right for the LG mask, I would definitely consider it.There is a great opportunity here for LG or someone else to take the wearable air purifier concept and expand upon it by adding in fun features like voice changing so you can sound like Bane or Darth Vader. I would love it if third-party sellers came to the table and offered skins, allowing you to look like Bane and hopefully, sound like him too.The real question I have in all of this is, where is Dyson? When it comes to fans and air purifying, Dyson has been one of the strongest innovators in the segment and as far as I am aware, an air-purifying face mask isn’t even on their radar. Although, Dyson did patent some strange concept of headphones with air purifiers built into them (which I hope never sees the light of day).headphones with air purifiers",773
Is Blazor The Future of Development?,"For a while now, Microsoft has demoed and spoken about their highly hyped WebAssembly framework that aims to blur the lines between front and back-end programming.If you are a .NET developer or just like to keep your finger on the pulse when it comes to new frameworks and web tech, you have most likely heard of Blazor. If not, you can keep reading and just smile and nod so people don’t find out you’re behind the times.Blazor is a framework that will allow developers to write applications targeting different platforms, with the web being the first priority. Right now, you can already write HTML, CSS and C# to create Blazor based web applications. By default, Blazor is a web-first framework made evident by the parts they have released so far.When people talk about Blazor and get excited, they generally fall into two categories.Developers who hate Javascript and want to use C# to write web applicationsDevelopers who are excited about WebAssemblyDevelopers who hate Javascript and want to use C# to write web applicationsDevelopers who are excited about WebAssemblyGiven how new WebAssembly is (browser support wise), Blazor is one of the first frameworks of its kind to support and commit to WebAssembly (to the extent that they have). As you can see, WebAssembly is in a good place right now and keeps on getting better. is in a good place right nowSomething that many developers probably don’t realise is Microsoft has been grooming us to get used to C# syntax in the form of TypeScript for over eight years now.I love TypeScript and I honestly can’t remember the last time I worked on a project that wasn’t TypeScript. I am also not a C# developer, but my experience with TypeScript and its overlap means if I wanted to write C# code, I would probably pick it up pretty quickly.Is it just a coincidence that behind both C# and TypeScript is the same man, Anders Hejlsberg? This was undoubtedly the plan all along, to bring C# into the browser.  TypeScript arrived early enough that Anders and his new world Javascript order friends could slowly lure us into the C# void with their beautiful enhanced Javascript siren song.Anders Hejlsberg
		
							
					
				
								
	
					
				Jokes aside, Blazor is an impressive and highly ambitious effort on Microsoft’s part. Not only are they targeting the browser, but it appears that Blazor is going to be a platform for building across; desktop, mobile and web.Blazor consists of the following pieces all being worked on separately:Blazor ServerBlazor WebAssemblyBlazor ElectronBlazor Mobile BindingsBlazor ServerBlazor WebAssemblyBlazor ElectronBlazor Mobile BindingsFor quite some time, Blazor Server was the only released offering in the Blazor suite and it shipped with .NET Core 3 back in 2019. In February 2020, experimental mobile bindings were announced. In May 2020, the ASP.NET team released Blazor WebAssembly 3.2.0.shipped with .NET Core 3experimental mobile bindingsBlazor WebAssembly 3.2.0As things currently stand you can build Blazor applications on the server using Blazor Server which will push the rendered components into the browser and handle rendering. You can now also build client-side only applications using Blazor WebAssembly (.NET backend not required)The road to web domination doesn’t appear to be stopping there. Microsoft is fast at work on .NET 5 which aims to unify the ecosystem aka global domination. A singular framework that can run anywhere: this is big deal type of stuff.fast at work on .NET 5However, before we get too excited, Blazor WebAssembly has some serious performance issues and they’re not all the ASP.NET development teams fault either. While work is ongoing, it seems the long-term solution to performance woes is Ahead of Time compilation (AOT). An issue on GitHub has been opened since 2018 and it appears AOT won’t be arriving until .NET 6 because more work needs to be done before it can be incorporated.serious performance issuesbeen opened since 2018more work needs to be doneThe wounds of Silverlight are still fresh for someThe wounds of Silverlight are still fresh for someFor those who have been around for longer than a minute, you might remember Silverlight which was often compared to Adobe Flash. It came at a time when the web was primitive and browsers sucked, to create rich experiences you had to leverage something else.Admittedly, Silverlight was superseded by HTML and Javascript as well as emerging web standards. Silverlight, like Flash, were bandaids on gaping holes in the web. However, the issue is Silverlight attempted to position itself as more than a Flash clone and it failed.Some of the core ideas of Silverlight are present in Blazor (only in spirit) and for those who committed to the Silverlight platform and ultimately found themselves abandoned when Microsoft ghosted the community and then eventually clarified their stance.Microsoft ghosted the communityclarified their stanceGiven how young and ambitious Blazor is, there is always the chance that Microsoft could drop it like a sack of potatoes in a couple of years if it doesn’t catch on. However, based on the sheer number of resources that are being thrown at this and integration with .NET, I think it’s safe to assume that Microsoft is committed to Blazor.But, who honestly knows what will happen? Even though I am not a .NET developer and I am a heavy user of front-end client-side frameworks (I actually like Javascript), the prospect of Blazor excites me. I am really excited to see how React and other frameworks and libraries adapt, will we see a React WebAssembly in the future?I can’t foresee a future where Blazor makes Javascript irrelevant or kills it. Blazor will be like any other framework, library, tool or skill. Some will avoid it, others will embrace it and the web will continue to move forward (just as we’ve seen with Javascript frameworks).",1468
"Microsoft Flight Simulator 2020 Releases August 27 Post-release Patch Notes, Still No Performance Fixes","Well, this is only mildly infuriating. The Microsoft Flight Simulator 2020 team have released the patch notes for an update coming in seven days for the game, which many hoped would fix some of the dismal performance issues in the game.released the patch notesWell, it turns out performance optimisations are taking a backseat to primarily fixing the installer…The patch notes header, “Install Issues” has the largest number of items in it:The install process will no longer be blocked after a partial decompression of a packageThe install process will no longer be blocked when a local user account includes non-ASCII charactersThe install process will no longer be blocked after a failed connection to serversThe install process will no longer display an empty onboarding screen under certain conditionsThe title will download a critical missing/deleted package to access the main menu even if the save data preference is set to offline (when an internet connection is available)The install process will no longer be blocked after a partial decompression of a packageThe install process will no longer be blocked when a local user account includes non-ASCII charactersThe install process will no longer be blocked after a failed connection to serversThe install process will no longer display an empty onboarding screen under certain conditionsThe title will download a critical missing/deleted package to access the main menu even if the save data preference is set to offline (when an internet connection is available)The photos are nice, it’s a shame that many get nowhere near a decent frame rate to render the game at higher graphic settings.It’s no secret that the launch was bungled. I and many others tried to download the game and encountered issue after issue, turns out half the battle in getting MSFS 2020 to run was getting it installed. It took me two days to get the game installed on a 250/25 (250mbps down) cable internet connection.Instead of letting those who pre-ordered the game pre-load it a day or two beforehand, it was a day one download. In Australia, customers on Aussie Broadband experienced congestion issues because it turns out Microsoft slapped the install files on an AWS Cloudfront server and didn’t have a localised region in Australia.experienced congestion issuesIn the above linked Whirlpool forum thread, an Aussie Broadband representative is unrelenting in his takedown of how Microsoft handled this, explaining how they had to upgrade capacity at a node over in the US to handle the load.Now we’re upgrading it to 100Gbps because we’re forced to do so, given it seems another game company thinks it’s okay for them to cheap out forcing your RSP to purchase more capacity and potentially increase their own operating costs to keep your speeds to their game fast on that 1 day every 30-60 days a patch comes out. You are their customer too, you should be on their forums angry at them, explaining you paid $180 for a game that they cheaped out on serving the Australian population.Now we’re upgrading it to 100Gbps because we’re forced to do so, given it seems another game company thinks it’s okay for them to cheap out forcing your RSP to purchase more capacity and potentially increase their own operating costs to keep your speeds to their game fast on that 1 day every 30-60 days a patch comes out. You are their customer too, you should be on their forums angry at them, explaining you paid $180 for a game that they cheaped out on serving the Australian population.Under the “Optimization” heading, there is only one line item:The performance of the title has been improved when the Display name plate option is set to activeThe performance of the title has been improved when the Display name plate option is set to activeAnd it is clear the game is not only taxing on the GPU, but seems to require a hefty CPU and even that doesn’t appear to be enough. In a Tom’s Hardware benchmark of the game, they conclude that right now, there is no consumer hardware that can run this game at high settings and have smooth decent framerates.Tom’s Hardware benchmark of the gameJust don’t plan on maxing out the graphics settings and getting ultra-smooth framerates. That might be possible five or ten years from now, depending on how much CPUs, in particular, can improve.Toms Hardware, Microsoft Flight Simulator 2020 benchmarkJust don’t plan on maxing out the graphics settings and getting ultra-smooth framerates. That might be possible five or ten years from now, depending on how much CPUs, in particular, can improve.Toms Hardware, Microsoft Flight Simulator 2020 benchmarkI understand this is a next-gen game that goes in directions that no other game has gone in, but it is also quite clear Asobo and partners need to iron out the kinks in this game. The benchmark above also shows that the game is CPU bottlenecked and even if you have the latest and greatest CPU with 12 cores (I have the AMD Ryzen 3900x), it will only use 4 cores.I hope in future when Microsoft delivers the promised Directx 12 (Dx12) patch, they also fix the game to utilise more than 4 cores, even 8 cores would be a dramatic improvement. It is quite weird to see 8 cores sitting idle while 4 cores are being smashed.Furthermore, it looks like the game is currently biased towards Intel CPU’s, with the benchmarks showing Intel clearly has an advantage, when we all know benchmarks for AMD’s latest powerhouse CPU’s are showing that the gap has closed dramatically. It’s possible Asobo were developing and testing this game on Intel CPU’s.I have confidence that Asobo and Microsoft will sort these performance issues out, but it is disappointing to see a game like this struggle on modern expensive hardware. Either Asobo releases a patch or hardware vendors are going to have to release some new hardware capable of playing this game properly.Tray tables up, enjoy your flight. Don’t worry about the framerate turbulence, we’ll get that sorted out for you eventually.",1497
Aurelia 2 Lifecycles Explained In As Few Words As Possible,"With Aurelia 2, everything has been reimagined from the ground up. While the syntax and way you build applications is largely the same as Aurelia 1, there are some key differences and one of those is lifecycles.When I say lifecycles I am talking about component lifecycles and router lifecycles. If you need a handy reference for what lifecycle you should use, you just found it.This will not cover all lifecycle methods, just the ones that most people would want to know about such as dom attached and where you should load data from an api.Component LifecyclesThere are a lot more lifecycles than shown below. These are the ones you want to know for loading data or working with the dom.beforeBind (equivalent to bind in Aurelia 1) – Fetch data in here, initialize things and set up subscriptions. If you make this method async, Aurelia will wait for your function to resolve before rendering. This is a great place to load data. You can modify the bindings before they’re attached to the view (coerce values and so on).beforeBindafterAttach (equivalent to attached in Aurelia 1) – Work with the dom, use 3rd party libraries. Can be async.afterAttachafterDetach (equivalent to detached in Aurelia 1) – Clean up anything that touched the dom. Can be async.afterDetachRouter LifecyclescanEnter (equivalent to canActivate in Aurelia 1) – Useful for loading data or verifying parameters passed in the URL that are required for the component to render.canEnterenter (equivalent to activate in Aurelia 1) – Similar to canEnter anything loaded in here is not necessarily crucial for the component to render.entercanEntercanLeave (equivalent to canDeactivate in Aurelia 1) – Can the current user leave the component?canLeaveleave (equivalent to deactivate in Aurelia 1) – The user is leaving the current route and heading to another one.leave",459
My Thoughts On The “no-code” Hype Trend,"The latest trend in web design and development is no-code. Well, it’s not exactly latest, the trend has been around for a while now. You can go back to the early 2000’s and point out numerous software apps and web offerings that would fall under the no-code umbrella.However, it’s hard to argue that 2020 besides being dominated by a highly contagious respiratory virus pandemic, no-code has been thrown around a lot. If you visit sites like producthunt, you would know what I am talking about.Do you remember Microsoft Frontpage, Macromedia Dreamweaver (now Adobe Dreamweaver)? And who can forget Geocities?According to this article which cites a Gartner report that predicts by 2024, low code (including no-code) platforms will make up more than 65% of all application development.this articleSixty-five percent, that’s a large slice of the pie, no matter how you slice it.The no-code trend in my opinion has been around for at least two decades, possibly even longer if you go back far enough. Since the creation of the internet and rich web applications, people have been predicting no-code would eventually take over and become a thing.I somewhat joke here as I see value in the no-code trend, tools and platforms making it easier to build products without needing to be an illustrious programmer. But, I stop short of seeing it as something that will make developers irrelevant or can be applied to every use-case.As someone who is a huge fan of Firebase, a platform you could argue is the very definition of a no-code platform, it still requires a certain degree of manual effort and coding to produce something substantial.I will admit, I have seen some decent results produced from people using website builders like Wix and Squarespace, no-code platforms which allow you to customise templates, changing colours and branding to suit your own simple needs.Then you have platforms like Shopify which allow you to create powerful and feature-laden storefronts without needing to write code. Choose a template, change some values and you have a storefront. Once again, no-code in action.The downside of these tools and platforms is you’re beholden to them. You have to pay recurring costs to keep using them, when they experience downtime, you can’t ask your developer or ops person to SSH in and fix the problem for you. You lodge a support ticket or call the support number (if you even get one on the plan you chose) and you wait.When no-code solutions work, they work and when they don’t work, they can have real implications for businesses who rely on them and don’t control those pieces of the stack. There is something to be said about owning each piece of your own business (well, most pieces).I have witnessed the same thing happen in the web hosting world. You have managed hosting which means the hosting provider will handle server resources and issues if something goes wrong, they have one-click installs for popular apps like WordPress and everything comes in a box.Self-managed means you control the server, you choose the version of Linux/Windows, you can start and stop services, you manually compile apps from source or through package managers. It also means when a self-managed server breaks (besides a datacentre hardware fault) you have to fix it, you have to maintain backups, but you control it.Sometimes all you need is a no-code solution for an MVP, but there will come a time where you will outgrow no-code and will need someone to write code for you. The move from no-code to code is usually driven by cost, platforms like Firebase start out cheap and once you start exceeding the resources given to you in your plan, they get really expensive.The lock-in aspect of no-code platforms and tools is also worth acknowledging. Sure, these magical black boxes that you put your hopes and dreams into and they spit out a result are great, but once you commit to them, the ability to easily migrate away is not always easy.Once you’re “locked-in” you’re beholden to the changes these services make. If they raise their prices, you either have to pay or spend more money migrating away to something more custom or a competing product which might suffer from the same issues.No-code is Lego for programming. You have to find the right pieces to connect together to produce a result, but you can’t change the shape/colour/size of the pieces you are using.There will always be people who find these platforms and tools are enough for them and will never need the services of a developer. However, I do not foresee developers all of a sudden becoming irrelevant. Automation is everywhere, not just in tech, but you can only automate to a certain degree before requiring human intervention.Will no-code replace developers? No. If anything, no-code tools will just be another skill that developers need to learn.Now if you will excuse me, it’s time to get ahead of the game and update my LinkedIn title to, “Senior no-code Engineer”",1238
"Sure, map(), .filter(), and .reduce() Are Great and All, But Don’t Use Them In Place of Common Sense","I see a lot of questionable blog posts and articles out there about Javascript. And, I should know, I have written some of them myself in the ten years I’ve been blogging on this site.However, over the last few years since the rise of Medium and Dev.to, I have noticed a few articles (I won’t link or name any) which go along the lines of this:Stop Using For Loops and Use map, filter and reduce insteadStop Using For Loops and Use map, filter and reduce insteadLoops Are Dead, Use These Functional Methods InsteadLoops Are Dead, Use These Functional Methods InsteadYou get the idea. Some people advocate for using these functional and chainable methods in place of for loops and in some cases, they are right. But, to say that all for loops can and should be replaced, it is insanity.A quick refresher on the three functional methods that the Javascript functional crowd pray to before they fall asleep at night and when they wake up in the morning..filter()Say you have a list of users who have ages. You want an array only containing users over the age of 20 and under the age of 31, using filter you can specify what items get discarded and what items get kept. You then get a new array containing your new users.const newUsers = users.filter(user => user.age >= 20 && user.age < 31);const newUsers = users.filter(user => user.age >= 20 && user.age < 31);.map()This is a useful method for transforming data. If you have one or more products coming back from the API and you want to normalise some of the property names or even make the product objects small, map is a good way to change the shape.const newProducts = products.map(product => ({
    id: product.id,
  	name: product.name,
    price: product.price
}));const newProducts = products.map(product => ({
    id: product.id,
  	name: product.name,
    price: product.price
}));.reduce()This powerful method allows you to take data and change its structure completely. If you take the theoretical list of users from the filter section above and you wanted to know the combined age total of your users, you can use reduce to do that.filterreduceconst ages = users.reduce((accumulator, user) => accumulator + user.age, 0);const ages = users.reduce((accumulator, user) => accumulator + user.age, 0);Reduce works on the concept of an accumulator which can be a running total, it could be an array, anything. The value of zero above is the initial value. If you were working with an array, it would be [] and so on.[]The best feature about these methods is they can be chained together. You can filter, map and reduce to your hearts content. They are effective methods and when used correctly, can make your code cleaner and easier to test as well.However, they don’t work in all situations. Perhaps one of the most notable examples of where these functions fall down is when dealing with asynchronous code, more specifically: promises and generators.If you’re working with async/await and inside of your loop you’re making an API request or anything else involving a promise, you want to wait for it to resolve before continuing to the next element in your array (or whatever you’re looping over).async/awaitThe reality is, filter, map and reduce fall down if you need to await anything. Without some clever hacks and extra code, they will not work with these methods. This is where the old trusty for loop comes in handy and there is no better for loop than a for..of loop.filtermapreducefor..offor (const item of items) {
    item.meta = await getItemMetadata(item.id);
}for (const item of items) {
    item.meta = await getItemMetadata(item.id);
}This is probably a poor example of using await inside of a for loop, but you get the idea. Don’t roast me too hard for using a for loop like a map here. For loops can work with asynchronous code and there will come a time when you might encounter such a use-case.mapAsync IterationAsync IterationEven better, async iteration which landed in ES2018 takes this concept a step further and use for await at the top-level of the loop. This is known as a for..await..of loop.async iterationfor awaitfor..await..of loopfor await (const user of getUsers()) {
	
}for await (const user of getUsers()) {
	
}The beautiful thing about this is that it will work with async iterable objects as well as sync iterables, you don’t just have to use promises or generators to use for..await..of.for..await..ofAnother example of where a for loop is more appropriate is when you’re iterating over a collection but you’re not doing anything with the data. Maybe you’re calling a function.for (const item of items) {
	markItemForCompletion(item);
}for (const item of items) {
	markItemForCompletion(item);
}Sure, you could use filter, map or reduce but you’re not filtering anything, you’re not changing the structure of the data and you’re not reducing it down to anything, you just want the items inside and to call a function.filtermapreduceAnother and perhaps a more fitting example is displaying data. I’ve seen a lot of React developers abuse the map method just to display a list of data. The proper use case when displaying data would be to use a for loop once again. You’re not changing anything, you’re just iterating over the items and displaying them, keeping their shape and structure intact.mapConclusionYou can get quite far using filter, map and reduce — no question about that. I believe you should use them until you can’t. filtermapreduceIf you are after some more robust examples of what happens when you try using async/await inside of the aforementioned methods, this blog post by Zell goes into a little more detail and has some great examples. It’s possible to get something working, but there are too many pitfalls.async/awaitthis blog postYou also might have noticed throughout this article, I barely mentioned forEach and, I didn’t show any examples of it. I am of the opinion that for..of loops are better than forEach not only in performance, but their support for control flow as well as the ability to work with all iterables without the need to coerce values.forEachforEachcontrol flowall iterablesBut, don’t go using them in instances where they A) do not work or B) are not applicable just for the sake of being a purist or because someone on Medium told you too.P.S. No, the irony that I call out blog posts and content telling people how they should do things in Javascript and then proceed to unload my opinions on you was not lost and could not be helped.P.S. No, the irony that I call out blog posts and content telling people how they should do things in Javascript and then proceed to unload my opinions on you was not lost and could not be helped.",1668
Launching The Ideas Digest Newsletter,"I have a lot of ideas, in notebooks, in the form of half-completed side projects on GitHub and my computer. Shipping has always been a problem for me and I rarely share ideas or insights into anything, but today that is all changing.I am launching The Ideas Digest, a newsletter which will send an occasional newsletter about a particular idea or topic. It’ll provide research and insights for how someone might build a product or offering around the idea or problem.The Ideas DigestFrom time to time, you’ll get a new edition featuring a new problem, opportunity, a trend or perhaps all three. This will work two ways: give you ideas and insights into potential niches and it will allow me to get better at writing and expressing my ideas.Episode #1 is available now on the site here. If you like what you see, subscribe to get future editions sent right to your inbox..here",219
It’s Almost 2021 & Meteor Is Still Alive,"Apparently, rumours of Meteor.js demise have been greatly exaggerated. Back in March 2020, the Meteor team released version 1.10 which saw a plethora of updates to the universal app platform.Admittedly, it has been quite a few years since I have worked with Meteor. I remember trying it in the early days and being wowed by its ability to allow you to build applications that bring the front and back-end together without needing to configure anything.I truly believe Meteor had a winning concept all of those years ago. Even now in 2020 and presumably 2021, server-side rendering (SSR) and static site generation are problematic and configuration heavy with a lot of room for error.Continuing on from their big release a few months ago, Meteor has announced version 1.11.announced version 1.11I don’t mean this to sound snarky, but I am both delighted and surprised to see that Meteor is alive and well. It is no surprise that developers flocked to React years ago and it has become the defacto standard for front-end applications.But, it appears Meteor instead of trying to compete with the likes of React or Vue have embraced these frameworks. instead of being a framework, Meteor positions itself as a platform that can play nice with other frameworks and libraries in the ecosystem.I have always seen Meteor as a platform, but in the earliest days of Meteor, it wasn’t overly flexible and getting it to work with other frameworks and libraries was quite problematic.The Meteor CLI in my opinion has always been ahead of the game. Before React and Vue, Angular and other frameworks shipped with fancy CLI’s, Meteor has had an impressive one since the early days.As a huge Firebase fan, I am tempted by the latest releases to give Meteor another try. It appears the early day issue of node_modules and performance issues have all been solved with the later releases and it is now a platform that allows you to build apps without worrying about configuration or things like MongoDB instances.node_modulesOn my todo list this week is seeing if I can get Aurelia 2 to work with Meteor. Considering other frameworks and libraries appear to be compatible, I am sure that it’s probably not that difficult.",551
How To Create An Iframe and Populate It With Dynamic HTML In Javascript,"At work recently, I had a use-case where I needed to show a preview of some HTML dynamically sent from the server inside of an iFrame.It is quite possible before you found this blog post, you found a solution that looked something like this:iframe.src = 'data:text/html;charset=utf-8,' + encodeURI(html);iframe.src = 'data:text/html;charset=utf-8,' + encodeURI(html);This will work for simple HTML and instances where the HTML isn’t overly long. But, if your use-case is like mine, the HTML the server is returning is massive. In my case, I had base64 encoded images making the returned HTML huge.The above solution will not work for overly large strings. The solution is to create a blob of type text/html and pass in the HTML. Finally, use createObjectUrl and pass it to the source of the iFrame.text/htmlcreateObjectUrlconst iframeContainer = document.querySelector('#container');
const iframe = document.createElement('iframe');

iframe.frameBorder = 'none';

const finalHtml = `
<!DOCTYPE html>
<html>
  <body>${html}</body>
</html>`;

const blob = new Blob([finalHtml], {type: 'text/html'});

iframe.src = window.URL.createObjectURL(blob);

iframeContainer.innerHTML = '';
iframeContainer.appendChild(iframe);const iframeContainer = document.querySelector('#container');
const iframe = document.createElement('iframe');

iframe.frameBorder = 'none';

const finalHtml = `
<!DOCTYPE html>
<html>
  <body>${html}</body>
</html>`;

const blob = new Blob([finalHtml], {type: 'text/html'});

iframe.src = window.URL.createObjectURL(blob);

iframeContainer.innerHTML = '';
iframeContainer.appendChild(iframe);This will work for overly large strings. It will also work for special characters and any weird caveats of the string approach above, which will not always work.",442
Should I Use Firebase?,"I have been a Firebase user for quite a few years now, since 2016. In that time, I have seen Firebase grow and change as a product. I have also seen the problems people have had with it, as well as the successes.Googling opinions on Firebase will yield mixed results.Some are for Firebase and its all-inclusive platform offering features like authentication, a NoSQL database called Firestore, storage, hosting, Firebase Functions (basically AWS Lambda, Google Cloud Functions). Others advise against Firebase because of issues around costs and how the use of resources (reads and writes) are counted.The biggest upside (and possibly a downside) is that Firebase makes it easy to build applications on it. Perhaps its best feature is Firebase Authentication, allowing you to use the SDK to easily add in a plethora of different authentication options as well as numerous OAuth providers.The authentication feature alone is a massive selling point of Firebase, it saves you hours (possibly days) of authentication-related headaches, especially for OAuth authentication which in 2020 can still be a painful nightmare to implement.However, a consideration when using Firebase is that you’re locking yourself into the Firebase ecosystem. At present, Firebase does not make migrating to a different platform or provider easy. If you want to migrate away, you have to write your own migration scripts and handle it yourself.If you are not careful in how you write your Firestore rules or if you make reads from within the rules themselves, it is possible if you get a lot of traffic to run up a huge bill because you didn’t properly architect your app and optimise reads or writes.Fortunately, Firebase has billing alerts these days, so nasty surprises should be a thing of the past. I personally have never run into any problems with billing, I don’t think I have ever paid for than a few cents for my Firebase usage, if ever. The free limits are generous. I always spend a lot of time carefully writing my Firestore rules.I am more than capable as a developer of writing my own simple Node.js backends, but do I trust them? Not really. Firebase gives you a platform, but they also take care of some of the security behind-the-scenes for you as well. You don’t have to worry about XSS attacks or SQL injection attacks, vulnerabilities in servers or backend frameworks.From a plug and play perspective, Firebase gives you a tonne of stuff as soon as you turn it on. You just have to be careful about having open public databases, never run in test mode and work your way back from there by opening things up through rules.As for apps, I have a few I’ve done over the years. My most recent Firebase app is Tidyfork, it uses cloud functions as well as authentication to log users into GitHub. The beautiful thing about the client-side SDK is that it even supports scopes for OAuth.TidyforkConclusionWith Firebase, I am of the opinion the good outweighs the bad. Sure, it’s possible it could get expensive for large-scale use and you might need to migrate. However, if you’re a hobby programmer or working on a side-project or startup that has a high chance of failing anyway, Firebase is a safe bet.You should use Firebase, unless, of course, you don’t want too. And in that case, use something else.",823
Waiting For Elements To Exist In The Dom With User-specified Attempts,"I had to implement some testing logic recently where I wanted to wait for a heavy page to load and for specific elements to become available. I knew I wanted a function that polled the page and returned true if it found the element or rejected if it didn’t.trueI actually looked some a pre existing solution and turned up a blank. There are code examples out there, but some developers have created some over engineered solutions using things like proxies, mutation observers and other convoluted ways of waiting for elements.All I wanted to be able to do was call a function which would wait for an element to exist in the page and return true if it was found or error out if it wasn’t.async function waitForElement(selector: string, attempts: number = 10) {
	const query = document.querySelectorAll(selector);

	if (query.length) {
		return true;
	}

	setTimeout(() => {
		attempts--;

		if (attempts > 0) {
			return waitForElement(selector, attempts);
		}

		throw new Error('Element not found');
	}, 100);
}async function waitForElement(selector: string, attempts: number = 10) {
	const query = document.querySelectorAll(selector);

	if (query.length) {
		return true;
	}

	setTimeout(() => {
		attempts--;

		if (attempts > 0) {
			return waitForElement(selector, attempts);
		}

		throw new Error('Element not found');
	}, 100);
}This code is written with TypeScript and is quite easy to follow. We create a new asynchronous function which will return a promise. It accepts a selector which gets passed to querySelectorall and a second argument which is the number of attempts before it errors out.querySelectorallThe function recursively calls itself until the number of attempts equals 0 and then it throws an error, thus, rejecting the promise. If it finds the element, it returns true.const elementFound = await waitForElement('#someelement');const elementFound = await waitForElement('#someelement');Sure, some of the other solutions out there might be more technically impressive, but I’m a realist and more often than not, I go for the solutions that are readable and functional.",523
Working From Home Is The New Normal,"The 2020 COVID-19 pandemic really turned the world on its head. Despite the fact people have lost their jobs, businesses have been hurt, airlines are struggling and tourism has taken a nosedive, many of us have been fortunate enough to keep our jobs and work from home.Many large tech companies have already announced permanent changes allowing employees to work from home indefinitely. Twitter, Square and Facebook being some of the biggest who have announced changes.Some companies are clamouring to get their workers back into the office, despite the fact we have no COVID-19 vaccine yet and the outbreak is still ongoing. Even countries who have handled the outbreak better than most such as New Zealand and Australia are still playing a game of cat and mouse with the virus, dealing with outbreaks.The reality is, even prior to the pandemic, most companies were already partially distributed. It’s possible you might not have even noticed or thought about it. But, even companies who had no work from home policies, they were already facilitating remote work.It’s not uncommon for a CEO to be working from home, travelling and working, someone out working onsite with a customer (troubleshooting a problem or doing some training). And I am sure we are all guilty of checking work emails outside of work hours on our phones or computers.The Office Isn’t DeadFor some, working from home isn’t an option. I know people who not only do not trust themselves working from home, but want to return to the office as soon as possible. Some people like the ritual of leaving the house and commuting to the office.The concept of an office isn’t dead, offices will always exist. If you’re a company that has meetings with people outside of the company, partnerships, consultants and the like, you need an office to facilitate these meetings.Even if you do work from home, chances are you’ll want to head into the office from time-to-time, even from a social catch up perspective. That is what the company I work for is doing, everyone might meet up every month or so and have a catch up day, have lunch together and maybe a little brainstorm session.The Shift Has Been Happening For YearsThe pandemic sped everything up, but many companies were already transitioning into remote work friendly workplaces. I know people who have been working from home since the 90’s.Companies who fought working from home had their hands forced, it was either let your employees work from home or shut your business down. But, this battle extends beyond working constraints.The reality is companies that fight remote work and expect their employees to return to the office without being given the option not too will lose out to companies that do. Remote work is not a perk, it’s an expectation now. Millennials are connected people, they don’t want to be confined to an office all of the time.Remote work allows companies to hire from a wider talent pool, you are no longer constrained to your small region. If your company is based in a small city or in a place like Seattle or San Francisco where it can be impossible to find top talent, expanding beyond your own state or country can solve the problem.",796
How To Handle Async/Await Errors in Javascript,"Some developers are still new to async/await in Javascript. If you’re used to callbacks or using .then to get the value of a promise, async/await can be a bit of a black box, especially when it comes to errors.The most straightforward and recommended way to handle errors is using try/catch. This will allow you to catch errors if your async function rejects or throws an error.try/catchasync function getProducts() {
  try {
    const response = await fetch('/products');
    const products = await response.json();
  } catch(err) {
    console.error(err);
  }
}async function getProducts() {
  try {
    const response = await fetch('/products');
    const products = await response.json();
  } catch(err) {
    console.error(err);
  }
}If you had a function which handles API requests, you could do something like this:async function request(endpoint) {
    const response = await fetch(endpoint);
    return response.json();
}

async function getProducts() {
  try {
      const products = await request('/products);
  } catch (err) {
    console.error(err);
  }
}async function request(endpoint) {
    const response = await fetch(endpoint);
    return response.json();
}

async function getProducts() {
  try {
      const products = await request('/products);
  } catch (err) {
    console.error(err);
  }
}You can even make the request method shorter by doing this:requestasync function request(endpoint) {
	return (await fetch(endpoint)).json();
}async function request(endpoint) {
	return (await fetch(endpoint)).json();
}My preference when working with promises is to always use async/await and break my logic for fetching content into separate functions. I tend to exclusively work with native Fetch these days and find breaking up my code makes it easier to work with.async/await",448
How To Copy A Public and Private Key From Windows Linux Subsystem Terminal,"In WSL (Windows Subsystem for Linux) you get access to a Linux terminal right inside of Windows 10. For the most part, it works great. However, when it comes to copying and pasting the contents of files from Linux into Windows, the option is obscured.You can right-click the toolbar on the WSL terminal window, go to properties and enable the copy and paste features if you’re on one of the latest versions of it.This will enable copy and paste from files. However, as you might also learn, this doesn’t appear to always work.. I wanted to copy the contents of a public key to a website to use SSH authentication. Which is where the next solution definitely works.Piping to the clipboardYou can also use a lesser-known trick of piping the contents of a file using cat into clip.exe which will be the Windows clipboard. This always works for me, where enabling the above did not.catclip.execat < ~/.ssh/id_rsa.pub | clip.execat < ~/.ssh/id_rsa.pub | clip.exe",239
How To Optimise Microsoft Flight Simulator 2020 Including CPU Performance Issues,"If you are like me, you do not have the world’s most expensive and powerful gaming PC. When Microsoft Flight Simulator 2020 was announced, I was excited like a kid on Christmas.While my machine allegedly falls somewhere in the medium performance category, admittedly, the performance has been anything but smooth for me playing the game.Through trial and error, I’ve struck a good balance between the game not looking like Minecraft and not running like a potato.First, one thing I want to make known here is MSFS 2020 has some bugs and performance issues that no amount of tweaking can fix. Besides the very obvious memory leaks requiring a restart every so often, the game appears to suffer even on high-end gaming rigs.I am sure that after a few patches are released they will make the performance better, but for now, even if you do have decent hardware, there is still the likelihood this game is going to be anything but smooth (especially in high trafficked areas like New York).One of the bottlenecks for this game is the CPU. If you are a few upgrade cycles behind, your CPU might not be able to handle ultra or high, but the game can still look phenomenal on medium. I have an Intel Core i5 6600K, it’s a couple of generations behind, but it manages okay.You want to make sure you have a graphics card, ideally better than the recommended as per the specs. While CPU is definitely a bottleneck, a terribly outdated graphics card will even struggle on low settings.Close down unneeded applicationsI made the mistake of having Google Chrome opened with a few tabs and quite a bit of memory and CPU usage: big mistake. Chrome is a notorious memory hog and some sites can chew up precious CPU with poorly optimised Javascript and garbage collection events.I also ensure that my anti-virus has whitelisted Microsoft Flight Simulator and that it’s not performing any scans while I am playing the game. Anti-virus can chew through CPU as well as I/O.Performance Graphics SettingsV-SyncOffRender Scaling100Anti-AliasingFXAATerrain Level of Detail60Tesselation QualityMediumBuildingsHighTreesMediumGrass and BushesMediumObjects Level of Detail60Volumetric CloudsMediumTexture ResolutionHighAnisotropic Filtering8xTexture SupersamplingOffTexture SynthesisHighWater WavesMediumShadow Maps1024Terrain Shadows512Contact ShadowsHighWindshield EffectsHighAmbient OcclusionMediumReflectionsMediumLight ShaftsHighBloomOffDepth of FieldHighLens CorrectionOffLens FlareOnGeneric Plane Models AIOnGeneric Plane Models MPOffV-SyncOffRender Scaling100Anti-AliasingFXAATerrain Level of Detail60Tesselation QualityMediumBuildingsHighTreesMediumGrass and BushesMediumObjects Level of Detail60Volumetric CloudsMediumTexture ResolutionHighAnisotropic Filtering8xTexture SupersamplingOffTexture SynthesisHighWater WavesMediumShadow Maps1024Terrain Shadows512Contact ShadowsHighWindshield EffectsHighAmbient OcclusionMediumReflectionsMediumLight ShaftsHighBloomOffDepth of FieldHighLens CorrectionOffLens FlareOnGeneric Plane Models AIOnGeneric Plane Models MPOffV-SyncOffV-SyncV-SyncOffRender Scaling100Render ScalingRender Scaling100Anti-AliasingFXAAAnti-AliasingAnti-AliasingFXAATerrain Level of Detail60Terrain Level of DetailTerrain Level of Detail60Tesselation QualityMediumTesselation QualityTesselation QualityMediumBuildingsHighBuildingsBuildingsHighTreesMediumTreesTreesMediumGrass and BushesMediumGrass and BushesGrass and BushesMediumObjects Level of Detail60Objects Level of DetailObjects Level of Detail60Volumetric CloudsMediumVolumetric CloudsVolumetric CloudsMediumTexture ResolutionHighTexture ResolutionTexture ResolutionHighAnisotropic Filtering8xAnisotropic FilteringAnisotropic Filtering8xTexture SupersamplingOffTexture SupersamplingOffTexture SynthesisHighTexture SynthesisHighWater WavesMediumWater WavesMediumShadow Maps1024Shadow Maps1024Terrain Shadows512Terrain Shadows512Contact ShadowsHighContact ShadowsHighWindshield EffectsHighWindshield EffectsHighAmbient OcclusionMediumAmbient OcclusionMediumReflectionsMediumReflectionsMediumLight ShaftsHighLight ShaftsHighBloomOffBloomOffDepth of FieldHighDepth of FieldHighLens CorrectionOffLens CorrectionOffLens FlareOnLens FlareOnGeneric Plane Models AIOnGeneric Plane Models AIOnGeneric Plane Models MPOffGeneric Plane Models MPOffIf you’re still experiencing issues due to CPU bottlenecks, lower the terrain level of detail and object level of detail settings down a lot more (somewhere around 30 for really old CPU’s and it can help).",1124
How To Use Autonumeric.js With Aurelia,"As a long-term user of the InputMask plugin, I’ve long put up with its downsides such as issues around negative amounts and decimals. Recently, I was tasked with integrating Autonumeric into an Aurelia application at work and the result was simple and elegant, I thought I would share it.The result works out a lot nicer than InputMask which I personally find to be an absolute nightmare to work with for currency scenarios especially.Install DependenciesWe will be using the autonumeric plugin as well as lodash for this one. You can replace the Lodash dependency with your own solution if you like.autonumericlodashnpm install autonumeric lodashCreate The ComponentCreate a new file called auto-numeric.ts and add in the following. This is the view-model for our custom element.auto-numeric.tsimport { customElement, bindingMode } from 'aurelia-framework';

import AutoNumeric from 'autonumeric';
import { bindable } from 'aurelia-framework';
import merge from 'lodash/defaultsDeep';

// Equivalent of setting 'dollar' as the secondary config option for the plugin
const defaultOptions = {
  digitGroupSeparator          : ',',
  decimalCharacter             : '.',
  currencySymbol               : '$',
  currencySymbolPlacement      : 'p',
  negativePositiveSignPlacement: 'r'
};

@customElement('auto-numeric')
export class IaNumeric {
  private im;
  private input: HTMLInputElement;

  @bindable({ defaultBindingMode: bindingMode.twoWay }) value;
  @bindable() options;

  constructor(private element: Element) {

  }

  attached() {
    this.im = new AutoNumeric(this.input, merge(this.options, defaultOptions));

    this.im.set(this.value);

    this.element.addEventListener('autoNumeric:rawValueModified', this.rawValueChanged);
  }

  detached() {
    this.element.removeEventListener('autoNumeric:rawValueModified', this.rawValueChanged);
  }

  rawValueChanged = () => {
    this.value = this.im.getNumber();
  }

  optionsChanged() {
    if (this.im) {
      this.im.update(merge(this.options, defaultOptions));
    }
  }
}
import { customElement, bindingMode } from 'aurelia-framework';

import AutoNumeric from 'autonumeric';
import { bindable } from 'aurelia-framework';
import merge from 'lodash/defaultsDeep';

// Equivalent of setting 'dollar' as the secondary config option for the plugin
const defaultOptions = {
  digitGroupSeparator          : ',',
  decimalCharacter             : '.',
  currencySymbol               : '$',
  currencySymbolPlacement      : 'p',
  negativePositiveSignPlacement: 'r'
};

@customElement('auto-numeric')
export class IaNumeric {
  private im;
  private input: HTMLInputElement;

  @bindable({ defaultBindingMode: bindingMode.twoWay }) value;
  @bindable() options;

  constructor(private element: Element) {

  }

  attached() {
    this.im = new AutoNumeric(this.input, merge(this.options, defaultOptions));

    this.im.set(this.value);

    this.element.addEventListener('autoNumeric:rawValueModified', this.rawValueChanged);
  }

  detached() {
    this.element.removeEventListener('autoNumeric:rawValueModified', this.rawValueChanged);
  }

  rawValueChanged = () => {
    this.value = this.im.getNumber();
  }

  optionsChanged() {
    if (this.im) {
      this.im.update(merge(this.options, defaultOptions));
    }
  }
}
Now, create a view for our custom element auto-numeric.htmlauto-numeric.html<template>
  <input type=""text"" ref=""input"" one-time.bind=""value"">
</template><template>
  <input type=""text"" ref=""input"" one-time.bind=""value"">
</template>UsageTo use the plugin, make sure you import it and then you reference it like any normal input. Now, the value going in doesn’t override the input when you type into it, we only care about the value coming out.<auto-numeric value.bind=""myVar""></auto-numeric><auto-numeric value.bind=""myVar""></auto-numeric>",956
Revisiting My Open Source Ideas From Six Years Ago,"Six years ago I open-sourced some ideas onto Github I had floating around in my head. The web has changed a lot in six years, so I thought I would revisit my ideas and see what was good and what wasn’t. I open-sourced some ideas onto GithubMassive Cryptocurrency APIThe idea was simple, an API for sending and receiving cryptocurrency payments. While Coinpayments was listed as a previous competitor, nobody else that I know of came to the table with a solution that has survived into 2020. Coinpayments is all but the standard for crypto payments now.API for sending and receiving cryptocurrency paymentsConcise Meal PlanningSince 2014, meal planning apps have really kicked up. The idea was an app where you specify a budget and you get a meal plan delivered to you that meets that budget.specify a budget and you get a meal planWhile I am not sure a 1:1 solution exists, there are many meal planning services now including Equalution and even apps like Myfitnesspal offer recipes now.Kickstarter Meets Not-For-ProfitBack in 2014 and presumably in 2020, Kickstarter does not support not-for-profit projects. The idea was a platform that offered Kickstarter like fundraising for not-for-profit projects. Reading the title, I think we both know who emerged victorious in this area: GoFundMe.Kickstarter like fundraising for not-for-profitSalads On DemandThe idea was a meal delivery service that delivered fresh salads to you. Since 2014, delivery services like Uber Eats and Menulog have saturated the market, in this pandemic world, you can get almost any cuisine delivered to you including salad.meal delivery service that delivered fresh saladsThe Best PizzaI still think about this idea, finding the best pizza based on sentiment analysis. From a fun project perspective, I still think this would be cool and could be expanded to other areas.about this ideaCrypto For FiatThe idea was a platform where users can buy and sell cryptocurrencies using fiat with as little steps as possible. Those in cryptocurrency know that the biggest player solved this problem, I am of course, talking about Coinbase.buy and sell cryptocurrencies using fiatBooks to eBooksThis idea centred around being able to swap your physical books for digital copies. Due to the complicated nature of publishing and copyright, this is an idea I can never see taking off or being possible, unless publishers or someone like Amazon did it. swap your physical books for digital copiesNutrition CompanionYou can’t stop progress. This idea involved an app that could scan barcodes and give you nutritional information. There are quite a few of these apps around now, Myfitnesspal seems to offer this functionality and then some more.scan barcodes and give you nutritional informationNews CMSThis one gave me a laugh. This news CMS idea was something akin to WordPress with more news-organisation oriented features. News has really taken a tumble since 2014 and WordPress has improved in the features department in combination with something like Grammarly, you don’t need a dedicated CMS.news CMS ideaLocalised ShopperBeing able to limit the scope of buying things to a particular geographical region or distance. Since 2014, we have seen many dominant players implement these features including Gumtree as well as Facebook Marketplace.buying things to a particular geographical region or distanceUber For Roadside AssistanceThis one was an obvious evolution of the Uber concept, it just needed time. My initial idea would have experienced regulatory issues with vetting mechanics and whatnot. Existing networks like RACQ and other roadside assistance offerings globally now offer apps and you don’t have to always be a member.This one was an obvious evolution",933
Recursively Fetch Data From The GitHub API Using Octokit,"Recently I began updating TidyFork to be written in Aurelia 2. As such, I took the opportunity to change how the repositories were loaded in the UI. There are numerous Octokit libraries provided by GitHub which let you make requests to the GitHub API.TidyForkUsing the request.js package, I make calls to the GitHub API. I also use the Parse Link Header package to parse the pagination headers that GitHub returns on its API responses.request.js packageInstall the needed packages by running: npm install @octokit/request parse-link-headernpm install @octokit/request parse-link-headerHere is the code in all of its glory. It’s a function which will recursively load all user-owned GitHub repositories, it will keep loading additional repos until there are no more to load.import { request } from '@octokit/request';
import * as parse from 'parse-link-header';

async function getUserRepos(username, page = 1) {
	const repos = [];
  
  	// Call the GitHub API and get all repositories
    const result = await request(`GET /user/repos`, {
      headers: {
        authorization: `token ${localStorage.getItem('github_auth_token')}`,
      },
      visibility: 'all',
      sort: 'created',
      direction: 'desc',
      per_page: 100,
      page: page
    });
    
    // Push the result into the repos array above
    // this will be called for the initial function call
    repos.push(...result.data);

  	// Parse the pagination headers
    const pagination = parse(result.headers.link);
  
    // If we have a next property, we have more results to load
    if (pagination.next) {
      // Recursively call this function again
      const response = await getUserRepos(username, parseInt(pagination.next.page));
      repos.push(...response);
    }
  
    // Return the loaded repos
    return repos
      .reduce((acc, repo) => {
        // If the repo owner matches the logged in user, allow it
        if (repo.owner.login === username) {
          acc.push(repo);
        }
        return acc;
      }, []);
}import { request } from '@octokit/request';
import * as parse from 'parse-link-header';

async function getUserRepos(username, page = 1) {
	const repos = [];
  
  	// Call the GitHub API and get all repositories
    const result = await request(`GET /user/repos`, {
      headers: {
        authorization: `token ${localStorage.getItem('github_auth_token')}`,
      },
      visibility: 'all',
      sort: 'created',
      direction: 'desc',
      per_page: 100,
      page: page
    });
    
    // Push the result into the repos array above
    // this will be called for the initial function call
    repos.push(...result.data);

  	// Parse the pagination headers
    const pagination = parse(result.headers.link);
  
    // If we have a next property, we have more results to load
    if (pagination.next) {
      // Recursively call this function again
      const response = await getUserRepos(username, parseInt(pagination.next.page));
      repos.push(...response);
    }
  
    // Return the loaded repos
    return repos
      .reduce((acc, repo) => {
        // If the repo owner matches the logged in user, allow it
        if (repo.owner.login === username) {
          acc.push(repo);
        }
        return acc;
      }, []);
}The majority of the functionality inside of our function happens before the reduce call at the bottom. We make the request, push the data into an array, check if there is pagination data and get the next page number. We then recursively call the function again, passing in the username and next page number.reduceIt’s possible you might not even need to parse the link headers, the Octokit package might have functionality that handles this for you. This is just the result I came up with.",938
How To Calculate A Javascript Date X Months Ago With Vanilla Javascript,"Working with dates in 2020 are still a mess. Presumably, they’ll also be a mess to work within 2021, 2022 and for a good while after that. Many (myself included) reach for a date library to fill the gaps.For years Moment.js reigned supreme and for good reason, it can do everything with dates, including working with different time zones. Unfortunately, Moment can also result in bundle bloat.And then came along date-fns. It could do almost everything Moment did and was good enough for 95% of all date and time related use cases. However, depending on the task, you can still use native vanilla Javascript to work with dates.One such task I did recently was needing to calculate a day from today, my scenario was six months, but the solution I am going to show you can be changed to be any value and work for days and so on.const SIX_MONTHS_AGO = new Date();
SIX_MONTHS_AGO.setMonth(SIX_MONTHS_AGO.getMonth() - 6);const SIX_MONTHS_AGO = new Date();
SIX_MONTHS_AGO.setMonth(SIX_MONTHS_AGO.getMonth() - 6);We create a new date object, then we set the month to that of our month value minus 6 (where 6 is the number of months we want to go back). You can also change this so you can go 6 months into the future by changing the minus to a plus.Finally, if you want to do date comparison and take a date and determine if it is older than six months, we can do something like this.export const compareDates = (a: string | Date, b: string | Date): boolean => {
    const dateA = new Date(a).getTime();
    const dateB = new Date(b).getTime();

    return dateB > dateA;
}export const compareDates = (a: string | Date, b: string | Date): boolean => {
    const dateA = new Date(a).getTime();
    const dateB = new Date(b).getTime();

    return dateB > dateA;
}This will determine if the second value supplied to our function is greater than the first value supplied. The first value is the base and the second value is the comparative value.If you were to call compareDates with SIX_MONTHS_AGO as the first argument and new Date() as the second, we know the result would be true as today’s date is definitely older than six months ago.compareDatesSIX_MONTHS_AGOnew Date()compareDates(SIX_MONTHS_AGO, new Date())compareDates(SIX_MONTHS_AGO, new Date())No additional libraries required. I agree that the syntax is a little dry in comparison to the nicer API’s of Moment and date-fns, but it’s still a lighter and viable option.",605
Protected User Uploadable Files With Firebase Storage,"Recently, while I was building an application on Firebase and implementing Firebase Storage, I realised that I have to always consult old Firebase projects I have built or scour the web for blogs and documentation that explain not only how to do uploads to Firebase Storage but also write rules to protect them.My use-case is as follows:My use-case is as follows:Users can create listingsA listing can have one or more imagesAn image is uploaded to Firebase Storage in the form of listing-images/userId/filename.extensionI only want to allow users to write to this folder if they are that user currently logged in. This folder becomes a bucket for all of the users listing images.Users can create listingsA listing can have one or more imagesAn image is uploaded to Firebase Storage in the form of listing-images/userId/filename.extensionlisting-images/userId/filename.extensionI only want to allow users to write to this folder if they are that user currently logged in. This folder becomes a bucket for all of the users listing images.Uploading Filesconst storage = firebase.storage().ref(`listing-images/${this.auth.currentUser.uid}/${image.name}`);
const upload = storage.put(image);

upload.on('state_changed',
    // Uploading... 
    (snapshot) => {
        this.frontImageUploading = true;
    }, 
    // Error
    () => {
        this.frontImageUploading = false;
    }, 
    // Complete
    () => {
        this.frontImageUploading = false;
    }
);const storage = firebase.storage().ref(`listing-images/${this.auth.currentUser.uid}/${image.name}`);
const upload = storage.put(image);

upload.on('state_changed',
    // Uploading... 
    (snapshot) => {
        this.frontImageUploading = true;
    }, 
    // Error
    () => {
        this.frontImageUploading = false;
    }, 
    // Complete
    () => {
        this.frontImageUploading = false;
    }
);Uploading files with Firebase’ Javascript SDK could not be any easier. You create a reference to where you want to store a file and then you call the put method providing a file to upload.putWe then listen to the state_changed event which can take three callback functions. The first is called as the file is uploaded, the second is the error callback and the third is called when a file has been uploaded.state_changedFirebase Storage RulesMy storage.rules file ended up looking like this. Your use-case might differ, but the premise is the same.storage.rulesI want to allow all reads as these images are public, so I create a rule called allow read and provide if true as the value.allow readif trueFor image writes, I first check if the currently logged in user matches the userId provided in the image path. I then check if the size of the image is less than 5mb and if its content type is an image.userIdrules_version = '2';

service firebase.storage {
  match /b/{bucket}/o {
    match /listing-images/{userId}/{allPaths=**} {
      allow read: if true;
      allow write: if request.auth.uid == userId &amp;&amp; request.resource.size &lt; 5 * 1024 * 2014 &amp;&amp; request.resource.contentType.matches('image/.*');
    }
  }
}This allows users to only be able to upload to their specific folder inside of listing-images. Anyone can read it, but only the logged-in user can upload here. It’s simple and it works.listing-imagesAccessing The Uploaded FileOnce the file has successfully uploaded, you most likely want to access some information about it like where it uploaded and so on. We can use the reference we created to get that information.const storage = firebase.storage().ref(`listing-images/${this.auth.currentUser.uid}/${image.name}`);
const upload = storage.put(image);

upload.on('state_changed',
    // Uploading... 
    (snapshot) => {
        this.frontImageUploading = true;
    }, 
    // Error
    () => {
        this.frontImageUploading = false;
    }, 
    // Complete
    async () => {
        this.frontImageUploading = false;

        const meta = await storage.getMetadata();
    }
);const storage = firebase.storage().ref(`listing-images/${this.auth.currentUser.uid}/${image.name}`);
const upload = storage.put(image);

upload.on('state_changed',
    // Uploading... 
    (snapshot) => {
        this.frontImageUploading = true;
    }, 
    // Error
    () => {
        this.frontImageUploading = false;
    }, 
    // Complete
    async () => {
        this.frontImageUploading = false;

        const meta = await storage.getMetadata();
    }
);We made the success callback async and then we awaited the getMetadata method which is called on the ref itself. This gives us information like which bucket the file is in, the fullPath, it’s md5Hash, size and other useful values.asyncgetMetadatareffullPathmd5HashsizeIf you want to generate an image URL string to the file which can be used in the browser, you can call getDownloadURL(); like so.getDownloadURL();const url = await storage.getDownloadURL();const url = await storage.getDownloadURL();",1237
How To Convert An Object To An Array In Vanilla Javascript,"I do quite a lot of work with Firebase and when you are working with authentication claims, they will be returned as an object containing your simple values (usually booleans).Thankfully, since ES2015 landed, each Javascript release has introduced a new and easier way to work with objects and convert them into arrays. You don’t need any libraries like Lodash, this is all native and well-supported vanilla Javascript.To convert an object of properties and values, you can use Object.entries however, it will return an array of arrays, which isn’t terrible but might confuse Javascript novices as to how you would even work with these.Object.entriesconst claims = {
    admin: true,
    superAdmin: false
};

const claimsArray = Object.entries(claims);const claims = {
    admin: true,
    superAdmin: false
};

const claimsArray = Object.entries(claims);Now, if you were to console.log claims array from above, this is what you would get.console.log[
  [
    ""admin"",
    true
  ],
  [
    ""superAdmin"",
    false
  ]
][
  [
    ""admin"",
    true
  ],
  [
    ""superAdmin"",
    false
  ]
]To work with this nested array, you can simply use a for..of loop like this:for (const [key, value] of claimsArray) {
    console.log(key, value)
}for (const [key, value] of claimsArray) {
    console.log(key, value)
}The key is the object property name and the value is the value. keyvalueIt’s amazing how easy modern Javascript makes doing things like these. To think only a few short years ago we were still using jQuery, supporting old versions of Internet Explorer and crying out for a better web.",398
How To Store Users In Firestore Using Firebase Authentication,"As much as I love Firebase, especially it’s easy to implement authentication, for some things Firebase can be a bit confusing when you go to implement them. For Firebase Authentication, sadly, you cannot store any additional information and easily query it for authenticated users. You can leverage custom claims to add little pieces of meta to a user (like roles), but for things such as profile data, you can’t.Fortunately, there is a solution you can easily implement using Firebase Functions and triggers.The WorkflowA user signups for your application using Firebase Authentication. At the same time, you want to also create a new document inside of Firestore for that user and store some of their user information like uid as well as any custom claims, display name and so on.uidThis will then allow you to query Firestore for any additional data for this user. It might be fields like; date of birth, country, first and last name, their likes, a bio, anything.Firebase TriggersInside of Firebase Functions, you can add a trigger for onCreate which will get called when a user creates an account in your application. This will be called when the user logs in using social oAuth (Google, GitHub, etc) as well as email/password and other login methods.onCreateexport const createUser = functions.auth.user().onCreate((user) => {
    const { uid, displayName, email } = user;

    return admin.firestore()
        .collection('users')
        .doc(uid)
        .set({ uid, displayName, email })
});export const createUser = functions.auth.user().onCreate((user) => {
    const { uid, displayName, email } = user;

    return admin.firestore()
        .collection('users')
        .doc(uid)
        .set({ uid, displayName, email })
});The code is fairly easy to understand. When a new user is created, we pull out their uid as well as displayName (if social oAuth login) and email. The technique here is creating a new user, using their uid as their unique identifier (document name).uiddisplayNameemailuidSomething to keep in mind is this trigger will only fire once. You will have to completely delete the user and force them to sign up again if you want it to trigger again.Cleaning UpIf a user deletes their account, you also want to make sure that you use the onDelete trigger to remove the user from your database.onDeleteexport const deleteUser = functions.auth.user().onDelete((user) => {
    return admin.firestore()
        .collection('users')
        .doc(user.uid)
        .delete();
});export const deleteUser = functions.auth.user().onDelete((user) => {
    return admin.firestore()
        .collection('users')
        .doc(user.uid)
        .delete();
});Defining RulesWhen you are working with Firestore, it is important to properly create rules to prevent potential security issues in your application. You don’t want just anyone being able to read your users database and leaking sensitive information.Because Firestore rules allow you to define helper methods, we are going to create two methods isAuth and isUser. The auth method will check if a user is logged in and the second method will allow us to pass in a uid and compare that with the currently logged in user.isAuthisUseruidrules_version = '2';

function isAuth() {
    return request.auth != null && request.auth.uid != null;
}

function isUser(uid) {
    return request.auth.uid == uid;
}

service cloud.firestore {
  match /databases/{database}/documents {
    match /{document=**} {
      allow read, write: if false;
    }

    match /users/{userId} {
        allow read: if isUser(userId);
        allow create: if isAuth();
        allow update: if isUser(userId);
        allow delete: if isUser(userId);
    }
  }
}rules_version = '2';

function isAuth() {
    return request.auth != null && request.auth.uid != null;
}

function isUser(uid) {
    return request.auth.uid == uid;
}

service cloud.firestore {
  match /databases/{database}/documents {
    match /{document=**} {
      allow read, write: if false;
    }

    match /users/{userId} {
        allow read: if isUser(userId);
        allow create: if isAuth();
        allow update: if isUser(userId);
        allow delete: if isUser(userId);
    }
  }
}We only want the logged in user to be able to read their own document. For creating new users,  a user has to be logged in. For updates and deletes, we want to verify the logged in user matches the document.With that, you now have a functional auto-workflow that will create new user documents in Firestore whenever someone signs up to your application and the data will be protected.Expanding this out, you would add in additional checks to also allow admins to view and manage users as well, but that goes beyond the scope of this simple article.",1189
Lo-fi Music Is The Perfect Genre To Work & Study To,"Whenever I am working I listen to a wide-variety of different genres of music. My dominant genre is metal and other derivatives of heavy music which I enjoy. Then I also listen to blues as well as rap/hip-hop and instrumental music too.One of my more recent genre additions is Lo-fi. Admittedly, I am late to the party on the Lo-fi music genre, but it has been a game-changer for me and how I work. It’s not a genre of music I would have listened to a couple of years ago.To me, Lo-fi is like a modernised version of elevator music (only more innovative and not shitty), combined with other elements which make it the perfect background music that sits somewhere in the back of your mind and doesn’t distract you like other forms of music do.As you can see, interest in Lo-fi music has exploded in the last five years. As more people find themselves working from home and finding ways to relax and concentrate on work, you can see 2020 was a big year for Lo-fi music.While platforms like Soundcloud have increasingly seen more Lo-fi type tunes added, my goto platform for Lo-fi is surprisingly YouTube. People were uploading Lo-fi to YouTube five years ago before it was even really that popular.Perhaps one of the most popular mixes on YouTube is the Rainy Days In Tokyo mix.

It was uploaded in 2017, but the majority of comments are far more recent than that which is pretty indicative of recent interest. I would be interested in knowing how many of those 11 million views were in the first year or two of listening (probably not many).Another popular Lo-fi mix on YouTube is this one titled, 1 A.M Study Session.

And perhaps one of my favourites is this live stream YouTube mix, it just perpetually keeps playing inoffensive Lo-fi tunes. Whenever I do code live streaming on Twitch (follow me here), this is my goto video for background music, playing other forms of music usually results in my videos being muted for copyright matches.here

The one thing I find impressive about the video above is that it is relatively new in comparison to other videos and it has 2.1 million thumbs up and 43k thumbs down. That is an insanely good ratio of likes to downvotes for a video. Word of advice, avoid the comments section, it’s a cesspool of the worse the internet has to offer.To me, the appeal of Lo-fi is how bland it is. When you’re listening to it, you’re not being impressed by any stand out elements. Lo-fi is largely vanilla, it’s the John Smith and Plain Jane of the music world. But, like white noise, it perfectly taps into that part of our brains that craves repetition and calm.Do you listen to Lo-fi?",654
How To Make Face Masks,"In these uncertain pandemic times, seeing mask shortages and other shortages has our family thinking about self-sustainability. What can we rely on when supply chains fail? Not just food, but things like clothing and the hot topic right now: masks.A decent mask requires three layers of protection and material. You have the outer layer, the middle layer and then the inner layer. The ear loops are the final step, but they don’t offer protection.As mask shortages became an urgent topic, the likes of the World Health Organisation as well as numerous governments released guides on how you can make your own face masks during the pandemic.One of the guides that we found helpful, was one on the Australian Department of Health website here. If that link doesn’t work for you, let me know and I’ll send you the PDF.hereWhile it is possible to make a mask by hand sewing it, I highly recommend getting yourself a sewing machine to not only ensure a neater sew line, but to also ensure the pieces of fabric are correctly joined. The effectiveness of your mask will depend on how well those three layers are joined to one another.It is surprisingly easy to make your own mask, so if you can’t find one at the shops, making one of your own will be more effective than not wearing a mask whatsoever.",324
Why Remote Work Is Better,"I think many would agree that 2020 has been a terrible year on multiple fronts. One of the biggest dampers on 2020 was COVID-19, which has changed how we live, interact, work and go about our daily lives.Perhaps the biggest positive and upside to COVID-19 is the remote work revolution that was forced upon everyone. For some of us, we were ready for it and either already remote working or wanting too. Companies had a choice, allow their employees to work remotely or not work at all.You’ll find articles arguing for both sides. However, there are more upsides than downsides for remote work, which makes it the clear choice for those who want it.Saving MoneyThe biggest benefit of remote working is how much money you will save. It’s not until you’re no longer in a conventional office environment that you realise you spend more than you know on incidentals like coffee, lunches and other little pocket picking expenses. Then you have transport costs (public transport, fuel) as well as things like parking, it all adds up.And it’s not only employees saving money, it’s the businesses themselves. If you require less office space, you are saving on leasing fees, utilities like electricity and internet, in-office server space and redundancy solutions. Reducing overheads, especially in uncertain times is a win-win for everyone.Reduced DistractionsOffices are distracting. It doesn’t matter if you’re working in an open-plan office or you have your own cubicle, distractions in the office are rampant and they will always find a way to hunt you down.Either someone is constantly coming up to your desk (even with noise cancelling headphones on) and asking you for something, or loud phone calls and conversations are embedding themselves into the deepest parts of your ears.Impromptu meetings are also another distraction that will derail productivity faster than an air raid siren in a library.Remote Teams Get More Shit DoneMore flexibility leads to better productivity. Combined with reduced distractions, remote teams are less distracted and more productive: remote teams get more shit done.A traditional office is a 9-5 thing, some places offer “flexible” working hours, but what they really mean is, a majority of your office working hours still need to align with everyone else. Instead of starting at 9, some places might let you start at 10 instead.Everyone, especially in the development/programming industries works on different time cycles. I do some of my best work in the afternoons, so I tend to start later. Some do their best work early in the morning.We Were Already “Sort of” Working RemotelyStep into any tech office of developers and designers prior to the pandemic and the first thing you would have noticed is everyone is wearing noise cancelling headphones (a large majority anyway). When you realise that we were already working remotely, your perspective changes.If you are in an office and you’re talking to John who is a few meters away from you over Slack instead of an in-person conversation, that’s basically working remotely in the office.Many of my interactions before working remotely were done over Slack. In-person conversations in open offices are kind of frowned upon because of their destructive impact on those around them.A Wider Talent PoolHere is the often understated benefit of remote working, you’re not limited to your local city to find new talent. Hiring can be hard at the best of times, finding someone suitable for the position is tricky business. With remote work, you can hire someone from the other side of the world.This works both ways, once again. Companies have a wider hiring pool, but job seekers also are afforded the same benefit. This means you no longer have to pin your hopes on somewhere local, spin the globe and throw a dart and see where it lands.Employees Are Just HappierWhenever I would tell people I spend the majority of my working week working from home (this was prior to the pandemic), people were in awe and surprised. While many companies have been slowly adapting and offering some form of remote work, many do not.Even as some parts of the world return to normal, companies were itching to get people back into the office as quick as they could. This in part comes down to trust issues from management and bosses of the company too scared to trust their employees working remotely.It’s a good feeling knowing that company you work for trusts you  to do work even when not in an office, that they are affording you the flexibility to choose your start and finish times, and to get your work done. Trust makes everyone happy.When you work remotely, you get the sense you’ve got the best job in the world.Everyone Should Get A ChoiceUnless you’re knowingly applying for a 100% remote work company, some people don’t want to work remotely or work remotely all of the time. Everyone should get a choice. For companies who are not already permanent remote, they should offer employees the option of an office or remote.",1250
How To Make Slime,"COVID-19 has changed how we live and how we work, it has also changed how we parent. As parents have been thrown into the unknown as schools are closed or parents take their kids out over fears of bringing the virus home.My wife and I have two children; a five-year-old boy and a one-and-a-half-year-old girl. Keeping our active son entertained during moments of quarantine has been very challenging, to put it mildly.One thing our son loves to do is make slime. It’s one of those simple things that kids love and the slime itself once made will keep them entertained for days.The basics of slime are simple: glue and borax, with some water and food colouring mixed in.In terms of glue, you want a PVA based glue and one of the best and perhaps easily accessible glues is Elmer’s School Glue. Office Works has a 3.8l bottle of it for $30 here.herePour in about 120 millilitres of the glue into a bowl. If you are using the small bottle of Elmer’s glue (the squeezable kind, it’s about this much)Pour in 120 millilitres of warm water. If you want to add some drops of food dye, add them in here.Mix 1 teaspoon of Borax with 1/2 cup of warm water and dissolve. Pour it into the glue.Pour in about 120 millilitres of the glue into a bowl. If you are using the small bottle of Elmer’s glue (the squeezable kind, it’s about this much)Pour in 120 millilitres of warm water. If you want to add some drops of food dye, add them in here.Mix 1 teaspoon of Borax with 1/2 cup of warm water and dissolve. Pour it into the glue.Mix it all together thoroughly and you should see the slime start to form. That’s it. It’s simple and effective, the kids love it and it’s not overly messy either.",420
GitHub Was Never About Fun,"A few days ago I came across an article by Jared Palmer titled GitHub isn’t fun anymore besides the somewhat clickbait-y title he talks about the changes that GitHub has made to the trending section and how GitHub doesn’t feel fun any more.GitHub isn’t fun anymoreSure, the trending page is a cool little gimmick section where you can see popular repositories (or used to be able too), but GitHub was never about fun or non-code features. GitHub is a tool.Since Microsoft acquired GitHub they have introduced a lot of great new features, one of which I find extremely useful is GitHub Actions. The code review workflow is awesome, protected branches, free private repositories and more.Why does everything have to be gamified? I am 32 and part of a generation that has short attention spans and inability to do mundane tasks. Like children, my generation seemingly needs instant gratification, karma, scoreboards, points and other features to keep us engaged. The way that trending used to work was too easily gamed and did not necessarily mean the quality of the repos was good. I am glad they changed how it works, how it works now is properly more indicative of popularity than the previous way it worked.If you think GitHub isn’t fun, you should try Bitbucket, it is terrible and literally the worst source management platform around. You’ll know what funless really feels like using Bitbucket where projects go to die.",356
How To Deploy Aurelia 2 Apps To GitHub Pages (gh-pages),"You have yourself an Aurelia app (or you will soon), and you want to host it on GitHub Pages because GitHub provides a generous free hosting solution that gets powered from the Git repository itself.soonFortunately, the process couldn’t be more straightforward. A lot of this post will apply to other frameworks and libraries besides Aurelia 2. However, we will be focusing on Aurelia 2 only.This article assumes the following:This article assumes the following:You already have a Git repository for your Aurelia projectYou are using GitHub to host your Git repositoryYou already have a Git repository for your Aurelia projectYou are using GitHub to host your Git repositoryCreate a new branchWe need to create a new branch called gh-pages which GitHub will load our site from. If you use the command line, first run git branch gh-pages followed by git checkout gh-pages to switch to the gh-pages branch. If you’re using a GUI, follow the appropriate steps to create a new branch and switch to it.gh-pagesgit branch gh-pagesgit checkout gh-pagesgh-pagesModify .gitignoreIf you used the recommended way to initialise a new Aurelia 2 application, your .gitignore file by default will ignore the dist folder where your project files are built..gitignoredistInside of this file remove the entry ignoring dist and save it. Now commit and push your changes to the repository.distBuild & DeployBuilding your Aurelia 2 application is a simple matter of running npm run build (or appropriate build command) which will then build the files into the dist directory.npm run builddistOnce the build has completed, all you need to do is push up the changes to the dist folder to the gh-pages branch and GitHub will serve them at the following URL https://github-username.github.io/my-repo — where github-username is your GitHub username and my-repo is the name of your repository on GitHub.distgh-pageshttps://github-username.github.io/my-repogithub-usernamemy-repoGitHubTo push the contents of the dist folder run the following: git subtree push --prefix dist origin gh-pages and visit your site to see it in its deployed glory.distgit subtree push --prefix dist origin gh-pages",541
Fixing The Certbot Issue “The client lacks sufficient authorization/404 Not Found…”,"I am a huge fan of Let’s Encrypt and their free SSL certificate service using Certbot. However, recently whilst setting up a new domain name and attempting to get a certificate, I encountered an error I had never experienced before.The client lacks sufficient authorization :: The key authorization file from the server did not match this challengeIt couldn’t access the folder where it stored the secrets and was resulting in a 404 error. I manually created the folder and I could access it, so why Certbot couldn’t was a mystery.After some investigation and dead-end Googling, I found the problem and fixed it. I use Linode for my hosting and use the default DNS entries option when adding a new domain.Well, it turns out by default Linode will add IPv6 AAAA entries to the server and if you do not have Nginx configured to handle IPv6, it will not resolve properly. It looked something like this:It looked something like this:The culprit was the second entry for the domain with the weird value 2400:8902::f03c:91ff:fe59:f74c this is an IPv6 address and unless you have your server configured to support them, it’ll result in an error when trying to create an SSL certificate.2400:8902::f03c:91ff:fe59:f74cThe fix ends up being rather simple. Either update your server to support those types of addresses or remove the IPV6 entries from your DNS settings and make sure you wait a good 10-20 minutes before trying again.",356
Ditching Travis CI For GitHub Actions,"I have been using Travis CI for my continuous workflow needs for a very long time now. It does what it does and it does it well. However, Travis is an additional service you have to configure and login to, it is a bit disjointed from the code itself.When GitHub announced Actions, it was a game-changer. Essentially, it was Travis CI embedded into GitHub itself. Over time, the community have run with GitHub Actions and now there are numerous “recipes” to do tasks inside of your actions.When the feature was first announced and released, it had some teething issues. I actually gave up on GitHub Actions after running into issues I just didn’t bother working around and stuck with the tried and tested Travis CI.Fast forward to now and I have begun switching over my own projects to use GitHub Actions as well as numerous Npm packages and open-source projects I contribute too are also equally making the switch.To me, one of the biggest drawcards of GitHub Actions is they are coupled to the repository itself. I try and reduce the number of external dependencies in my projects as much as possible (within reason).In the past, I have run into issues configuring Travis CI. I once struggled getting Travis CI setup to rsync some files via SSH to a remote server, spending hours upon hours trying to use the Travis CLI to generate an encrypted file in the format it expects .enc I eventually got there, but it as painful..encIronically, I recently was tasked with creating a build in GitHub Actions that deployed via SSH and it was easy, using a script from the marketplace, I had it all working in five minutes. You can store the private key itself inside of a secret (which is not visible to anyone).I was never doing anything overly complicated in Travis CI, so I doubt I will notice any difference switching over. The speed of GitHub Actions does seem to be faster (in both spin up and build time), so that is a huge plus.",482
How To Generate An SSH Key and Add The Public Key To A Remote Server,"The thing with SSH authentication is I can never remember the steps to generate an SSH key, and then add that SSH public key to the remote server so SSH authentication works.I had all of this in a text file, but honestly, I reference my own blog for knowledge on how to do things all of the time, I thought I’d write up a quick post.You can find numerous blog posts on this, but I always seem to find a straightforward explanation to give me what I need, that I just consulted my text file on my desktop.Generating An SSH KeyThis will generate both private and public keypairs.ssh-keygen -t rsa -b 4096 -C ""johnsmith@gmail.com""
# Generates a new private and public keypair, using the email as the labelssh-keygen -t rsa -b 4096 -C ""johnsmith@gmail.com""
# Generates a new private and public keypair, using the email as the labelYou’ll be asked to enter a keyphrase. Personally, I don’t use keyphrases for my keys (I know I probably should). So, I skip the following.Enter passphrase (empty for no passphrase):
Enter same passphrase again:Enter passphrase (empty for no passphrase):
Enter same passphrase again:For the key names, by default it’ll generate id_rsa and id_rsa.pub but you can name these whatever you want. Because I am dealing with CI providers like Travis CI and GitHub Actions, I generate keys every time I do something with a server.id_rsaid_rsa.pubAdd Your Public Key To The Remote ServerBasically, we copy the contents of the public key and store it in the authorized_keys file in the .ssh folder on the server.authorized_keys.sshcat ~/.ssh/id_rsa.pub | ssh username@domain.com 'cat >> ~/.ssh/authorized_keys'cat ~/.ssh/id_rsa.pub | ssh username@domain.com 'cat >> ~/.ssh/authorized_keys'If you kept the default name, keep id_rsa.pub as the key name. For username@domain.com add in your server username and the server domain name or IP address. The second string part just copies the contents of the file into the authorized_keys file on the server.id_rsa.pubusername@domain.comauthorized_keys",503
How To Add Feature Flags Into Your Javascript Applications,"Feature flags are a great way to prevent stale branches by regularly shipping features in your code without officially enabling them. A feature flag let’s you turn code on and off, in the case of features, a feature flag means you can regularly merge branches and release them.While there are many different ways you can approach this, one of my favourite and most simple approaches is a features.json file in your application.features.jsonIt can be something as simple as a JSON file of properties and boolean values.{
    ""feature1"": true,
	""feature2"": false
}{
    ""feature1"": true,
	""feature2"": false
}Or in the case of something more flexible and less straightforward it can be something like this where we can specify specific roles who are allowed to use a certain feature.{
    ""feature1"": {
        ""roles:"" [""admin"", ""editor""]
        ""enabled"": true
    },
    ""feature2"": {
        ""enabled"": false
    }
}{
    ""feature1"": {
        ""roles:"" [""admin"", ""editor""]
        ""enabled"": true
    },
    ""feature2"": {
        ""enabled"": false
    }
}In your code, you import the features.json file and then reference the features. Depending on your framework or environment, this will look different.features.jsonimport features from './features.json';

const FeatureOne = features.feature1 ? import ('./features/feature-one') : null;import features from './features.json';

const FeatureOne = features.feature1 ? import ('./features/feature-one') : null;If you were to do this in a framework like Angular or Aurelia, you would create a service or middleware of some kind which compares the route against the property in your features file and reacts accordingly.import features from './features.json';

export class FeatureService {
    getFeature(key) {
        return features?.[key] ?? null;
    }
}import features from './features.json';

export class FeatureService {
    getFeature(key) {
        return features?.[key] ?? null;
    }
}There are a few feature flag services out there which provide SDK’s and ways to turn features on and off, do A/B testing, but for most use-cases, you don’t need anything else other than a JSON file and a little code to wire it all up.",546
How To Do Prepared Statement LIKE With SQLlite 3 and The Better SQLite 3 Package,"I love SQLite and I am using it in my Aurelia 2 book for the server aspect to provide users with a real server backed by a database. I am using the equally brilliant Better SQLite 3 library which allows you to work with SQLite 3 databases in a performant and easy way.Better SQLite 3 libraryThis is how I did a prepared statement for my LIKE query which searched products by title in my database. The statement itself needs LIKE ? and then on the all method we provide the argument. LIKE ?allUsing the template literal syntax with backticks, we handle our wildcard syntax and value in there. You can use whatever pattern you want here, but my use case needed %${query}%%${query}%const db = sqlite('mydatabase.db');

const rows = db.prepare(`SELECT * FROM products WHERE title LIKE ?`).all(`%${query}%`);const db = sqlite('mydatabase.db');

const rows = db.prepare(`SELECT * FROM products WHERE title LIKE ?`).all(`%${query}%`);It’s simple and it works beautifully. Presumably, this would also work for other SQLite 3 libraries, but I highly recommend the Better SQLite 3 library for working with SQLite databases.",278
How To Get Last 4 Digits of A Credit Card Number in Javascript,"Recently whilst working on my Aurelia 2 book, for the example application where you checkout I needed to add in the ability to provide a card number and when it saves, the last 4 digits of the card number get saved.This is one of those things that as developers we won’t do too often, but it’s easy using substr to trim a text string to a certain length.substrlet ccNumber = '4560265043620583';
let lastFourDigits = ccNumber.substr(-4);let ccNumber = '4560265043620583';
let lastFourDigits = ccNumber.substr(-4);By providing -4 as the value to substr you’re telling it to take the last 4 characters from the string. The result from our example will be 0583.  You can also provide non-negative numbers to go from the beginning as well.-4substr0583I am sure there are other ways of doing the same thing, but I always look for the easiest to read and easiest to understand solutions and substr fits the bill nicely.substr",230
Announcing The Aurelia 2 Book,"Buy the Aurelia 2 book here.Buy the Aurelia 2 book here.hereWith the Aurelia 2 alpha coming very shortly, I have had plans for a while to write another Aurelia book, this time around on Aurelia 2. I learned a lot writing my first book and admittedly, made a few mistakes. The learning experience was invaluable.With my first book, it came at a time when the Aurelia documentation was subpar. The book served as more of a stand-in for the lack of detailed and concise documentation. With Aurelia 2, extensive documentation work has been undertaken to the point where a book telling you about every little thing makes no sense.The Aurelia 2 documentation which is updated regularly can be found here if you are wondering where it is located.hereThis time around, I wanted to write a book that is different from the first, something fun. So, I set out one night to start writing the Aurelia 2 book, not really having a clear goal in mind and after a couple of days, I had one.The book will touch upon the fundamentals without parroting the documentation too much, it will support TypeScript and you will build an application using Aurelia 2 to get acquainted. The app you will be building is an online cat pictures store, where you can buy pictures of cats.What You’ll LearnHow to generate a new Aurelia 2 app from scratchHow to configure Aurelia 2, globalising resources and including pluginsHow to work with the new direct routerHow to structure your Aurelia appsHow to adopt a component-driven mindset to developing complex UI’sHow to leverage new Aurelia 2 features and API’sHow to write unit tests using Jest as well as mocksHow to test Aurelia 2 components, staging them, querying HTMLHow to write end-to-end tests with CypressHow to generate a new Aurelia 2 app from scratchHow to configure Aurelia 2, globalising resources and including pluginsHow to work with the new direct routerHow to structure your Aurelia appsHow to adopt a component-driven mindset to developing complex UI’sHow to leverage new Aurelia 2 features and API’sHow to write unit tests using Jest as well as mocksHow to test Aurelia 2 components, staging them, querying HTMLHow to write end-to-end tests with CypressThe book aims to go beyond just being solely about Aurelia 2, giving you practical skills.What You’ll BuildAn online store for selling cat pictures. It’ll have a homepage, category page, a detail page, a checkout screen and logged in/logged out functionality as well. An accompanying local server running SQLlite will allow your app to feel real as things are persisted.The application will touch upon how you can add in authentication, how you can create routes, creating secured sections and other fundamentals that will translate across to actual real-world applications.By the end of the book you’ll have a semi-real online store driven by Aurelia on the front-end, complete with tests and all. The idea is you’ll learn Aurelia 2 by building an application piece-by-piece.A Work In ProgressPlease be aware that the book structure is still being finalised, chapters added and moved around. There will be typos, mistakes in the code and possibly even things that change in Aurelia 2 itself which get removed or added to the book.As Aurelia 2 evolves in development, so too, will the book. For the entire life of Aurelia 2, the book will be updated in step to remain an up-to-date resource. You only purchase once and all future updates to the book are free.only purchase onceall future updates to the book are freeBuy the Aurelia 2 book here.Buy the Aurelia 2 book here.here",891
Level Up Aurelia Store With pluck and distinctUntilChanged,"Aurelia Store is a powerful state management library for your Aurelia applications, but behind-the-scenes it merely wraps the powerful RxJS observable library. This means you can use a plethora of RxJS methods to change how Aurelia Store works.Problem: Aurelia Store will fire all state subscribers regardless of changeWhenever your state changes, all listeners of the state object will be fired. While smaller applications won’t introduce any noticeable differences, as your application grows in size and complexity, depending on what you’re doing inside of those store subscribers you can run into some issues.Sometimes you only want code to run inside of an Aurelia Store subscription if it needs too, akin to some kind of if statement that checks if the code needs to be run.For example, if you have a isLoading property in your store that changes depending on whether or not something is loading, any code that watches this property should only fire when it changes, not whenever the store’s state changes. Why should isLoading checks care if you’ve loaded a bunch of users or products data?isLoadingisLoadingSolutionUsing the RxJS methods pluck and distinctUntilChanged, we can tell our subscriptions to only fire if a specific property in our store has changed. The pluck method allows us to tell RxJS to only watch a specific property in our state object. For the above loading example, we would want to “pluck” the isLoading property pluck('isLoading').pluckdistinctUntilChangedpluckisLoadingpluck('isLoading')Lastly, we want to use the distinctUntilChanged method which accepts no arguments. All it does is takes our “plucked” value and compares it to its previous value to see if it changed or not.distinctUntilChangedimport { distinctUntilChanged, pluck } from 'rxjs/operators';

this.store.state.pipe(pluck('isLoading'), distinctUntilChanged()).subscribe((isLoading) => {
    // The isLoading property changed
});import { distinctUntilChanged, pluck } from 'rxjs/operators';

this.store.state.pipe(pluck('isLoading'), distinctUntilChanged()).subscribe((isLoading) => {
    // The isLoading property changed
});Now, this works great for most use cases. However, as documented in the distinctUntilChanged the documentation details a caveat on how the check works you need to be aware of.distinctUntilChangeddistinctUntilChanged uses === comparison by default, object references must match!distinctUntilChanged uses === comparison by default, object references must match!===If you want to do a check based on an object property, you need to use the distinctUntilKeyChanged method which is not covered in this post.distinctUntilKeyChangedReally, that is all there is to it. You pluck a property and then you do a distinct check to react whether or not the value has changed. This is all just RxJS code and nothing overly Aurelia Store specific. There are a tonne of other RxJS operators you should read up on.",730
"How To Fix HiveJS AssertionError “Expected version 128, instead got 149” and “Expected version 128, instead got 38”","If you are trying to use any methods in the HiveJS client which require the use of a private key, you might have encountered one or both of these errors. In my case, I was using the memo encode method which takes a private key as the first argument, unfortunately, pass it the wrong key and you’ll get a non-helpful AssertionError about an expected version.You’re seeing the error because you’re providing the wrong key. If you get the error, “AssertionError: Expected version 128, instead got 38” it means you are trying to use a public key when you need to use a private key. AssertionError: Expected version 128, instead got 38In my instance, the error, “AssertionError: Expected version 128, instead got 149” was because I was attempting to use my account password instead of my private memo key to encode the memo string.AssertionError: Expected version 128, instead got 149These errors really need to be changed to be more helpful, right now they’re as cryptic as the functionality itself.",249
Forget CSS-In-JS: Combine Sass With CSS Modules Using Webpack,"I really dislike the CSS-In-JS trend. Nothing against anyone who is a fan, but writing CSS inside of Javascript doesn’t feel natural and honestly, it’s just an unnecessary abstraction. I understand why it became a thing, but the problems CSS-In-JS promises to solve have already been solved thanks to CSS Modules and Shadow DOM.CSS ModulesShadow DOMAs usual, the front-end development community are focused on tooling and not on the end-user experience. Numerous benchmarks have proven CSS-In-JS can introduce performance issues into your application. For cross-platform development, I will not dispute CSS-In-JS solutions can be useful, but many of us only target one platform most of the time. The thought of having to write components for even the most simple of UI tasks sounds daunting.I am a big fan of using CSS Modules in my projects. They solve the naming issue in CSS giving you the same result as something like Styled Components, only you get to write CSS inside of CSS files. In combination with a pre-processor, even better.Separating module and non-module SCSSWhen using CSS Modules in your application, it is considered best practice to separate your module and non-module CSS styles. If you treat all CSS/SCSS as a CSS Module, when working with third-party packages with CSS/SCSS such as Bootstrap, you will run into problems.I like to treat all CSS/SCSS as non-module by default. For CSS Modules, you want to specifically opt-in to use the modules functionality. This way you can have traditional global CSS and when you want localised component styles, you opt-in.For module styles I like to use the .module.scss or .module.css naming convention. Inside of my components I will do something like this:.module.scss.module.cssimport styles from './mycomponent.module.scss';import styles from './mycomponent.module.scss';In my case, I am using autoprefixer as well as cssnano to add browser prefixes and de-duplicate my CSS styles when imported. Your webpack.config.js configuration might look slightly different to this, alter to your taste.autoprefixercssnanowebpack.config.jsDefine the loadersconst cssModuleRules = [{
        loader: 'css-loader',
        options: {
            importLoaders: 2,
            sourceMap: true,
            modules: {
                localIdentName: '[name]__[local]____[hash:base64:5]',
            },
        }
    },
    {
        loader: 'postcss-loader',
        options: {
            plugins: () => [
                require('autoprefixer')(),
                require('cssnano')()
            ]
        }
    }
];

const cssRules = [{
        loader: 'css-loader'
    },
    {
        loader: 'postcss-loader',
        options: {
            plugins: () => [
                require('autoprefixer')(),
                require('cssnano')()
            ]
        }
    }
];const cssModuleRules = [{
        loader: 'css-loader',
        options: {
            importLoaders: 2,
            sourceMap: true,
            modules: {
                localIdentName: '[name]__[local]____[hash:base64:5]',
            },
        }
    },
    {
        loader: 'postcss-loader',
        options: {
            plugins: () => [
                require('autoprefixer')(),
                require('cssnano')()
            ]
        }
    }
];

const cssRules = [{
        loader: 'css-loader'
    },
    {
        loader: 'postcss-loader',
        options: {
            plugins: () => [
                require('autoprefixer')(),
                require('cssnano')()
            ]
        }
    }
];We have two CSS Loader configurations here, one for CSS Modules and one for standard CSS.The following loader configurations going inside of the module rules property.rules{
    test: /\.css$/i,
    use: extractCss ? [{ loader: MiniCssExtractPlugin.loader }, ...cssRules] : ['style-loader', ...cssRules]
},
{
    test: /\.scss$/,
    exclude: /\.module\.scss$/,
    use: extractCss ? [{ loader: MiniCssExtractPlugin.loader }, ...cssRules, ...sassRules] : ['style-loader', ...cssRules, ...sassRules],
},
{
    test: /\.module\.scss$/,
    use: extractCss ? [{ loader: MiniCssExtractPlugin.loader }, ...cssModuleRules, ...sassRules] : ['style-loader', ...cssModuleRules, ...sassRules]
}{
    test: /\.css$/i,
    use: extractCss ? [{ loader: MiniCssExtractPlugin.loader }, ...cssRules] : ['style-loader', ...cssRules]
},
{
    test: /\.scss$/,
    exclude: /\.module\.scss$/,
    use: extractCss ? [{ loader: MiniCssExtractPlugin.loader }, ...cssRules, ...sassRules] : ['style-loader', ...cssRules, ...sassRules],
},
{
    test: /\.module\.scss$/,
    use: extractCss ? [{ loader: MiniCssExtractPlugin.loader }, ...cssModuleRules, ...sassRules] : ['style-loader', ...cssModuleRules, ...sassRules]
}We have a CSS rule for being able to deal with standard CSS, not using modules. Then we have a scss rule for non-module scss styling. And lastly, we have a rule for module styles using our CSS Modules configuration.scssIf you wanted to use CSS Modules with standard CSS, you could easily add another rule testing for .module.css to allow that as well..module.cssThat is all there really is to it. Now, you have CSS Modules allowing locally scoped CSS that will have randomly generated class name strings, allowing you to write specific CSS without global naming conflicts.",1329
The Australian CovidSafe App Doesn’t Even Work On iOS Properly,"And the government wonders why people were sceptical of the CovidSafe rollout (besides the very real safety concerns). It seems the CovidSafe rollout is flawed, with the discovery that the iOS version doesn’t even work properly.Software developer Joshua Byrd recently posted his findings on Twitter and they’re pretty damning. Basically, the app will only ever work in the foreground with the screen on for iOS users.posted his findingsOn a technical level as explained in further Tweets, the phone will not broadcast UUID’s unless it is open. Coincidentally, the same issue was reported with Singapore’s app which CovidSafe is based upon and seemingly, nobody learned their lesson from.Basically, unless you are aware of the power saving mode which just makes you leave the app open in your pocket upside down meaning you can’t use any other apps, the app itself is useless for a majority of iOS users.Apple and Google have been working on their own OS level contact tracing API’s and presumably, it would be in the interest of the Australian government to migrate over to these API’s asap. Not only from a technical perspective, but also to alleviate privacy concerns users have over the app being used in the future as a surveillance tool.",311
Why Is Voat Still A Thing?,"Remember Voat? The non-censored alternative to Reddit that saw an influx of users in 2015 after Reddit started cleaning up its house a bit and claims of censorship became a hot button issue that drove people away from Reddit.VoatThe site has now become an anti-vaxxer, conspiracy theorist platform that resembles an uncontrollable raging dumpster fire. Although to be fair, the site was always teetering on the edge even in the beginning.After Reddit started banning subreddits such as those dedicated to PizzaGate, many moved over to Voat. And when the whole QAnon thing came to be and Reddit subsequently banned that community, they also moved to Voat.As you can see, Voat sure knows how to attract the right people to its platform. I haven’t been to the site in years, so it was an interesting experience to go and see what constitutes front-page material these days and well, the screenshots speak for themselves.Given the COVID-19 pandemic is a hot button issue right now, it is not unexpected to see a tonne of paranoid conspiracy theories surrounding vaccinations and COVID-19.My question is, how is Voat even still alive? Many of its subverses (sub communities) seem to be completely dead like /v/javascript and the only ones that seem to be thriving are the conspiracy theory sub communities./v/javascript",329
I Am Starting To Lose Respect For Elon Musk; Has He Become Unhinged?,"The signs have been there for a while now. Elon Musk is the eccentric real-life version of Tony Stark, who some believe will save the world with his forward-thinking investments and ideas like making electric vehicles cool or making his own rockets.But, for all of Elon’s great and commendable achievements come some highly questionable decisions and remarks. Known for his non-corporate approach to communication, he has landed in hot water several times over the years.And I say non-corporate approach, but what I mean is his childish approach to communication. Musk is a giant man-baby who just so happens to have hit the jackpot in the lottery of life. One of the most notable instances is when he called a scuba diva who helped rescue some kids trapped in a Thai cave a “pedo”. It landed him in court.Then there are Elon’s run-ins with the SEC for his Tweets which have been perceived as market manipulation. It doesn’t take a financial genius to see that the intentions behind Elon’s Tweets are a form of manipulation.The Coronavirus panic is dumb, will go down as one of the stupidest things such an intelligent person has ever said. Then he voiced his doubts about the connection between COVID-19 and the deaths in Italy. Were all of those deaths crisis actors? Because I am sure the families of those lost to the pandemic would think otherwise.The Coronavirus panic is dumbvoiced his doubtsAnd more recently, Musk called for America to be “freed” in a Boomer-style all-caps Tweet, FREE AMERICA NOW. The kind of remark that wouldn’t sound out of place at a Trump MAGA rally or pro-gun lobby-funded event deriding legislation to take away their guns. You only have to look at some of the responses agreeing with Elon affiliated with the “right”, many of who are self-described patriots according to their bios.FREE AMERICA NOWIt’s evident that Elon Musk has become the new conservative hero. A slightly more intelligent and younger version of Donald Trump. I honestly wouldn’t be surprised if Elon attempts to run for president in the next decade. Because, that’s what the world needs, more con artist billionaires in higher positions of power.Many would argue that Elon has never been completely hinged, to begin with. The awkward genius that seemingly can do no wrong amongst with die-hard fanbase.",577
The New Imgflip AI Meme Generator Is Fun (and dark),"I love a good meme and when I encountered Imgflip’s AI meme generator recently, even more so. As I cycled through the generator I noticed that it started producing some interesting memes.AI meme generatorThe site describes the process for obtaining data to produce some of the AI memes:The network was trained using public images generated by users of the Imgflip Meme Generator for the top 48 most popular Meme Templates. Beware, no profanity filtering was done on the training data so you may encounter vulgarity.The network was trained using public images generated by users of the Imgflip Meme Generator for the top 48 most popular Meme Templates. Beware, no profanity filtering was done on the training data so you may encounter vulgarity.I have spent a lot of time just randomly generating memes, more than I would like to admit. It produces some questionable memes and some good ones as well.The Eric Andre “why would” meme perhaps produced some of the darkest memes of all.One of my favourite of the bunch is this one, which seems quite accurate:And finally, one of my favourites:You will never be a good communist.",281
How To Centre Columns In Bootstrap 4,"It’s 2020 and while there are many things I do know, there are still some trivial silly things I do not or always forget. Case in point being, how to centre columns in a Bootstrap 4 layout.Unlike Bootstrap 3, Bootstrap 4 uses Flexbox and therefore, you have more options for centring your layouts. One such method is the justify-content-center class defined on the row which will align the columns center. Finally, you use the text-center class to middle align the content of the column.justify-content-centertext-center<div class=""container"">
    <div class=""row justify-content-center"">
        <div class=""col-6 text-center"">
            <p>My content</p>
        </div>
    </div>
</div><div class=""container"">
    <div class=""row justify-content-center"">
        <div class=""col-6 text-center"">
            <p>My content</p>
        </div>
    </div>
</div>This works by using the Flexbox property for justifying the content. We can also specify a column is display: flex; and align its contents like so as well, which can work well in certain situations.display: flex;<div class=""container"">
    <div class=""row"">
        <div class=""col d-flex justify-content-center"">
            <p>My content</p>
        </div>
    </div>
</div><div class=""container"">
    <div class=""row"">
        <div class=""col d-flex justify-content-center"">
            <p>My content</p>
        </div>
    </div>
</div>In the above, you might notice we don’t use text-center to center the text, we just make the column d-flex which makes it a flex element and its children and be aligned.text-centerd-flexNothing overly fancy. This post serves as more of a reminder to myself, but if you find it useful then that is a win as well.",428
Level Up Your Aurelia Applications With Router Layouts,"When it comes to building applications, inevitably you will encounter situations where the base markup of your page is different than other parts of your site. The base structure of your front-facing pages like the homepage might be different from your administration panel or user dashboard, which might look similar but work very much differently.Surprisingly, I don’t see a lot of developers who work with Aurelia opting to use them. I think Router Layouts are a forgotten feature to some, requiring a little upfront work which can net you a long-term negation of pain when trying to make your markup work.A layout at its core is a dynamic template with one or more &lt;slot&gt; elements inside of it acting as content projectors. You have the one base &lt;router-view&gt; element like you usually would, except on your routes you can define a new layout.&lt;slot&gt;&lt;router-view&gt;Inside of your app.html or whatever file is your main application entry point template, you start off with something like this:app.html<template>
    <div>
        <router-view layout-view=""default-layout.html""></router-view>
    </div>
</template><template>
    <div>
        <router-view layout-view=""default-layout.html""></router-view>
    </div>
</template>Now, create our default layout and call it default-layout.htmldefault-layout.html<template>
    <slot></slot>
</template><template>
    <slot></slot>
</template>Now, inside of our app.js file we want to define a couple of routes:app.jsexport class App {
  configureRouter(config, router) {
    config.map([
        { route: '', name: 'home', moduleId: 'home' },
        { route: '404', name: '404', moduleId: '404', layoutView: 'error-layout.html' }
    ]);

    this.router = router;
  }
}export class App {
  configureRouter(config, router) {
    config.map([
        { route: '', name: 'home', moduleId: 'home' },
        { route: '404', name: '404', moduleId: '404', layoutView: 'error-layout.html' }
    ]);

    this.router = router;
  }
}Now, let’s create our 404 view and view-model pairs. Firstly, 404.js:404.jsexport class FourOhFour {
}export class FourOhFour {
}Then, 404.html:404.html<template>
    <h1>404</h1>
    <p>Page not found</p>
</template><template>
    <h1>404</h1>
    <p>Page not found</p>
</template>Now, we need our layout for error pages which we will call error-layout.htmlerror-layout.html<template>
    <div class=""h-100"">
        <div class=""center"">
            <slot></slot>
        </div>
    </div>
</template><template>
    <div class=""h-100"">
        <div class=""center"">
            <slot></slot>
        </div>
    </div>
</template>By default, our app will use the default view layout supplied on the router-view element, but we can override this on a per-route basis. Furthermore, you can also supply a view-model as well as data to pass through and even define viewports.Rather than going through it all, the official documentation does a great job explaining these other properties and concepts for you here. In most cases, a basic HTML only view template is what you want, but for more complicated scenarios the view-model and data properties are helpful.here",789
"I Used To Hate Using Reduce In Javascript, And Now I Can’t Stop","I used to be vehemently against using `reduce` in Javascript. It wasn’t because I thought there was a better way, I actually think the functional programming purists and their aggressive approach to advocacy are what really turned me off. That and those who advocate for using it over a basic for loop as well.Now, there are instances where I truly believe that filter, map and reduce should not be used. If you’re just wanting to quickly iterate over a collection of values, in many instances. a for..loop will beat all of those aforementioned methods in performance every single time. And if you’re dealing with large collections of items, you will notice.filtermapreducefor..loopThe reduce method is great for reducing values to either a singular value or manipulating the shape of data from one form to another. If you want to flatten an array, look no further than reduce. Want to average out a bunch of numbers and return a singular value? Oh, hi there reduce.reducereducereduceHere is an actual snippet of code I wrote not long before writing this post:return rows.reduce((arr, row) => {
    row.contractPayload = JSON.parse(row.contractPayload) ?? {};
    arr.push(row);
    return arr;
}, [])return rows.reduce((arr, row) => {
    row.contractPayload = JSON.parse(row.contractPayload) ?? {};
    arr.push(row);
    return arr;
}, [])I am taking a bunch of rows from an SQLlite database, iterating over them and parsing a property which is stringified JSON, parsing it into an object or empty object if it doesn’t parse properly.If I was one of those fancy developers that used shorthand for everything, I could have used the spread operator to really condense this code down into a one liner (inside the callback), but I am far too practical and prefer writing readable verbose code in most situations.For a long time I used to be the type of developer who would create a variable with an empty array inside of it and use a for loop to push into the array, which gives you the same result. I instinctively find that I naturally will reach for reduce over a traditional for loop whenever possible. In instances where I notice substantial performance issues, I’ll write a loop.reduceThe real power and value of reduce is that it negates the need for unnecessary code. I guess they call it reduce because it reduces the amount of code you need to write as well. Neat.reduce",595
I downloaded the Australian government’s CovidSafe Tracking App (so you don’t have too if you don’t wanna),"The controversial Australian government contact tracing application based on the Singapore version has finally been released for Australians. Understandably, a lot of people are concerned about their privacy and whether or not the government messed this up.I had a spare phone lying around, so I installed the application for the lols. I decided to see if I could find anything nefarious with the app or if it drains my battery like Singaporeans reported their app did.I am talking about the Android version in this article, presumably the Apple version is also the same, albeit when we speak about permissions, iOS offers its own set of permissions that developers can request.Firstly, I commend the Australian government to an app which doesn’t look terrible. Usually, government built apps look terrible and work terribly, probably because it’s not a from-scratch application and based on Singapore’s app TraceTogether.TraceTogetherThis is not going to be a technical deep-dive on the application or decompiling it and discussing lines of code. If you’re looking for the decompiled source, the community has already started doing that here. Although, the Australian government says the code will be made open source, who knows if or when that will be happening.hereIt appears on the surface the app is quite unremarkable. Like the Singapore version, it encrypts your data locally on your device and only through a request can you allow your data to be decrypted. Right now as the app currently stands, it seems safe. However, I have valid concerns which you should also have.Does CovidSafe Drain My Battery?From what I could see running it for a few hours, no. While I did not go out and try and get it to make a “trace handshake” with anyone else, it appears the app doesn’t use that much battery. In the few hours I used it, I saw a few percent drop which is nothing compared to Spotify, Pokemon Go or YouTube.The application also appears to work fine in the background on Android as well. You get a notification telling you the app is active in the background and that’s it. There is no need to run the app and have it the only one open meaning you can interact and use other apps and features on your phone.ACCESS_FINE_LOCATIONWhile the app only appears to access Bluetooth right now and uses it to determine the vicinity you are in relation to other app users, the app asks for the ACCESS_FINE_LOCATION permission on Android. If you read up on this permission here in the Android documentation, pay close attention to what it provides.ACCESS_FINE_LOCATIONhereAllows the API to determine as precise a location as possible from the available location providers, including the Global Positioning System (GPS) as well as WiFi and mobile cell data.Allows the API to determine as precise a location as possible from the available location providers, including the Global Positioning System (GPS) as well as WiFi and mobile cell data.Without fearmongering, I want to point out that the application at the time of writing this only ever uses Bluetooth. The ability to use GPS or anything else for location does not appear to be in the current application.In theory, there is nothing stopping the Australian government from making an update which allows them to also track your location via GPS. This permission gives the app the ability to track GPS data if it wanted too, but I want to stress again, the app does NOT currently do this.Concerns About PrivacyThe CovidSafe application as far as I can see is safe. It does everything that the government said it would and it also appears to not be sending anything off in secret to the government or tracking your movements. However, I implore you the reader to be sceptical for a few reasons.The first reason relates to the government’s mandatory data retention legislation. Law enforcement agencies are being given browser history of people under investigation, despite the fact the legislation specifically excluding it.being givenGiven the LNP passed such legislation in 2015 and ever since there have been numerous reports and instances of metadata being incorrectly given to agencies, people and agencies getting access to data that they shouldn’t and overall confusion around how the scheme and subsequent legislation works.Since the introduction of this legislation, there has also been significant scope creep where more and more agencies and bodies are able to access metadata. (from Greyhound Racing Victoria to Consumer Affairs and various local councils). If you think only law enforcement and national security agencies can access your metadata, think again.A very real scenarioThe second and most important reason relates to the controversial encryption weakening legislation that was passed in 2018. Given everything that has happened since then, it’s easy to forget this legislation passed and still exists.encryption weakening legislationIf you think that the concerns around privacy and data are invalid, looking no further than law enforcement agencies asking for added capabilities to be added into the application. A request which the Morrison government knocked back, but the fact they even asked in the first place should concern you.asking for added capabilitiesHere is where things get muddy and it’s a concern that not even the government can reassure citizens on. The encryption weakening legislation passed in 2018 could in theory, allow the government to add in a backdoor or added features to the application in a stealth update and nobody would be allowed to say anything.All of the legal provisions are there to allow the Morrison government to enable this application (with a few lines of added code) to become something that can track your location through GPS given the permission to do so has already been granted.Legally speaking (I am not a lawyer) the interpretation of all of this is the promised safeguards for the CovidSafe app would NOT override the backdoor encryption legislation or any other established legislation if they were to ever conflict with one another. As such, the reassurances and promises that protections are in place for CovidSafe are nothing more than empty words.While I do not doubt that this app could be incredibly effective in helping trace COVID-19 infections and spread, there are just too many unknowns for me to consider installing this app. At the end of the day, make the decision that feels right to you and do not let this post or anyone else’s opinion do anything more than inform you and allow you to make your own decisions.",1638
How To Mock uuid In Jest,"When it comes to mocking dependencies in Jest, it couldn’t be easier. You can create an actual mock module that gets loaded in place of the dependency in your app or you can do it manually by mocking the implementation or module.import { SomeClass } from './some-file';

// Mock the uuid module and v4 method
jest.mock('uuid', () => ({ v4: () => 'hjhj87878' }));

describe('Test Case', () => {
    let sut;

    beforeEach(() => {
        sut = new SomeClass();
    });

    test('My Test', () => {
        expect(sut.returnUuid()).toReturn('hjhj87878');
    });

});import { SomeClass } from './some-file';

// Mock the uuid module and v4 method
jest.mock('uuid', () => ({ v4: () => 'hjhj87878' }));

describe('Test Case', () => {
    let sut;

    beforeEach(() => {
        sut = new SomeClass();
    });

    test('My Test', () => {
        expect(sut.returnUuid()).toReturn('hjhj87878');
    });

});For reference, our SomeClass implementation looks like this:SomeClassimport { v4 as uuidv4 } from 'uuid'; 

export class SomeClass {
    returnUuid() {
    	return uuidv4();
    }
}import { v4 as uuidv4 } from 'uuid'; 

export class SomeClass {
    returnUuid() {
    	return uuidv4();
    }
}To explain what we are doing here, we will use bullet points:The class we want to test is importing the v4 function which generates our guids from the uuid packageWe have a method called returnUuid which calls the function and returns the generated guidInside of our test file, we mock the v4 function inside of the uuid package by re-implementing it and returning a mocked value we can test forOur test case checks this mocked value is what is returnedThe class we want to test is importing the v4 function which generates our guids from the uuid packagev4uuidWe have a method called returnUuid which calls the function and returns the generated guidreturnUuidInside of our test file, we mock the v4 function inside of the uuid package by re-implementing it and returning a mocked value we can test forv4Our test case checks this mocked value is what is returnedIt is worth noting that this article previously detailed how you can mock the uuid package using jest.spyOn, however, some changes to the uuid package meant that older method of using spies to mock functions no longer works.",571
Dinnerly Australia — COVID-19 Review,"As I explained in my HelloFresh review here, we have been trying out at home meal kits because of the shortages of basics in the grocery stores here in Australia such as; mince, flour, eggs and so on.review hereAfter trying HelloFresh, we decided to try out Dinnerly which markets itself as, “Australia’s most affordable home dinner kit” it’s actually owned by Marley Spoon, just a cheaper version for those who cannot afford Marley Spoon which is one of the more expensive options.First ImpressionsThe ordering experience itself was smooth. Enter your details and then choose your meals, it’s a similar story to HelloFresh and presumably every other meal kit service out there.The menu we were shown for the week we were ordering was a stark contrast to HelloFresh. The affordability aspect of Dinnerly is most of their meals have just six ingredients and basic ones.We got a box with four meals, and two portions in each. We assumed like HelloFresh, the portions would be massive and feed my wife and I as well as our two young kids.On the surface, this all looks great. I was particularly excited for the tacos and if you’re wondering why I rated them 1 star, keeping reading because I explain why further down.You get what you pay forLike anything in life, when you pay less you get less. With Dinnerly the meals are the kinds of things that anyone with a copy of Jamie Oliver’s cheap meal recipe books would expect to make. Chillis, curries and chicken/veg.The first sign you have ordered a cheap meal kit is the packaging. This is no HelloFresh, the ingredients are kind of just thrown into the small box that ships to your door. Nothing is categorised by colour or any system, you have to dig through and find what you’re looking for.Quality-wise, the produce just didn’t look that good. The vegetables and salads just looked less fresh than what you might find in a supermarket. Allegedly they come straight from the farm, which is maybe an acronym for some large freezer where they keep everything stored.Our favourite meal by far was the parmesan meatloaf, it was a pleasant surprise given we are not big meatloaf eaters. It was actually a really nice meal, we had this on the first day. It was a nice introduction to Dinnerly, until the subsequent nights.Weird pantry staplesBecause you get less in the box, you’re required to provide more pantry staples beyond oil, salt and pepper. For the Chimichurri Chicken recipe, this is what the recipe asks you to provide; red wine vinegar, 1 garlic clove, honey and olive oil from the pantry.  The Indian Halloumi Curry required; 2 garlic cloves, olive oil and tomato paste.Perhaps the most demanding of all of the recipes in terms of pantry staples was the Beef and Parmesan meatloaves. This recipe required you to provide; 1 egg, olive oil, tomato paste, Worcestershire sauce and tomato sauce.This is a recurring theme with Dinnerly. They might ship you 6 ingredient meals, but they require you to have another six ingredients in your pantry. The oil is a common staple, but tomato paste not so much.I am looking at this through a distorted HelloFresh lens and with HelloFresh they required pantry staples as well, but never to this degree.  At most HelloFresh required oil, butter and an occasional egg. Never tomato paste, tomato sauce or anything else. They also always provided garlic when needed.The Ranchero Taco Incident 2020This is the recipe we were looking forward to the most. After eating our way through the collection of so-so meals, the tacos seemed like the redeeming meal of the week (or so we thought).After meticulously following the steps, as the mince was cooking a noticeable amount of fat was present. The recipe calls for 1 tsp of olive oil added to the pan before cooking the mince, we didn’t do that and it was a good call considering the pan was incredibly oily, it didn’t need any more.The disappointment of these tacos emanated around the dining table. Our kids who love tacos and were excited for these as well barely ate any. My wife and I were similarly disgusted with the amount of fat dripping onto the plate.There was so much oil on our plates, I was convinced that the USA was preparing to invade our dining room.This incident really cemented that Dinnerly was not for us. We are not food snobs, we just don’t like being drowned in copious amounts of oil and meals that have some semblance of taste in them.Maybe we got a bad box or maybe Dinnerly is so focused on cutting costs and being affordable they’re willing to send inferior meat and produce to their subscribers.I was so disappointed I contacted Dinnerly to let them know of my disappointment and frustration. I felt misled, these tacos were meant to taste good, how hard is it to make tacos? While they apologised, they said they would pass on my feedback to the “culinary team” which is probably code for, “We’re not going to do anything”We will not be paying to try Dinnerly again. I wouldn’t recommend Dinnerly to my friends, family or even my enemies. It was subpar and disappointing, really not that much cheaper than HelloFresh. Save your money and get a HelloFresh box instead or just buy your own ingredients, your chances of disappointment will be so much lower.",1306
How To Develop A Seedable Dice Roll/Number Guess In JavaScript,"Recently whilst working on some blockchain specific code, I needed to write a random number generator to simulate generating a range of numbers, except it needed to be predictable. The same input values produce the same output when provided.In my case, I used values from immutable blockchain data such as the blockchain number, reference block number and transaction ID. I then allow the user to provide a client seed and generate a unique server seed on each roll as well.For this, we will use the tried and tested and ever-versatile Seedrandom library by David Bau. It provides a wide variety of different algorithms for producing random numbers.Seedrandom libraryIn this post, we won’t do anything outside of the norm. We will just use the default algorithm which for most purposes does what you would expect it to do.For this example, we are going to assume the random number generator lives on the server in a Node.js environment. This would also work in a browser, but when generating numbers for a game or any serious application, you would never do it server side.Firstly, make sure you install the seedrandom package by running: npm install seedrandomseedrandomnpm install seedrandomconst seedrandom = require(""seedrandom"");

const rng = (seed1, seed2) => {
  const random = seedrandom(`${seed1}${seed2}`).double();
  const randomRoll = Math.floor(random * 100) + 1;

  return randomRoll;
};const seedrandom = require(""seedrandom"");

const rng = (seed1, seed2) => {
  const random = seedrandom(`${seed1}${seed2}`).double();
  const randomRoll = Math.floor(random * 100) + 1;

  return randomRoll;
};The seedrandom method accepts a seed string. This value produces a deterministic result, meaning if our rng function above is provided the same values for seed1 and seed2, the result is the same.seedrandomrngseed1seed2We then use Math.floor to take our random number and multiply it by 100 (the largest number we want to allow) and plussing it by one means the number starts at 1 instead of 0. This will produce a number between 1 and 100. You can change these values to suit.Math.floorFor example, if you wanted to generate a number between 1 and 2 (for some kind of seedable coin flip) you would do something like this:const randomRoll = Math.floor(random * 2) + 1;
const headsOrTails = randomRoll === 1 ? 'heads' : 'tails';const randomRoll = Math.floor(random * 2) + 1;
const headsOrTails = randomRoll === 1 ? 'heads' : 'tails';You can make your seed as long as you like. I highly suggest allowing the user to provide their own client seed as well as randomly generating a client seed which you reveal to the user after the fact. With all of these values, users should be able to produce the same result independent of your site, this results in a provably fair algorithm for producing numbers.A working code sample of what we did above can be found here on Codesandbox.here on Codesandbox",725
COVID-19 Remote Is Not Working Real Remote Work,"Globally, many of us are all in the same unfortunate and unprecedented situation because of the Novel Coronavirus COVID-19. I am fortunate to both work in an industry where I still have a job and for a company mostly unaffected by COVID-19.Sadly, for many, this is not the case as people find themselves out of work through no fault of their own. For others, they find themselves working remotely; for many, it’s their first time.With many countries in some kind of lockdown, unnecessary travel has meant we can only leave the house for essential purposes like food and exercise. It’s a difficult time for everyone for a multitude of reasons.I have been fortunate to already work remotely for the last two-and-a-half years. Working remotely is not a new experience to me, but working remotely during the COVID-19 pandemic is a unique and trying experience.I want you to know as someone who worked remotely before this, that if you’re finding remote work difficult right now, this is not what is usually looks like. It’s not this difficult or stressful; it’s terrific if done right.I am not the most outgoing person in the world, but not being able to go out for dinner, to a Cafe, for a nice breakfast somewhere or catch up for a beer with a friend. You do not realise you are more social than you are until you can’t be.Right now, there is no separation of work and life. We work at home, and then we stay at home. We sleep and wake up at home, and we work at home. The need to run errands and other non-essential activities have taken a backseat for us all for the time being.We are all stressed. We’re currently experiencing COVID-19 news fatigue, distant from friends and family. For those of us who have kids, they’re probably at home and causing some new logistic problems to work around.For many suddenly forced to work from home, it sucks for you right now. Many are probably begging to go back into the office and have some kind of colleague interaction. The situation you find yourself in right now is not remote work; this is self-imprisonment. People shouldn’t be forced to work remotely, merely given the option to take it or leave it.For our family, we have two young kids at home. My wife is studying to be a nurse, and she relied on our 4.5-year-old energetic sun going to kindy so she could study and complete all of the extra subjects she took on to finish her degree faster. All of a sudden, he is home, and our 1.5-year-old daughter exploring cupboards and draws is as well.The house is chaotic at the best of times, sometimes it’s Armageddon as our son, in particular, is used to playing with other kids, playing on the playground or doing things outside. He loves the science centre and Dreamworld theme park, both of which are not possible right now.Don’t let this pandemic warp your perspective or make you think that working remotely is always this stressful and terrible; it’s not. When all of this passes, I hope many give it another go and realise that working from home during a pandemic versus not working during a pandemic are two different experiences.If you are struggling to work remotely right now, that’s to be expected given the circumstances. But, when things go back to normal (whenever that is) you will appreciate the flexibility and cost-savings of working remotely versus an office and commute.Times are tough for everyone right now. We will get through this.",851
HelloFresh Australia — COVID-19 Review,"This COVID-19/Novel Coronavirus has really changed the world and how we live. Even the basic things we take for granted like being able to go to the shop and buy meat, bread, rice and toilet paper have become difficult tasks.We have a family of four. Two adults and two children, my son is four years old and my daughter is fourteen months old. Out of frustration of not being able to find basics in the stores like; flour, eggs, rice and mince, we decided to explore home meal kits.In Australia, you have a plethora of different options like HelloFresh, Marley Spoon, Dinnerly and a few others I am failing to name here. Ultimately, we settled on trying out HelloFresh first.We had avoided meal kits up until now because we prefer shopping and cooking for ourselves, and the cost of meal kits can be really expensive if you don’t get a deal. HelloFresh is notorious for their first-order offers, to the point where it is a no-brainer to give it a shot (especially in times like these).Ask my wife or anyone that knows me well, I am a sceptical person and when we decided to try HelloFresh I was sceptical that it would be any better than cooking food ourselves. As a family of young active children who keep us busy, sometimes coming up with ideas for what to cook is actually one of the hardest parts of cooking for a family.Before continuing, I want to point out this is not a paid review. I didn’t receive any free boxes or meals, this review is based on our own experience of trying out HelloFresh.The Initial ExperienceThe ordering experience is quite smooth. Like all meal kit providers, you don’t actually get to see what options are available before going through the order process and putting in your details because the meals change on a weekly basis. We chose the Classic Plan and we opted for the two servings given our kids are not adults, they eat a lot less.There was a decent variety of dishes to choose from, as well as a nice selection of premium meals you can add which cost extra per serving on top of the base box cost. After choosing our delivery day and timeframe, we hit submit and chose our meals. You have a little bit of time to make alterations to your box before it gets delivered. We chose a delivery between 12:00 am and 7 am, so we could wake up to the box at our door.The selection of meals changes from week-to-week, with a selection of the meals you get included in your chosen box and the option to add one or more “premium meals” into your box for an additional cost.If you’re sceptical about the meals, they offer their recipes up for free on their website here which is crazy. So, if you wanted to try before trying, you could theoretically buy the ingredients yourself and try out some of the meals.hereThe Food // IngredientsBefore we get into the recipes themselves, the packaging and coordination of the recipes themselves inside of the box are fantastic. The bags are colour/pattern coded to match the recipe cards, so you know what belongs to what recipe. Everything is nicely contained.Now, we got the Classic Plan box but, we also added a premium meal into the box as well, the Tarragon Fillet Steak. Based on the images and recipe, we were looking forward to this one the most (we ate it last).Tarragon Fillet SteakHouston, we have a packaging problemHouston, we have a packaging problemLike many, as a household, we are conscious of our footprint and try minimising waste. Which is why when you open up your HelloFresh box for the first time, you might be shocked over the amount of packaging inside. From the food packaging itself to the cool gel bags keeping the cold stuff cool, there is a lot of packaging inside of the box.However, I want to commend HelloFresh for using as much recyclable and biodegradable packaging as possible. There is actually very minimal use of single-use plastics, and the box through to the cool gel bags can be reused for other purposes.One thing that immediately stood out was the freshness of the ingredients. The salads looked fresh, nothing was wilted or depressed looking. Onions and garlic for the dishes equally as fresh and quality of the other more expensive inclusions like meat and parmesan cheese equally impressive from a quality perspective. The fillet for the Tarragon Fillet Steak above was exceptionally trimmed and a decent portion as well, nice colouring to the meat.Of all of the meals we got in our first box, we kept the recipe cards for all of them. In fact, just last night prior to writing this review, we tried out the Tarragon Fillet Steak recipe again, but using storebought ingredients because we loved it so much. And the only noticeable difference between ours and HelloFresh was we didn’t use the same quality parmesan cheese as they provided, but it still tasted amazing nonetheless.One thing I want to point out is the portion sizes are HUGE, a lot bigger than we anticipated. We chose the two-person option for our family of four, between my wife and I, as well as kids, we still had instances where we had some leftover food. You get really good value for money with the Classic Box, I cannot speak for the other boxes and portions.One thing to be aware of if you’re cooking for younger kids or people who do not have a tolerance for spice, many of the meals HelloFresh send you will contain chilli or some degree of spice. Nothing was “blow your head off” level of spice, but our 4.5 year son hates anything spicy and seems to be quite sensitive to it. Fortunately, like if you were cooking at home, leave the chilli out or only put a little bit in.HelloFresh teaches you how to cookJust because HelloFresh is an at-home meal kit doesn’t mean it is lazy. This is the one thing that will surprise you about HelloFresh (I can’t really speak for other kits) you learn cooking techniques you might not be accustomed to. The boxes advocate for making as much from scratch as possible, from fragrantly throwing spices into a pan to a classic French technique of cooking in butter, you will learn invaluable cooking skills.While there are some meals which require very little time, there are some that require upwards of an hour (cooking and preparation), but I can assure you that the end result makes it all worth the while.Is HelloFresh Expensive?While I understand everyone has a different budget for their weekly grocery shop, the benefit of HelloFresh meals is they come with basically everything but the pantry staples like oil. For some, HelloFresh beyond the discount you get off your first box (using something like the link below) will understandably be too expensive to continue on with.We are currently in an unprecedented time in society where the COVID-19 pandemic is causing people to lose their jobs and livelihoods through no fault of their own. For the Classic Box, you’re looking at about $10 per serving (per person), so two meals for $20. Honestly, that’s a lot cheaper than eating out, even if you get a good deal at the local pub (which nobody is right now given everything is closed).From our perspective, we spend around $250-$300 AUD per week in household grocery expenses. Keep in mind that is for breakfast, morning tea, lunch, afternoon tea, dinner and other snacks and pantry staples. HelloFresh really only covers the dinner component, leaving you with two other slots and two in-between spaces to fill.If you compare HelloFresh to eating out, like I mentioned above, it works out cheaper. The quality of the meals is quite high, that they make eating out not seem like an attractive option. Even if you were to splash out for a premium meal, it would still work out cheaper than eating out.I don’t think HelloFresh is expensive if you view it as “eating out” if you view it as a replacement to grocery shopping, that changes the perspective and equation entirely to the point where I do not think you can compare them. Another upside we appreciated is everything is portioned out, with nutritional information meaning if you’re counting calories it takes out the guesswork.If your family is on a budget, maybe you recently lost your job or were stood down, or you know how to meal prep and enjoy grocery shopping, HelloFresh probably isn’t for you. Still, giving it a try using the generous first-order discount below might be a nice little treat, even if you cancel your subscription afterwards. A week without having to meal prep or budget might be a nice change.If you do give it a tryIf you’re interested in trying it out and getting a cheap box, using this link will knock $59 AUD off of your first order. You don’t have to use my link, I’m sure you might be able to find a deal elsewhere, but if this review helped to convince you to try it out, it would mean a lot to my family and me if you use our link if you were going to try it anyway.this linkIf you have already tried out HelloFresh, drop a comment below and let me know how it went and if it worked out for you.",2230
Reassurances I Need Before I Will Consider Installing Any Australian Government Created COVID-19 Contact Tracing App,"If you have been watching the news of late and let’s be honest, who hasn’t given we are a part of an unprecedented global pandemic? Then you would have heard of the announcement of a contact tracing application you install on your phone which notifies you and others if you’ve been in contact with anyone who tests positive for COVID-19.On the surface, every day Australians will hear the government say, “If everyone installs this, we can ease restrictions faster and flatten the curve by being able to control the spread” – but for those in tech like myself who are critical of the government’s ability to produce an app that won’t be a privacy nightmare, things are a little more convoluted.While I agree that unprecedented times call for unprecedented measures, an application that involves any level of tracing and data collection needs to be handled in a delicate manner. The app is based off of the Singaporean application TraceTogether which uses proximity (distance) to other phones and Bluetooth.TraceTogetherWhen you get close to someone else, the apps swap anonymised encryption keys which are stored on the phone and deleted after 21 days. After some initial confusion as to whether or not the app would be mandatory (not helped in part by authoritarian sounding remarks from Chief Deputy Medical Officer Paul Kelly), Scott Morrison cleared things up in a Tweet.
The App we are working on to help our health workers trace people who have been in contact with coronavirus will not be mandatory.— Scott Morrison (@ScottMorrisonMP) April 18, 2020
The App we are working on to help our health workers trace people who have been in contact with coronavirus will not be mandatory.— Scott Morrison (@ScottMorrisonMP) April 18, 2020The App we are working on to help our health workers trace people who have been in contact with coronavirus will not be mandatory.April 18, 2020Allegedly, Morrison and others are pushing for an application which would have greater privacy protections than the Singapore application. However, we should be wary until we see the final application. Even some federal government MP’s have come out and said they won’t install it.some federal government MP’sInstalling government-sanctioned surveillance software on my phone would require some serious guarantees from the government before I would consider doing such a thing.A hard sunset date that cannot be extended. The government needs to provide a date in which the servers are wiped and the apps are rendered non-functional that they cannot freely extend.Guarantees that the scope of the tracking will not be expanded beyond its initial purpose. We all saw what happened with the site-blocking list legislation initially intended to block child pornography and other illegal sites, then amended to make it easier for the entertainment industry to block torrent sites. The code made completely open-source and freely accessible to not only security researchers but to the general public like myself who will want to dig into the code and see for myself what it is doing. Furthermore, the source code for the server as well to ensure that data isn’t being sent to an insecure honeypot.Protections put in place preventing who has access to the data sent to government servers. We all saw what happened with the mandatory data retention laws and reports of violations of people obtaining access to information which they shouldn’t have been able to.A hard sunset date that cannot be extended. The government needs to provide a date in which the servers are wiped and the apps are rendered non-functional that they cannot freely extend.Guarantees that the scope of the tracking will not be expanded beyond its initial purpose. We all saw what happened with the site-blocking list legislation initially intended to block child pornography and other illegal sites, then amended to make it easier for the entertainment industry to block torrent sites. The code made completely open-source and freely accessible to not only security researchers but to the general public like myself who will want to dig into the code and see for myself what it is doing. Furthermore, the source code for the server as well to ensure that data isn’t being sent to an insecure honeypot.Protections put in place preventing who has access to the data sent to government servers. We all saw what happened with the mandatory data retention laws and reports of violations of people obtaining access to information which they shouldn’t have been able to.We will see what happens when the app is released, but I implore others to be sceptical of a government created tracking application, even if on the surface the intentions seem to be good. We have seen time and time again the government passing legislation and reducing freedoms under false pretenses and then subsequently amending legislations to expand the scope.",1219
How To Get The Hash of A File In Node.js,"Whilst doing some work in a library I maintain, I needed to add in the ability to calculate the hash of an included file for an integrity check feature I was adding in. The resulting solution is simple and not boast-worthy, but given others might encounter a situation where they need a hash of a file, this might help.We use the fs module to open up the file we want to calculate the hash for, use the createHash method on the crypto package to then pass in our file buffer from the readFileSync method, and that’s it.fscreateHashcryptoreadFileSyncconst crypto = require('crypto');
const fs = require('fs');

const fileBuffer = fs.readFileSync('myfile.js');
const hashSum = crypto.createHash('sha256');
hashSum.update(fileBuffer);

const hex = hashSum.digest('hex');

console.log(hex);const crypto = require('crypto');
const fs = require('fs');

const fileBuffer = fs.readFileSync('myfile.js');
const hashSum = crypto.createHash('sha256');
hashSum.update(fileBuffer);

const hex = hashSum.digest('hex');

console.log(hex);For the createHash method you can supply different supported algorithms including; md5, sha1 and sha256. To the digest method, you can supply hex or base64. If speed is essential to you, sha1 and base64 are the two fastest options in most cases. However, all options are pretty fast anyway.createHashmd5sha1sha256hexbase64",336
How To Use/Enable The New Tab Groups Feature In Google Chrome 81,"For years the people have been asking for tab groups in Chrome. While extensions do exist, they’re somewhat fickle. Now, tab groups are natively supported in Google Chrome itself. While the feature is rolling out in Google Chrome 81, if you’re like me, you’re running Chrome 81 and the feature isn’t on for you yet.If you right-click on a tab and can’t see the new tab group options, you need to enable it. To visit the flags screen, open a new tab and visit: chrome://flags/ – in the search input, enter groups.chrome://flags/Enable it and then relaunch Chrome. Right clicking on a tab should give you some new options for tab groups.It really is that easy. And then you can name your groups or choose from a few colours to distinguish them in the UI. While the new feature is great, it still feels like it needs a bit more polish before it’ll be a nice to use feature.",218
Freeing Up Space on Ubuntu When You Unexpectedly Run Out of Disk Space,"Recently, whilst working on an open-source project I work on we found ourselves running out of space on the server. The weird thing is the projects on the server themselves were barely 100mb in total file size, but we had run out nonetheless.After a little investigation to see what is using up the majority of space, the search led to the /usr/src folder which contains source header files for Ubuntu’s APT package manager. A trove of files in here weighing around 100mb seemed to add up to 4 gigabytes of used space./usr/srcWhile it might be tempting to delete these, you shouldn’t touch this folder manually. Using sudo apt-get autoremove the package manager will cleanup unneeded source files in this folder. In this instance, it resulted in 4 gigabytes freed up.sudo apt-get autoremove",198
Testing Event Listeners In Jest (Without Using A Library),"I love using Jest to unit test my Aurelia applications. Sadly, one of the most popular options for mocking event listeners and simulating events called Enzyme is targeted at React applications and to my knowledge, does not work with Aurelia or any other framework like Angular.If you are wanting to test events registered using addEventListener there is an easy way to mock them.addEventListenerdescribe('My Test', () => {
	let sut;
	let events = {};

	beforeEach(() => {
		sut = new Dependency();

		// Empty our events before each test case
		events = {};

		// Define the addEventListener method with a Jest mock function
		document.addEventListener = jest.fn((event, callback) => {
      		events[event] = callback;
    	});
      
        document.removeEventListener = jest.fn((event, callback) => {
      	    delete events[event];
        });
	});

	test('Test Keypress fires callback', () => {
        // Watch the function that gets called when our event fires
        jest.spyOn(sut, 'pressed');
        
        // A method inside of our dependency that sets up event listeners
		sut.setupEvents();

		// Fire the keypress event
		events.keypress({ key: 'Enter' });
        
        // We fired an event, so this should have been called
        expect(sut.pressed).toHaveBeenCalled();
	});
});describe('My Test', () => {
	let sut;
	let events = {};

	beforeEach(() => {
		sut = new Dependency();

		// Empty our events before each test case
		events = {};

		// Define the addEventListener method with a Jest mock function
		document.addEventListener = jest.fn((event, callback) => {
      		events[event] = callback;
    	});
      
        document.removeEventListener = jest.fn((event, callback) => {
      	    delete events[event];
        });
	});

	test('Test Keypress fires callback', () => {
        // Watch the function that gets called when our event fires
        jest.spyOn(sut, 'pressed');
        
        // A method inside of our dependency that sets up event listeners
		sut.setupEvents();

		// Fire the keypress event
		events.keypress({ key: 'Enter' });
        
        // We fired an event, so this should have been called
        expect(sut.pressed).toHaveBeenCalled();
	});
});Inside of the beforeEach we do a few bootstrapping things before we run our tests. The sut variable is actually the thing we are testing. In this instance, Dependency is a class with some methods inside of it.beforeEachsutDependencyWe populate our object which stores our events called events when addEventListener is called, our mocked version will be used, so we replicate how this would work in the browser, except we just use an object.eventsaddEventListenerInside of our test case, we are calling a method on our class called pressed here is what the dependency might look like:pressedexport class Dependency {
    setupEvents() {
        document.addEventListener('keypress', this.pressed.bind(this), false);
    }
  
    pressed() {
        // Called when keypress event is fired
    }
}export class Dependency {
    setupEvents() {
        document.addEventListener('keypress', this.pressed.bind(this), false);
    }
  
    pressed() {
        // Called when keypress event is fired
    }
}This approach works for a lot of other things you want to test and mock.",821
Is It Safe/Okay To Public Expose Your Firebase API Key To The Public?,"Perhaps one of the most confusing aspects of building a publicly visible Firebase application hosted on GitHub is when you add in your SDK configuration details and commit them you’ll get warnings from a bot called Git Guardian and an email from Google themselves.I am not sure if everyone gets these, but I do for every publicly visible Firebase application I have on GitHub.The code in question that triggered these latest warnings for me looked like this:const firebaseConfig = {
  apiKey: 'AIzaSyCz0wlgveUQ65qa8hs5A4kxPsrotOn_fSc',
  authDomain: 'binary-people.firebaseapp.com',
  databaseURL: 'https://binary-people.firebaseio.com',
  projectId: 'binary-people',
  storageBucket: 'binary-people.appspot.com',
  messagingSenderId: '617061139341',
  appId: '1:617061139341:web:c16aacb98727f9a68bf3c4',
  measurementId: 'G-3E37M44VBZ'
};const firebaseConfig = {
  apiKey: 'AIzaSyCz0wlgveUQ65qa8hs5A4kxPsrotOn_fSc',
  authDomain: 'binary-people.firebaseapp.com',
  databaseURL: 'https://binary-people.firebaseio.com',
  projectId: 'binary-people',
  storageBucket: 'binary-people.appspot.com',
  messagingSenderId: '617061139341',
  appId: '1:617061139341:web:c16aacb98727f9a68bf3c4',
  measurementId: 'G-3E37M44VBZ'
};This is the code that you are provided when you add and configure your project in Firebase. It’s code you’re told to add into your application to configure the Firebase application.Committing this resulted in the following messages. The first one from GitGuardian:The first one from GitGuardian:And another from Google Cloud complianceAnd another from Google Cloud complianceIf you’re new to Firebase, these emails would terrify you. Rest assured, there is no problem with committing your Firebase configuration details for the client. These errors are warnings, they can’t tell the difference between public API keys and private ones. The one above simply identifies your website with Firebase servers, that’s it.If your application has open security rules on your database, at worse, this just makes your application URL public and means someone could write to it if you do not have it locked down.",530
Did The Australian Federal Police (AFP) Violate Its Own Charter?,"Today, I came across something which quite frankly shocked me. The AFP Tweeted out the following Tweet.TweetOn the surface, this might seem like a harmless attempt to tell people about efforts to notify and help those affected by the bushfire, but this singular Tweet truly masks a horrifying truth of an allegedly impartial agency that investigates serious crimes Tweeting about a matter, not in their interests whatsoever.According to the Australian Federal Police’s own charter:own charter You can expect us to:  be professional, impartial, fair, honest and reasonable in our dealings;  You can expect us to:  be professional, impartial, fair, honest and reasonable in our dealings; The AFP is allegedly meant to be an impartial agency, although, given recent events surrounding certain allegations against LNP politicians, suffice to say confidence in AFP’s impartiality is not exactly the highest right now.At first, I honestly thought it was a parody account (which would have made sense), but this is a real Tweet by the AFP. I had to ask myself, “Am I overreacting, is anyone else seeing what I am seeing?” you only have to see the replies to the Tweet to see that many take issue with the AFP advertising the government.As you can see, people are legitimately questioning this Tweet and why the AFP published it. Did the AFP write it, were they asked/instructed by the government to do so? Why was this published?The AFP has some explaining to do about this Tweet. Will they be called into question and who investigates the AFP over charter violations like this (if anyone)?There is cause for concern when the very agency that exists to investigate serious crimes, as well as politician misconduct and illegal activity, appears to be putting out PR pieces for a government currently embroiled in controversy on numerous fronts.Given all of the controversy surrounding other political matters like the rort sports situation, Angus Taylor and a plethora of other controversies and allegations in amongst one of the worse bushfire seasons on record: this will probably get buried along with everything else.",528
Next Level Conspiracy Insanity: Direct Energy Weapons Allegedly Used To Start Australian Bushfires,"I love a good conspiracy theory. Some of my favourite conspiracy theories include the Royal Family being shape-shifting lizards, part of some global reptilian elite controlling the world or Alex Jones’ famous rant where he claims the government is putting chemicals into the water turning frogs gay.The late-2019 Australian bushfires which have burned into 2020 have attracted some crazy individuals claiming all kinds of crazy things. People have lost their lives, thousands of homes destroyed, towns completely wiped, millions of hectares burned, over 1 billion animals estimated to have been killed.It all started when Barnaby Joyce helped start the rumour that the Greens were responsible for the bushfires by proclaiming they have stopped needed fire-reduction efforts and locked up national parks. In amongst all of this, another conspiracy has been spreading amongst the inner crazy circles of the internet.Allegedly, some elite secretive entities with an agenda for a high-speed rail line started the fires in the needed areas where the line would go and furthermore, an agenda to force people in regional areas into cities so they can be “more easily controlled”.I would say you can’t make this stuff up, but here we are talking about it.One of Australia’s most well-known weather centric Facebook groups Higgins Storm Chasers has also helped spread the rumour to their 10k followers.helped spread the rumourThis post, in particular, loses credibility almost instantly by claiming that dry lightning is a new and made-up term. Ignoring the fact that dry thunderstorms are well-documented and occurring phenomena that happen in dry areas.well-documentedAs can be expected, crazy attracts crazy. The comments section, things start to spiral out of control quite quickly. People start sharing images of what they believe to be chemtrails and planes spray chemicals in the sky, presumably to cover bushland in some kind of combustible material.The thing is, the unprecedented fires we are seeing don’t need anything sprayed in the areas to make the fires spread. The fuel is the incredibly dry bushland catching alight.Furthermore, there are much better ways to make money than a train line. Look to other established rail lines and services, Amtrak doesn’t turn a profit and it turns out in the UK private railway operators have realised that they’re not profitable either.doesn’t turn a profithave realisedWhoever these elite corporate shadow entities are, they have a terrible business sense if they think a high-speed rail line is going to be their ticket to riches. Given the exorbitant cost of constructing such a line, it would take decades for it to earn the money back (if it ever manages to achieve profitability).These bushfires are being spread by insanely dry conditions, caused by climate change. We need to be smart and plan accordingly for the future because this is just going to keep happening. We can let the crazies play in their little crazy corner on the internet while the sane ones try and come up with solutions to stop these fires from spreading as badly as they have.",775
What Comes Next After USB-C?,"I have the weirdest and sometimes most profound thoughts about the most useless stuff. I actually asked myself this question whilst in the shower this morning: is there going to be a USB-D? Do we need a successor to USB-C or is it good enough for the time being?When these types of questions pop into my head, I have to Google them. I actually stepped out of the shower and before reaching for a towel, I grabbed my phone and had to find out. With the water dripping onto my phone screen and floor, I set out to find the answer.Given the iPhone doesn’t even support the USB-C standard yet (opting for its own Lightning Connector™) I wonder if it’s due to limitations in the standard or fact Apple doesn’t want to have to change their cables again, after the controversy they generated a few years ago when they did it.Anyway, back on the topic at hand.  In terms of the USB-C specification, it is relatively quite new. It wasn’t published and finalised until August 2014, which isn’t that long ago.It turns out the answer is not exciting at all, there is no publicly announced successor to the USB-C cable standard. In terms of capabilities, it seems USB-C is capable of supporting quite a high throughput with the recently announced USB 4 standard supporting speeds up to 40gbps (which is super fast) and will require compatible USB-C cables to take advantage of it.It is naive to assume that USB-C will be as good as it gets. Once upon a time, USB-A and USB-B were probably considered enough and then technology evolved and times changes.I wonder though, will they call it USB-D or something a little less silly-sounding opting for something like USB-Next or USB-Z?",417
Preferring If Statements over Ternary Operators In Javascript,"Every so often thought-pieces will go around proclaiming that you are writing code the wrong way and that you should be writing your code this way instead.thisOne such opinion I have seen (and will not link because this isn’t a takedown) is recommending the use of Ternary Operators in Javascript over if statements. While ternaries can make your code look cleaner in some cases by replacing multi-line if statements with one-liners, there are instances where they fall apart quite quickly. Ternary operators exist in all programming languages and the problems they can introduce into a codebase are universal.Ternaries are hard to readSure, they might look cleaner, but ternaries can needlessly make code hard to read. This is the problem I have with “clever coding” and some developers pursuit to write the most convoluted code in an attempt to condense things.const canAccess = user.isAdmin || user.isEditor || user.level > 6 ? true : false;const canAccess = user.isAdmin || user.isEditor || user.level > 6 ? true : false;It’s a simple one-liner, but there is a lot going on here. Replaced with an if statement, things get a little easier to read.let canAccess = false;

if (user.isAdmin || user.isEditor || user.level > 6) {
    canAccess = true;
}let canAccess = false;

if (user.isAdmin || user.isEditor || user.level > 6) {
    canAccess = true;
}Understandably, this is an exaggerated example and even so, there is room for improvement here. But, my eyes are instantly drawn to the if statement, it is easier to read and if I need to change it, it will be easier to change as well.Ternaries fail at dealing with complex conditionsThe above example is quite a simple set of conditional checks, but what happens in a situation where things are more complex? A good example is detecting keycodes on the keydown event in Javascript and reacting accordingly.keydownWhile in simple use-cases it is more than fine to use a ternary, complex scenarios with multiple conditions should be avoided like the plague. If you need to check multiple values or check multiple expressions, a ternary condition will be a nightmare.const prevNext = (e.keyCode == 38) ? 'prev' : (e.keyCode == 40) : 'next' : null;const prevNext = (e.keyCode == 38) ? 'prev' : (e.keyCode == 40) : 'next' : null;This is a relatively tame example of multiple expressions, can you imagine throwing more into the mix?Ternaries are hard to debugIf you have a one-line ternary expression in your application, good luck setting a precise breakpoint. This is where the differences between a ternary statement and if statement is truly highlighted. Sure, you could use a console.log if you wanted to debug, but setting a breakpoint is not going to be possible.console.logCode that is broken up into multiple lines might not look as appealing as a condensed ternary condition, but at least you can set a breakpoint and go through it line-by-line to debug the flow.I am not saying that you shouldn’t use ternaries, because they have a purpose. But to go as far as recommending their use over if statements in general defies all common sense.",775
The Real Reason Virtual Reality (VR) Has Never Taken Off,"Recently, the BBC published an article titled Why we’ve never fallen in love with virtual reality in which they discuss virtual realities lack of mainstream consumer adoption.Why we’ve never fallen in love with virtual realityThe article then goes on to talk about one VR segment that is thriving: group entertainment. Specifically, virtual reality arcades, theme parks leveraging virtual reality in rides and offering an affordable means of immersing yourself without getting into debt.Truly immersive virtual reality experiences in 2020 are amazing. In countries like Tokyo, they have numerous public places where VR is employed for fun experiences. Disneyland has been showcasing the power of virtual reality for some time now.One of VR’s biggest problems in the consumer home entertainment space is the cost. The technology is on the high-end of the spectrum, often requiring the headset, sensors and a computer to power it. While the cost of the headsets themselves have gone down over the last few years, building a PC powerful enough to provide a low-latency, immersive and smooth experience requires top-shelf components (the most expensive being one or two graphics cards).The Oculus Quest which is an attempt to not rely on external hardware and provide an all-in-one virtual reality solution is a step in the right direction towards mainstream adoption. Still, the cost puts in way above a gaming console and the Quest is considered low-end VR.Oculus QuestIt goes a step above those horrendous standalone headsets like the failed Google Daydream which has been shut down by Google and requires you to put your phone inside of it.Virtual reality isn’t dying, it’s just considerably behind optimistic estimates of where it was heading. 2016 was allegedly meant to be the year of virtual reality, really, we are still 3 to 5 years away from mainstream VR adoption. The latest, we won’t see VR truly take off until 2025.Estimates of mainstream VR adoption were about ten years behind if you ask me. However, we cannot discount companies like Oculus and Sony who took the plunge and invested in early generation headsets.I believe headsets like the Quest which do not require expensive water-cooled gaming PC’s with $1500 graphics cards in them are the beginning of a new era of VR. Screen technology is getting quite good, refresh rates are increasing and experiences are also getting better.The one thing VR is also missing is a killer app. Something synonymous with VR like Facebook, Google and Twitter are with the internet, there is no one killer app that springs to mind for people when they think of VR. However, I think that will eventually change.We also cannot discount the fact that VR will never appeal to some people. For some, VR is an uncomfortable nauseous and disorienting experience. I think the future of VR is a hybrid of both virtual reality and augmented reality, not one over the other.Once the cost of VR comes down to a level that is comparable with a gaming console like the Playstation or Xbox, we are going to see considerable adoption and a new industry that will start a gold rush. It hasn’t started yet, but it is coming.The real problem to VR adoption right now is the cost. It’s a problem other industries like the electric vehicle industry are experiencing, but once the economy of scale kicks in, that’s when the adoption truly begins.",844
Should I Choose Firebase Cloud Firestore or Realtime Database?,"When it comes to Firebase for newcomers, the first point of confusion in what is quite a simple platform is what should you choose for your database: Firestore or Realtime Database?As someone who has been using Firebase for quite a few years, there was a time when Firestore never even existed. Initially, it used to just be Realtime Database and that was that. A couple of years ago, Firebase introduced the Firestore database which is the next evolution of databases on Firebase.While Firestore has been in beta for some time, I can tell you based on experienced that it is anything but a beta product. The difference between Firestore and Realtime Database used to be the reliability. For a while after introduction, Firestore’s reliability was terrible, regularly going down.In 2020, Firestore seems to be quite stable. I use it for several applications, including a highly trafficked cryptocurrency exchange and I have not witnessed a period of downtime with Firestore.cryptocurrency exchangeThe ability to query data in more flexible ways is more appealing than Real-Time. Previously, you would have to pull down entire trees of data (which could be massive) and then sort through them in either Node.js or on the client because of the lack of querying, this is why Firestore was created.Even so, Firestore still has its limitations when it comes to complex querying and you should absolutely read up on what those are. You can’t do everything that a normal RDBMS allows you to do or even completely what NoSQL solutions like MongoDB offer, but all limitations have workarounds.Rather than reiterate the technical differences between the two, there is already a great official post on the Firestore site that details the differences between the two.official postWithout delving too deeply into the specifics, you should choose Firestore and I would not recommend anything else. It offers way more powerful querying (which is important in a database), more flexible ways of storing data (Real Time can get messy) and it is still quite cheap.",512
Are We Finally Getting A New System of A Down Album In 2020?,"All signs are pointing to yes. System of A Down frontman Serj posted an image of himself in the studio working on what appears to be music for a System of A Down.Many might be quick to say this could just be Serj working on more solo material, the hashtags tell a different story at the end, using the band’s name as a hashtag.For years there has been rumour and speculation a new album is happening. Then various members speaking out about the band’s inability to get on the same page musically, could they have found a way to work past the problems they were happening?We have all been let down by the possibility of a new SOAD album, only for Serj or someone else in the band to come out and say it is not happening. Time will tell.",184
Thoughts On Ember Octane,"When it comes to JavaScript frameworks, few can lay claim to the longevity of Ember which just turned eight years old. To give readers some perspective, Ember is about as old as AngularJS (the first version of Angular), older than React, older than Vue and many other options out there. It harks back to the days when IE6 was still a browser many of us had to support.To the surprise of some who abandoned Ember (and JavaScript frameworks in general) years ago, Ember just released a large update which changes and improves Ember in many facets. For years, Ember has been trailing behind other frameworks and libraries. Even though updates were still being made, Ember has always felt like a relic of Web yesteryear.Despite trailing behind newer, faster and smaller options, Ember has enjoyed success at numerous companies including LinkedIn and Intercom. Nobody can argue that Ember isn’t used or that it is even dying, it’s just not that popular any more. It’s hard to deny that React has eaten front-end development.With Ember Octane, many facets of Ember have been improved, Ember applications are still overly verbose and the templating syntax which uses Handlebars feels outdated.In Octane, some of the changes actually resemble that of Aurelia dating back to 2015. Previously, view-models and templates were located in separate directories. The old approach looked like this:app/
  components/
    my-component.js
  templates/
    components/
      my-component.hbsIn Ember Octane, this now resembles that of frameworks like Angular or in my opinion as mentioned, more closely to Aurelia:app/
  components/
    my-component.js
    my-component.hbsAs you can see, the view-model and template are in the same directory now. Likewise, Ember Octane introduces a new decorator for computed properties called @tracked which is reminiscent of Aurelia’s @computedFrom decorator. A similar concept further cemented by the introduction of a decorator called @computed in Ember Octane.@tracked@computedFrom@computedI think Ember Octane is a step in the right direction. Some parts feel inspired by React and other parts feel inspired by Aurelia. Still, looking at Ember, it feels overly complicated and like it is still playing catchup with Aurelia from 2016 or really, every other established framework using modern Javascript syntax.For me personally, there is nothing in Ember Octane that is exciting or innovative enough that it would suddenly win back developers who left Ember or have eschewed it for other options such as React and Vue. Still, it is great to see the project is maintained and this is all a positive step in the right direction.I don’t want people to misconstrue this post as a beat-up of Ember with a hidden agenda to promote another framework. I think the more frameworks and libraries there are, the better. But when you’re competing with the big daddy React or up and coming superstar Svelte, you have to bring something substantial to the table and really, Ember doesn’t feel that different when you dig beneath the surface.",762
Is Ethereum Dying?,"Or is it already dead?The once-promising blockchain and beloved smart contract project seemed to be at the top of the world. From a high of $1431 in January 2018 to its current low of $126, it seems whatever hopes people have for Ethereum have faded quite a bit.Despite the fact that Ethereum still holds the number two spot on Coin Market Cap Ethereum has fallen out of the limelight somewhat with developers. On State of The DApps, Ethereum accounts for only three of the top ten applications. Klaytn accounts for four of them, Steem accounts for two and NEO just one. In the top five, Ethereum only has one DApp.Coin Market CapState of The DAppsVanity metrics aside, given Ethereum was one of the first smart contract platforms and holds the coveted #2 spot, you would expect more popular applications to be using it. However, over the last two or so years, many projects and exchanges have moved away from Ethereum to their own solutions.Ethereum used to be ICO blockchain. Almost every ICO in 2017 was an ERC20 token. Fast forward to 2019 and the projects that are still alive, quite a few of them have moved away from Ethereum. Crypto.com (formerly known as Monaco Card) previously used Ethereum, until they created their own blockchain called Crypto.com Chain (CRO). In early 2019, Binance famously ditched Ethereum for their own solution as well Binance Chain.Tron and EOS are also two other blockchains which received their funding through Ethereum and then subsequently created their own competing blockchain. It seems Ethereum has become the gateway to other blockchains.One of Ethereum’s biggest problems is that it has serious scaling issues. I am sure you all remember CryptoKitties which famously crippled Ethereum a couple of years ago. It also turns out that Tether may also be to blame for Ethereum’s high network usage and problems.Having said that, many blockchains have the same scaling problems. Bitcoin is also quite full, EOS recently experienced some serious problems with their network as well.While the anticipated Ethereum 2.0 release will alleviate some of the problems by moving away from proof-of-work to proof-of-stake, nobody can agree on anything. There has been a tonne of infighting within the Ethereum team and associated entities contributing to the blockchain.Allegedly the first upgrade will be happening in January 2020, but given the turbulence, Ethereum has experienced with nobody seemingly able to agree on anything, I would actually be surprised if that happens. People are afraid of Ethereum 2.0 and the changes being proposed.Even one of the co-founders of Ethereum recently sold off 90,000 ETH worth $11 million US dollars. Another nail in the coffin of an already fractured and turbulent cryptocurrency project.sold off 90,000Given Ethereum was one of the first general-purpose smart contract blockchains, perhaps it was inevitable that it would eventually lose some of its popularity in favour of other more modern solutions who have the luxury of beginning from scratch without having to worry about an entire legacy ecosystem built around them.I don’t think Ethereum will truly die, but there are so many better options out there now ranging from Steem to EOS, to Stellar, Tron and Klaytn that make Ethereum look antiquated and clunky. Ethereum is no longer the first choice that comes to mind for many DApp developers.",843
Select Change Event Not Firing When Using Characters On Keyboard,"Here is a nice bug-not-bug to close out in 2019. One of my Trello cards detailed what sounded like an error:When toggling between two options (yes and no) in a dropdown, entering “y” changes to yes and quickly entering “n” does not switch to no. However, waiting a second you can change between them.When toggling between two options (yes and no) in a dropdown, entering “y” changes to yes and quickly entering “n” does not switch to no. However, waiting a second you can change between them.Some initial debugging suggested this was not actually a bug in our application. But, I knew if I was going to get the ticket closed off as not a bug, I had to have an explanation.It turns out that browsers (well at least in Chrome and Firefox) select dropdowns are searchable by offering a delay allowing you to type in long values. The way I highlighted this was creating a dropdown with four options:YesNoYNoNYesYesNoYNoNYesTo highlight the error I created a JSFiddle demo here. The first dropdown contains the above options. Try pressing “Y” and then “N” quickly after, the selected value will then be “YNo” highlighting the searchability. Similarly, entering “N” followed by “Y” will yield “NYes” selected.hereThere is also a second dropdown with some years from 1988 to 1993 in the above linked JSFiddle demo. Try selecting the dropdown and then entering 1993 (which is the last option) you will see the searching feature in the browser selects 1993.So, not a bug, just a browser feature. Admittedly, I didn’t actually know you could search values in a dropdown this way. I usually use my mouse to select values in a dropdown. We have some people on our team who shun the mouse and navigate through our main app using their keyboard.",433
Quick & Easy Way To Reset Mocks & Spies In Jest,"When working with mocks and spies in Jest, it is quite easy to fall into a trap where they become stale (especially in cases where pure functions are not being used). Heading to the documentation for Jest yields a lot of similar-looking methods for restoring mocks, clearing mocks and resetting mocks.This is where confusion sets in. What is the best practice? Which ones should I call to ensure my tests don’t have stale mocks or spies? Even I struggled with this aspect.In my Aurelia applications, Jest is my prefered means of test tool. In my trial and error, I have settled on the following in my tests which ensures all mocks and spies are reset between tests being run.  afterEach(() => {
    jest.resetAllMocks();
    jest.restoreAllMocks();
  });  afterEach(() => {
    jest.resetAllMocks();
    jest.restoreAllMocks();
  });The jest.resetAllMocks method resets the state of all mocks in use in your tests. It is the equivalent of manually calling mockReset on every mock you have (which can be tedious if you have a lot of them).jest.resetAllMocksmockResetThe jest.restoreAllMocks method restores all mocks back to their original value, ONLY if you used jest.spyOn to spy on methods and values in your application. It is important that you use spyOn where you possibly can.jest.restoreAllMocksjest.spyOnspyOn",329
Boeing Is Too Big Too Fail,"People once thought the banking industry was too big to fail, some seriously big financial institutions ultimately proved that wrong during the Global Financial Crisis of 2008/2009 which saw many seemingly unsinkable companies go out of businesses.Early 2019, after two deadly crashes of the allegedly bigger and better 737 MAX, the plane was grounded by countries around the world as people scrambled to find answers for what happened. After numerous investigations, the culprit turned out to be MCAS also known by its non-abbreviated mouthful of a name Maneuvering Characteristics Augmentation System.The issue with the 737 MAX was engineers were tasked with fitting larger and heavier engines under the wing of the plane. The engines had to be moved slightly higher and moved forward, which ultimately caused a dynamically unstable airframe. The solution was MCAS which used a single sensor to determine if the nose of the plane should be pushed down (as a safety measure).While the 737 MAX continues to collect dust in parking lots and warehouses, Boeing is bleeding money. Just recently, they announced they’re halting production of the MAX (which will only slow, not stop the money bleeding). The solution Boeing has come up with involves comparing data from two AOA (Angle of Attack) sensors and comparing the difference if both sensors cannot agree, the MCAS does not override the plane as detailed here.hereThe company was hoping to have the 737 MAX recertified by the end of 2019, but this has been pushed back to 2020. Understandably, this entire situation does not just reflect badly on Boeing, but also the FAA who allowed this to happen in the first place.Too Big, Too InfluentialFor any other company, two tragedies and a grounding going on for almost a year would be enough to plunge them into bankruptcy and put them out of business. For Boeing, their stock has been affected a bit, but they’re still okay.Boeing is a company that has been around for over one-hundred years. When it comes to the aerospace industry, you don’t get any bigger than Boeing. Since the ’90s, the presidential fleet of planes consists of two Boeing VC-25’s which are military versions of the workhorse Boeing 747.In terms of employment size, Boeing is one of the largest American employers. They employ over 150,000 people, many of those work in the US. If Boeing were to go out of business, the US economy would be affected. Not to mention the supply chain Boeing has created rivals even that of a company like Amazon and its supply chain.Fly on any major airline in most parts of the world and chances are you are flying on a Boeing built plane, most likely a variant of the Boeing 747.To get an understanding of just how influential Boeing is and its importance to the US, look no further than the fact the CEO of Boeing (Dennis Muilenburg) still has his job (CEO’s have been fired or forced to step down over less) and has not been summoned to Washington to be grilled before the senate over the tragedies and mismanagement of the 737 MAX.You don’t get any more influential than not being held accountable or even being questioned on why two tragedies in such a short space of time even took place (the 737 MAX tragedies were unprecedented). When the banking industry collapsed, bankers were dragged before congress to be held accountable.Still Waters Run Deep, Boeing Runs DeeperWhen you and I think of Boeing, we think of a company that makes passenger planes. However, Boeing has its fingers in many pies, it’s producing the pies, it’s eating them, it owns the pie factory, it owns the supply chain that is shipping the pies to stores and restaurants.Not many people realise Boeing also props up other companies and industries. Their subcontracts with General Electric (GE) and United Technologies and Spirit Aerosystems are some of the biggest. You best believe Boeing is adding a few zeroes to the books of those contractors.Boeing is entrenched in both civilian and military sectors. They sell planes, rockets, satellites, telecommunications equipment and even missiles. They are the largest exporter by dollar value in the US. As far as size is concerned, Boeing is a juggernaut, a core pillar of the aerospace industry.They are already saying that the halting of the 737 MAX could affect the US’s GDP in 2020. Not many companies can lay claim to being so big they contribute to the overall GDP of a country.could affectEven if Boeing were to get into serious financial trouble, the US government would not even hesitate to bail them out. It would probably be considered a national security risk if Boeing were to go under given their influence and reach in the defence sector alone. Quite simply, Boeing could sustain many more missteps and accidents before it really affected them and forced the government to step in.",1206
Google Chrome v79 Broke The Ability To Hover Variables In Developer Tools,"Well, this is a pretty frustrating bug. The other day I and a few other people in my team noticed something peculiar while debugging some Javascript. The ability to hover over variables and function arguments in Chrome Developer Tools had stopped working.At first, we thought this might have been a Webpack configuration issue or an update to one or more of our packages breaking the way in which Chrome parses our Javascript. The issue turned out to be Chrome itself. There is an issue recently created where many voice their frustration (myself included) over this bug.recently createdAs a developer, the ability to debug is everything. As a result of this simple bug, the time required to debug has increased exponentially. Fortunately, this bug appears to have been fixed in Chrome Canary Version 81.0.4001.2. Even many of the developers I know do not use Canary because it can at times be unstable or introduce new features that seemingly get removed. So, until an update is released in the next couple of weeks, frustration will ensue for many.All of this has just motivated me to consider moving back over to Firefox as my primary browser, given Google’s anti-ad stance and now a bug that should not have been introduced, I am driven by frustration.",314
Crash Course: The Bindable Element In Aurelia 2,"In Aurelia 2, a new element called bindable has been introduced which is leveraged in HTML only custom elements. If you want a HTML only custom element which has one or more bindable properties, then you use the bindable element to specify them.bindablebindableThe ability to create bindables for HTML only custom elements existed in Aurelia 1, but was limited. The constraint being the bindable keyword had to be specified on the template element.template<template bindable=""user"">...</template><template bindable=""user"">...</template>In Aurelia 2, this now becomes something much more simple and clean because template is no longer a requirement for HTML only custom elements.template<bindable name=""user"" /><bindable name=""user"" />Besides the semantics changing, you can now also specify the binding mode for the defined variable. This was not possible in Aurelia 1 when creating HTML custom elements.Now, simply adding a mode property and valid binding mode value, you can change how the binding works just like you can when configuring the binding mode inside of a view-model.mode<bindable name=""user"" mode=""one-way"" /><bindable name=""user"" mode=""one-way"" />",291
Creating HTML Only Custom Elements In Aurelia 2,"If you are already familiar with Aurelia and have worked with Aurelia 1, then HTML custom elements are not a new concept to you. If you’re starting out with Aurelia 2, they might be a new concept to you.How HTML Only Custom Elements Looked In Aurelia 1We have a custom element called user-info.html which accepts a user object through a bindable property called user and we display their name and email.user-info.htmluser<template bindable=""user"">
  <p>${user.name}</p>
  <p>${user.email}</p>
</template><template bindable=""user"">
  <p>${user.name}</p>
  <p>${user.email}</p>
</template>How HTML Only Custom Elements Look In Aurelia 2The constraint of needing a template tag has been removed in Aurelia 2. It is now automatically handled for you by the framework, so now our HTML components look like this.template<bindable name=""user"" />

<p>${user.name}</p>
<p>${user.email}</p><bindable name=""user"" />

<p>${user.name}</p>
<p>${user.email}</p>Because there is no template tag, you need to create your bindable properties using the bindable element and the name property to specify what it should be called.bindableJust like Aurelia 1, the file name itself (in our case it is user-info.html) becomes the name of our HTML tag without the .html file extension. If we called it user-block.html our element would be referenced using that name instead.user-info.html.htmluser-block.htmlImporting & Using ItYou created a HTML only custom element in Aurelia, now what? Now, you import the component.<import from=""./components/user-info.html""></import>

<user-info user.bind=""user""></user-info><import from=""./components/user-info.html""></import>

<user-info user.bind=""user""></user-info>If you want to follow along with a tutorial for creating a simple weather application in Aurelia which involves creating a HTML only custom element and importing it, I have a weather application tutorial here.here",474
Building A Weather Application With Aurelia 2,"While Aurelia 2 is still not quite ready for release, you can use it right now as the basic core pieces are functional. I thought it would be fun to build a weather application using Aurelia 2.If you don’t want to run through a tutorial and just want the final result, all of the code for the weather application can be found on GitHub here.hereWhat Are We Building?In this tutorial, we are going to be building an Aurelia 2 application that displays weather information. Original, right? You will learn how to create new Aurelia 2 applications, as well as work with the OpenWeatherMap API where we will consume JSON for the information.Before we can continue, head over to the OpenWeatherMap website here and create a free account. This application will only be for personal use, so the limits of 60 calls per minute are perfect for us.hereYou will then want to generate an API key by heading over to the API Keys section once you are signed up and logged in.API Keys sectionA Note On Code In This TutorialAurelia allows you to leverage conventions for things such as custom elements, value converters and whatnot. It also allows you to be more explicit in how you name and import dependencies.For the purposes of this tutorial post, we will be leveraging conventions, but the code in the repository will leverage no conventions and use decorators for describing components. My personal preference is to be quite explicit in my Aurelia applications.Getting StartedUnlike Aurelia 1, there is nothing to install globally (the aurelia-cli is not a dependency you need any more). To bootstrap a new Aurelia 2 application, you simply open up a terminal/PowerShell window and run:aurelia-clinpx makes aurelianpx makes aureliaBecause TypeScript is the future, I recommend choosing the “Default TypeScript Aurelia 2 App” option in the prompt. Then choose, “npm” for the package installer option and wait for your app to be created.To confirm everything installed correctly, open up the generated application directory (in my case it is weather-app) and then run the application using npm start a browser window should open and point to port 9000.weather-appnpm start9000Create A Weather ServiceIn an Aurelia application, using singleton service classes is a great habit to get into too. Singletons are easy to test and work well with Aurelia’s dependency injection (DI).In the src directory create a new folder called services and a file called weather-api.ts which will handle making calls to the OpenWeatherMap API service.srcservicesweather-api.tsimport { HttpClient } from '@aurelia/fetch-client';
import { DOM } from '@aurelia/runtime-html';

const http = new HttpClient(DOM);

export class WeatherApi {
    private apiKey = '';
	private units = 'metric';

    public async getWeather(address) {
        const req = await http.fetch(`https://api.openweathermap.org/data/2.5/forecast?q=${address}&units=this.units&APPID=${this.apiKey}`);

        return req.json();
    }
}
import { HttpClient } from '@aurelia/fetch-client';
import { DOM } from '@aurelia/runtime-html';

const http = new HttpClient(DOM);

export class WeatherApi {
    private apiKey = '';
	private units = 'metric';

    public async getWeather(address) {
        const req = await http.fetch(`https://api.openweathermap.org/data/2.5/forecast?q=${address}&units=this.units&APPID=${this.apiKey}`);

        return req.json();
    }
}
This simple service will allow us to query the API for weather information. But, we are not using TypeScript to its full potential here. Let’s write some interfaces and type the response.import { HttpClient } from '@aurelia/fetch-client';
import { inject } from 'aurelia';

@inject(HttpClient)
export class WeatherApi {
    private apiKey = '';
    private units = 'metric';

	constructor(private http: HttpClient) {

	}

    public async getWeather(latitude: number, longitude: number): Promise<IWeatherResponse> {
        const req = await this.http.fetch(`https://api.openweathermap.org/data/2.5/forecast?lat=${latitude}&lon=${longitude}&units=${this.units}&APPID=${this.apiKey}`);

        return req.json();
    }
}

interface IWeatherResponse {
    cod: string;
    message: number;
    cnt: number;
    list: IWeatherResponseItem[];
}

interface IWeatherResponseItemWeather {
    id: number;
    main: string;
    description: string;
    icon: string;
}

interface IWeatherResponseItem {
    dt: number;
    main: {
        temp: number;
        feels_like: number;
        temp_min: number;
        temp_max: number;
        pressure: number;
        sea_level: number;
        grnd_level: number;
        humidity: number;
        temp_kf: number;
    };
    weather: IWeatherResponseItemWeather[];
    clouds: {
        all: number;
    };
    wind: {
        speed: number;
        deg: number;
    };
    rain: {
        '3h': number;
    };
    sys: {
        pod: string;
    };
    dt_txt: string;
}import { HttpClient } from '@aurelia/fetch-client';
import { inject } from 'aurelia';

@inject(HttpClient)
export class WeatherApi {
    private apiKey = '';
    private units = 'metric';

	constructor(private http: HttpClient) {

	}

    public async getWeather(latitude: number, longitude: number): Promise<IWeatherResponse> {
        const req = await this.http.fetch(`https://api.openweathermap.org/data/2.5/forecast?lat=${latitude}&lon=${longitude}&units=${this.units}&APPID=${this.apiKey}`);

        return req.json();
    }
}

interface IWeatherResponse {
    cod: string;
    message: number;
    cnt: number;
    list: IWeatherResponseItem[];
}

interface IWeatherResponseItemWeather {
    id: number;
    main: string;
    description: string;
    icon: string;
}

interface IWeatherResponseItem {
    dt: number;
    main: {
        temp: number;
        feels_like: number;
        temp_min: number;
        temp_max: number;
        pressure: number;
        sea_level: number;
        grnd_level: number;
        humidity: number;
        temp_kf: number;
    };
    weather: IWeatherResponseItemWeather[];
    clouds: {
        all: number;
    };
    wind: {
        speed: number;
        deg: number;
    };
    rain: {
        '3h': number;
    };
    sys: {
        pod: string;
    };
    dt_txt: string;
}Now, there is one thing I want to point out with the above example. We’re hard-coding the API key into the singleton class, in a real application, you would and should never do this. Anyone who has your API key will be able to make requests and blast through your limits quickly. Never store API keys client-side.We now have the class we will use to query for weather information. I won’t go into super specifics around markup and whatnot as those things can be seen in the GitHub repository for this tutorial here.hereLeveraging Dependency Injection (DI) To Import Our ServiceNow that we have our weather API service, we need to include it for use in our application. We will be editing the generated my-app.ts file as part of the Aurelia application creation process.my-app.tsWe want to replace the entirety of our my-app.ts file with the following:my-app.tsimport { WeatherApi } from './services/weather-api';

export class MyApp {
  private weather;

  constructor(private api: WeatherApi) {

  }

  attached() {
    navigator.geolocation.getCurrentPosition((position) => this.success(position), () => this.error());
  }

  async success(position: Position) {
    const latitude  = position.coords.latitude;
    const longitude = position.coords.longitude;

    this.weather = await this.api.getWeather(latitude, longitude);
  }

  error() {

  }
}
import { WeatherApi } from './services/weather-api';

export class MyApp {
  private weather;

  constructor(private api: WeatherApi) {

  }

  attached() {
    navigator.geolocation.getCurrentPosition((position) => this.success(position), () => this.error());
  }

  async success(position: Position) {
    const latitude  = position.coords.latitude;
    const longitude = position.coords.longitude;

    this.weather = await this.api.getWeather(latitude, longitude);
  }

  error() {

  }
}
Because my-app.ts is a rendered custom element (as can be seen inside of index.ejs we get DI automatically when we use TypeScript. This means we don’t have to use the inject decorator to inject things.my-app.tsindex.ejsinjectAll dependencies get passed through the constructor and using TypeScript with a visibility keyword, they get hoisted onto the class itself for use. It’s a nice touch and one of my favourite things about TypeScript.The attached method is a lifecycle method that gets called in components once the DOM is ready. This is where you handle interacting with the DOM or calling various API methods. We call the getCurrentPosition method here to request the users latitude and longitude.attachedgetCurrentPositionThe success callback is called via the navigation position callback on success. This is where we get the latitude and longitude values, then we make a call to our injected API class and call the getWeather method.successgetWeatherYou might also notice we are using async/await here which allows us to wait for the results of a promise and get the data. We assign the value to the class variable weather which will be referenced in the view template shortly.async/awaitweatherCreating A Value Converter For Formatting Dates/TimesWe are going to use the date-fns library for working with dates and formatting them. One of the values the weather API returns is a date string which we will parse and then format for display purposes.date-fnsYou might not have worked with date-fns before, but it is similar to Moment.js which is a heavier and often unnecessary option to go with.date-fnsTo install date-fns all we need to do is run:date-fnsnpm install date-fnsnpm install date-fnsIn Aurelia, a value converter is exactly what it sounds like. It converts values to something else (either to the view or from the view). In our use case, we only want to convert a value in the view.For resources, I highly recommend creating a resources directory which exports an array of resources to make global.resourcesCreate a file called src/resources/value-converters/date-format.ts and add in the following:src/resources/value-converters/date-format.tsimport { valueConverter } from '@aurelia/runtime';
import { format } from 'date-fns';

export class FormatDateValueConverter {
    toView(date): string {
        return date ? format(new Date(date), 'MM/dd/yyyy - h:mm bbbb') : date;
    }
}import { valueConverter } from '@aurelia/runtime';
import { format } from 'date-fns';

export class FormatDateValueConverter {
    toView(date): string {
        return date ? format(new Date(date), 'MM/dd/yyyy - h:mm bbbb') : date;
    }
}Inside of the resources directory create a file called index.ts with the following:resourcesindex.tsimport { FormatDate } from './value-converters/format-date';

export const resources = [
    FormatDate
];import { FormatDate } from './value-converters/format-date';

export const resources = [
    FormatDate
];This exports an array of one or more resources. For the purposes of this tutorial, we are only exporting one resource to make global. In a real application, you might have several. If you worked with Aurelia 1, this paradigm will look familiar with you with an array of resources that get globalised. Except, globalResources is no longer a thing.globalResourcesInside of src/main.ts we want to import those resources and register them:src/main.tsimport Aurelia from 'aurelia';
import { MyApp } from './my-app';
import { resources } from './resources';

Aurelia
    .register(resources)
    .app(MyApp)
    .start();
import Aurelia from 'aurelia';
import { MyApp } from './my-app';
import { resources } from './resources';

Aurelia
    .register(resources)
    .app(MyApp)
    .start();
The MarkupWe now have the basics in place, let’s start with our markup and styling. We are going to use Bootstrap 4 because it has a good grid system and will make styling things easier.Install and Configure Bootstrapnpm install bootstrapnpm install bootstrapBefore we start adding in any HTML, we need to import the main Bootstrap CSS file into our application.At the top of src/main.ts add the following import:src/main.tsimport 'bootstrap/dist/css/bootstrap.css';import 'bootstrap/dist/css/bootstrap.css';We now have the styling we need for marking up our columns and aspects of the grid system.Creating A Weather ComponentBreaking your application into components is a great way to create applications. Smaller components are easier to test, they are also easier to maintain and neater.Inside of the src directory create a new folder called components and create a component called weather-item which we will import and use to display our weather information.srccomponentsweather-itemNow, we want to create the HTML file for our custom element: src/components/weather-item.htmlsrc/components/weather-item.html<bindable name=""data"" />

<p><strong>Date:</strong> ${data.dt_txt | formatDate}</p>
<p><strong>Clouds:</strong> ${data.clouds.all}%</p>
<p><strong>Temperature:</strong> ${data.main.temp}&deg;</p>
<p><strong>Feels Like:</strong> ${data.main.feels_like}&deg;</p>
<p><strong>Humidity:</strong> ${data.main.humidity}%</p><bindable name=""data"" />

<p><strong>Date:</strong> ${data.dt_txt | formatDate}</p>
<p><strong>Clouds:</strong> ${data.clouds.all}%</p>
<p><strong>Temperature:</strong> ${data.main.temp}&deg;</p>
<p><strong>Feels Like:</strong> ${data.main.feels_like}&deg;</p>
<p><strong>Humidity:</strong> ${data.main.humidity}%</p>A brief explanation of what is happening here,  we just created a HTML only custom element. The bindable element at the top is telling Aurelia that we have a custom element which accepts a bindable property called data which allows data to be passed through.bindabledataYou will notice below when we import and use our element, we are binding on the data property by specifying data.bind. Inside of our custom element, we reference this bindable value specifically to get pieces of data passed in.datadata.bindIf you have experience working with other frameworks or libraries such as React, you might know of these as “props” in Aurelia they’re bindable properties.If you want to build custom elements with business logic that extends beyond simple bindables, you will want to consult the documentation on creating custom elements that leverage a view-model (something we do not need to do here).<import from=""./components/weather-item.html""></import>

<div class=""container spacer-v"">
    <form class=""mb-4"">
        <h2 class=""mb-3"">Weather. Whenever.</h2>
    </form>

    <div if.bind=""weather && weather.cod === '404'"">
        <h3>Error</h3>
        <p>${weather.message}</p>
    </div>

    <div class=""row"" if.bind=""weather && weather.cod !== '404'"">
        <div class=""col-md-3"">
            <div class=""row"">
                <div class=""col-md-3""><img src=""http://openweathermap.org/img/wn/${weather.list[0].weather[0].icon}.png""></div>
                <div class=""col-md-8"">
                    <h3>${weather.city.name}, ${weather.city.country}</h3>
                </div>
            </div>
        </div>
        <div class=""col-md-8"">
            <div class=""row"">
                <weather-item data.bind=""item"" class=""col-md-4 mb-3"" repeat.for=""item of weather.list""></weather-item>
            </div>
        </div>
    </div>
</div><import from=""./components/weather-item.html""></import>

<div class=""container spacer-v"">
    <form class=""mb-4"">
        <h2 class=""mb-3"">Weather. Whenever.</h2>
    </form>

    <div if.bind=""weather && weather.cod === '404'"">
        <h3>Error</h3>
        <p>${weather.message}</p>
    </div>

    <div class=""row"" if.bind=""weather && weather.cod !== '404'"">
        <div class=""col-md-3"">
            <div class=""row"">
                <div class=""col-md-3""><img src=""http://openweathermap.org/img/wn/${weather.list[0].weather[0].icon}.png""></div>
                <div class=""col-md-8"">
                    <h3>${weather.city.name}, ${weather.city.country}</h3>
                </div>
            </div>
        </div>
        <div class=""col-md-8"">
            <div class=""row"">
                <weather-item data.bind=""item"" class=""col-md-4 mb-3"" repeat.for=""item of weather.list""></weather-item>
            </div>
        </div>
    </div>
</div>The first line is simply using the import element to include our component for use in our view. The import element works like a Javascript import, except it can also import HTML files as well as CSS files. If you have experience with Aurelia 1, this is the same element as require.requireThe rest is standard Aurelia templating, if you’re familiar with Aurelia v1, this will look familiar to you already.If you’re new to Aurelia, I highly recommend reading the documentation to get a better understanding of how to author Aurelia templates and work with the templating.To complete the styling, open up my-app.css which is a CSS file included automatically by Aurelia (matching the name of my-app.ts using conventions).my-app.cssmy-app.ts.spacer-v {
    padding-top: 50px;
}.spacer-v {
    padding-top: 50px;
}Running The AppProvided you followed all of the steps correctly, to run the application simply type the following in the project directory:npm startnpm startA browser window should open and you should see an application that requests your current location and shows you the weather. It should look like this.There are some improvements you could make on your own like a field that allows users to type addresses, a map or more detailed weather information, adding in routing to view specific fine-grain information. This is a quick and basic tutorial showing you how easy it is to build in Aurelia 2.",4448
Learn Javascript First,"The front-end space over the last six years or so has really heated up, you could say superheated. As browsers become more powerful, devices continually improved and innovation a constant thing, no language is more popular and widely used than Javascript.And yet, as learning resources have become more easily accessible and coding boot camps have become a thing, newcomers are being taught to lean on frameworks and libraries straight out of the gate.This puts some newcomers into an interesting situation. They might have a good grasp of React or Vue, but lack basic fundamental knowledge of the language itself. It is all well and good to rely on a library, but the moment it can’t do something you want to do, you’re stuck.While React and Vue might seem like safe bets, I can assure you that people said the same things about Knockout, ExtJS, AngularJS, jQuery and a whole list of other frameworks and libraries that have come and gone over the years.People will tell you things are different these days, maybe they are. But what happens when Hype.js becomes the popular option and you’re forced to learn a new library with limited Javascript knowledge? You get left behind, that’s what happens.The only constant is JavascriptThe common theme here amongst these rising and falling trends when it comes to technology on the front-end is Javascript. While WebAssembly has high hopes of shifting some responsibility from Javascript, it will remain the number one choice for client-side scripting.As tempting as it might be to learn React Hooks or try Vue 3, the more you rely on a tool and use it as a language crutch, the further you’re falling behind. Learn Javascript and the rest comes naturally.If you are an experienced developer, you should be learning new frameworks and libraries, leverage your knowledge of Javascript to widen your skillset and pad out your C.V. If you’re a junior who graduates college or a coding boot camp, learn the language first.",491
Are Classes in Javascript Bad?,"Staunch functional proponents will fire up at the mere mention of classes or any form of object-oriented programming. These arguments go way back to before Javascript was even a thing or as popular as it is now.Let’s clear the air and point out that classes in Javascript are not real classes. They’re often compared to the likes of Java and other languages that promote OOP-style programming, but classes in JS are unlike those implementations. Javascript has prototypes and prototypes are not classes.When the ES2015 Javascript standard was released (its biggest update ever), a plethora of new and exciting features came with it. New syntax and API’s, in the mix, was classes (sugar over conventional prototypal inheritance). Perhaps the most controversial addition of them all.Here is the one thing that class opponents forget: classes in Javascript are optional. They’re not being forced on developers, you either use them or you don’t use them. It is really that simple. And yet, the arguments and noise around their inclusion (especially 2015/2016) you would be forgiven for thinking they’re a requirement to program in Javascript.classes in Javascript are optionalInheritanceOne of the biggest downsides to object-oriented programming and classes is inheritance. I am a fan of OOP style programming and even I agree that inheritance can be a nightmare and paint you into a corner.But here is a secret that functional programming proponents don’t want you to know: inheritance is optional.inheritance is optionalI use classes in my Aurelia applications and I avoid inheritance whenever possible. If I do use inheritance (which sometimes I do) I will be strict about it and only have one level of inheritance (parent-child relationship). More than one level of inheritance is a recipe for a bad time. I also try and avoid super calls as well.superNot all uses of inheritance are bad, it can be useful when you want to avoid duplicating code between multiple classes. If you’re building a widget-based component UI and your components all share similar implementation details except for a few configuration-specific pieces of data, inheritance works well here.In many cases, I use classes as structs for modelling specific pieces of data in my applications, for example:export class UserModel {
    constructor(name, email) {
        this.name = name;
        this.email = email;
    }
}export class UserModel {
    constructor(name, email) {
        this.name = name;
        this.email = email;
    }
}I actually prefer the aesthetics of a class over a function. I also love how I have to use the new keyword to explicitly create a new instance of my UserModel.newUserModelBut the argument that you shouldn’t use classes because it is easier to fall into certain traps is nonsense. Javascript is a language full of traps that extend beyond the likes of classes which are quite low on the scale of JS gotchas.If you are also working with TypeScript, the benefits of classes are even better when you throw collections and generics into the mix. The development experience just makes sense. I let the TypeScript compiler decide if my classes should be transpiled to functions, prototypes or classes.I’ve read quite a lot on the subject of OOP and the main argument always seems to boil down to inheritance, followed by personal preference.Composition <> InheritanceWhenever the classes vs functions debate arise, people take sides and stances on one side or the other. The truth is you should not and do not have to choose a side, you can use both.You can still use classes where they make sense and in other parts, use functions where they make sense. Sometimes you just need simple functions and other times, you might like the semantics of a class for organising your code.If you are implementing a feature/writing code and a function feels appropriate, write a function. If a class feels more appropriate, use a class instead.Web ComponentsIf you head over to the Google fundamentals for creating custom elements or Mozilla MDN in Web Components, surprise surprise you will find classes are what you use to author custom elements.head overMozilla MDNSure, you could just directly write the prototype chain yourself, but it’s going to result in ugly code that is just painful to maintain. The sweet syrupy abstraction that classes provide here is immediately obvious from an aesthetics perspective.I think classes make a lot of sense when creating custom elements. You’re extending the inbuilt element type and creating your own variant of it. One of the things that classes do well.Frameworks + First-Class CitizensAngular and Aurelia are two fully-featured front-end frameworks that have leveraged Javascript classes since the beginning in 2015. I have quite a few Aurelia applications in production, all leveraging classes, sprinkled with a function or two.The rewrite of Angular (Angular 2+) also treats classes as a first-class citizen. While React might be the most popular option out there, in the enterprise and government sectors, Angular is the king. A lot of Australian government agency applications are built using Angular.I have not seen or heard of any developer, agency or company running into any kind of problem as a result of classes being a requirement to build Aurelia or Angular applications. If you have, I would love to know.In instances where classes cause problems, it is because the developer using them is to blame. A bad mechanic blames their tools.Ignore The Noise, Form Your Own OpinionsI don’t pretend to have all of the answers or be the worlds greatest coder, but I know what works for me based on my years of experience. Be wary of anyone who argues there is only one right way to develop in Javascript because there isn’t.There are quite a few prominent figures in the JS community who will vehemently argue against classes. They will tell you they have seen companies lose millions, go bankrupt and projects completely scrapped because of classes.Most of the anti-class crowd have an agenda. You will discover the common thread amongst most anti-class dev-influencers is they’re selling training courses and other material. They will tell you there is only one way to do something and to signup for their “Right Way of Doing Things” course for developers.One person’s right is another person’s wrong.",1586
Thoughts On Svelte.,"The hype surrounding Svelte right now is inescapable. Every blog post comment section, the article comment section on Dev.to or Twitter thread/hot take seems to solicit a response about Svelte.If you are not familiar, Svelte is a Javascript library which leverages a compiler to turn your Svelte code into plain old Javascript and HTML. You write your applications inside of .svelte files and they get compiled to something that has no runtime. .svelteThis puts Svelte into a similar league alongside Elm, Imba and a few others, not directly in line with React or Vue. However, being compared to React or Vue seems to be unavoidable in 2019 and will continue to be the case into 2020 and beyond.In many ways, Svelte is an indirect competitor to the likes of React and Vue, both options which like to tout their small bundle and application sizes. On that front, they can’t compete with Svelte.Where Svelte differs from other options like React and Vue is that it does not have a virtual DOM, no runtime or anything else non-standard after it is built. Syntactically, your applications end up looking like old-school AngularJS and Vue syntax and a little sprinkling of React’s JSX syntax thrown in:<button on:click={handleClick}><button on:click={handleClick}>A lot of the examples you will see for Svelte highlighting its simplicity are not indicative of real-world applications. This rings true for many framework and library examples, showing as little code as possible. You don’t want to scare away users.Where things start to look less HTML and Javascript-y is things like loops or conditional each statements. The following is an example of iterating over an array of users.<ul>
    {#each users as user}
    <li>{user.id}: {user.name}</li>
    {/each}
</ul><ul>
    {#each users as user}
    <li>{user.id}: {user.name}</li>
    {/each}
</ul>If you have ever worked with Handlebars templating in Javascript before, then this syntax will take you back to the mid-2000s right away. This is one example of a few other uses which also resemble Handlebars in Svelte.Syntax aside, it is by no means a large criticism of Svelte. Every framework and library deals with looping over collections differently, except maybe for React and JSX which the community mostly uses a map to loop over items in collections.In React’s JSX you will find the following approach is the most widely used:In React’s JSX you will find the following approach is the most widely used:    <ul>
      {users.map((user, index) => {
        return <li key={index}>{user.name}</li>
      })}
    </ul>    <ul>
      {users.map((user, index) => {
        return <li key={index}>{user.name}</li>
      })}
    </ul>I actually would have loved to see Svelte adopt Aurelia’s approach to syntax instead of going down the path of Vue-like syntax and throwing in that Handlebars syntax.In Svelte binding the value of a text input looks like this:In Svelte binding the value of a text input looks like this:<input bind:value={name} placeholder=""enter your name""><input bind:value={name} placeholder=""enter your name"">And in Aurelia, binding a text input value looks like this:And in Aurelia, binding a text input value looks like this:<input value.bind=""name"" placeholder=""enter your name""><input value.bind=""name"" placeholder=""enter your name"">I realise my Aurelia bias is starting to show here, but in my opinion, I think the Aurelia approach looks a heck of a lot nicer and more JS syntax-like than the Vue bind:value approach. Not having to type a colon is a huge plus and it just looks neater.bind:valueAnyway, moving on. We are nitpicking here.It is Fast.There is no denying that Svelte is fast. The lack of runtime is the contributing factor here. The closer you are to the bare metal of the browser, the faster things will be.The truth is, all frameworks and libraries are fast. When it comes to speed and performance, the contributing factor is rarely the framework or library itself, it is the user code.Start pulling in various Node packages like Moment, adding in features such as validation and routing, and ultimately your bundle size is going to grow significantly. The end result might be the framework or library itself (even those with a runtime) accounts for 10% of your overall application size.This is why I always tell people to be wary of benchmarks. Sure, benchmarks might look impressive, but they are not indicative of real-world conditions where things like latency, bundle size, what libraries you are using, and how you write your code are really the determining factors.I think considerations to how a framework or library lets you author components and write code, what its features are, and what it allows you to easily and not easily do are more important than its speed. To put things into context, there are still many AngularJS 1.x applications out there in production, which are still working fine. I also know of many Durandal, Knockout and Backbone applications still being used which are also working fine.The generated code I have seen from Svelte applications is surprisingly readable as well. Usually compiled code is not easy to read (for humans) at all, so I was really surprised.Svelte Exposes The True Complexity of ReactFor years, React has hidden behind the claim that it is the V in MVC, that it is a simple and a humble view component library. Anyone who has ever worked with React on an actual application will tell you that you never just need the view part.I cannot recall a time where I have ever built a web application that didn’t have at least:Routing (the ability to define routes to different screens)The need to work with data from an APIForm validation (not always, but more often than not)Routing (the ability to define routes to different screens)The need to work with data from an APIForm validation (not always, but more often than not)If you want to add these features into a React app, you have to glue them all together. Because React utilises a Virtual DOM, the ability to just drop in and use any library that touches the DOM is not possible.The problem with React itself (without turning this into a post bashing React), is that it is too heavily invested into itself. It is also responsible for perpetuating FUD in the front-end ecosystem on quite a few fronts.React popularising Virtual DOM (and later on, Vue) would result in a lot of FUD around the DOM. When people tell you that the DOM is slow, they’re responding as a result of being programmed by the React community which drunk the “DOM is slow Koolaid” a few years ago.Svelte has proven that the DOM is not slow. Although to be fair, Aurelia has eschewed the Virtual DOM (in favour of reactive binding) since it launched in 2015 and managed to keep step with other frameworks and libraries for years (upcoming Aurelia 2, even more so).Now that React has introduced the concept of hooks into their library, it is yet another thing for developers to learn. Solutions like Svelte which do not require you to learn abstractions and ways of authoring applications definitely feel lighter and saner in the face of React.Cognitively React requires a few React-specific ways of working which just adds to the learning curve. The React of 2019 is not the React of 2014, that is for sure. Authoring applications using Javascript and HTML is kind of refreshing.Lack of ability to functionally compose viewsThis is one of those downsides of Svelte that some developers will struggle to look past. It requires you to use script tags and HTML to build your Svelte components. This means you are forced to use its templating syntax like #if, having to use #each for looping.#if#eachFor developers who have had a taste of “pure components” where all components are written in Javascript, this is going to be a hard pill to swallow.No TypeScript Support (Yet)Right now, there is no official support for TypeScript in Svelte. If you are not a TypeScript user or perhaps you work with Vue 2 which admittedly is not much better at supporting TypeScript, then this will not be a dealbreaker for you at all.no official supportIf you are like many other developers who realise the future is TypeScript and have switched over, the lack of TS support is going to be a dealbreaker for you. Some developers have gotten it to work sort of using hacks, but not ideal support by any means.ConclusionI think what Svelte has brought to the table is going to kickstart some innovation and competition in the front-end space. While React has been trudging along for quite a few years now and Vue picking up popularity as well, it’s nice to see some new thinking that doesn’t revolve around a Virtual DOM or leaky abstraction.Rest assured, you best believe that other frameworks and libraries are not going to sit idle while Svelte comes in and pulls the table cloth right off the dinner table.The AOT compiler coming in Aurelia 2, for example, is going to optimise your Aurelia applications to a high degree stripping away runtime and unneeded code. Angular has been focusing their efforts on improved AOT compilation with the Ivy compiler and renderer and other options are also focusing their efforts on the compilation as well.Even after playing around with Svelete just briefly, the lack of resulting code and marketing spin was refreshing to see after years of other players in the industry seemingly perpetuating immense amounts of hype.Having said that, the safety and stability that I get using a featured framework (in my case, Aurelia) still feels too hard to beat. I think Svelte is definitely going to get more popular and for non-complex UI’s it would be a great choice over React or Vue, but I still have hope that one day that Web Components becomes the norm and we see light abstractions on-top of WC that just compile to Web Components behind the scenes.I would love to see how Svelte scales in a large-scale web application. Not specifically in performance (because I think it would remain fast), but rather code organisation, maintainability, testability and how easy it is to bring new team members up to scratch with an existing codebase.Massive kudos to Rich Harris and everyone else who has worked on Svelte. I can definitely see the hype around Svelte is more than warranted and in the end, competition is healthy. We need fresh thinking and solutions to help drive standards and the ecosystem forward as a whole.",2599
Callback Functions in Aurelia Using .call,"Aurelia’s robust binding system allows you to not only bind values into your custom attributes and elements, but also the ability to pass in callback functions as well. While your first instinct might be to try using &lt;my-element callback.bind=""myFunction"" you will quickly realise that this will not work because of scoping issues.&lt;my-element callback.bind=""myFunction""This is where .call comes in. Using .call allows you to pass in callback functions to your bindables, and when they get called, their scope is retained..call.callCallbacks Without ParametersSay we have a custom element we have created called my-custom-element and it has a bindable property called callback defined inside of the view-model usingmy-custom-elementcallback@bindable callback = () => {}@bindable callback = () => {}Then our custom element callback binding looks like this:<my-custom-element callback.call=""someCallbackFunction()""><my-custom-element callback.call=""someCallbackFunction()"">Inside of our custom element, when the callback bindable is fired, it will call our callback function and the scope of someCallbackFunction will be retained (the view-model it is defined in).someCallbackFunctionWhen you are not using parameters things are easy enough. You just need to define your callback with the circular function brackets like you would if you’re using click.delegate or other more event-type bindings.click.delegateCallbacks With ParametersThis is where I see developers get caught out quite a bit, passing parameters to callback functions. Using our above function, let’s say that our callback accepts two parameters: user and item.useritem<my-custom-element callback.call=""someCallbackFunction(user, item)""><my-custom-element callback.call=""someCallbackFunction(user, item)"">Inside of your custom element, when you call the callback you might try something like this if you didn’t read or understand the documentation correctly:this.callback(this.selectedUser, this.selectedItem)this.callback(this.selectedUser, this.selectedItem)Because of how the .call feature works, this will not work (as you might have possibly already discovered). This is because you need to pass an object to the callback with your parameters matching the names you use in your HTML..callIn our case, we are expecting two parameters: one called user and one called item to be passed into the callback.useritemInside of our custom element, we need to pass them like this:this.callback({user: this.selectedUser, item: this.selectedItem})this.callback({user: this.selectedUser, item: this.selectedItem})Now, our values get passed correctly and the callback retains its scope.",662
Reasons To Use Aurelia in 2020,"It is almost the year 2020, and you are still not using Aurelia, say it isn’t so. No more excuses, it’s time to charge up your Bluetooth keyboard and mouse batteries, make yourself a coffee and start using Aurelia.Version 2 is coming and it’s going to be FASTWhile Aurelia 1 is plenty fast, Aurelia 2 is a complete rewrite of Aurelia from the ground up. It not only focuses on JIT (Just In Time) runtime performance but also sees the introduction of AOT (Ahead of Time) compiling support which will yield upwards 100x speed improvements in your applications.Aurelia 2 will take the learnings of Aurelia 1, along the way the features that many developers love and supercharge them, as well as add in new features that modern web applications require.Low Learning Curve, Highly ExtendableThe low-learning curve of Aurelia is unrivalled by any other framework. Show me a fully-featured client-side framework that can build ambitious large-scale web applications that require minimal documentation diving to use.I have helped upskill developers from all facets in Aurelia and it truly speaks for itself. Just recently two backend developers at my work started moving into the front-end and within the space of just a week were writing and shipping Aurelia code.Besides the ease-of-use, the extensibility is once again unrivalled. From the dependency injection through to templating, routing and compiling, every part of Aurelia can be overridden and replaced.Dependency InjectionThere are too many benefits of Dependency Injection to list, but there is no denying that DI is immensely useful and powerful. Aurelia has a great DI layer allowing you to pass around dependencies in your applications and specify the type of injection mode they should use.One of the great benefits of Dependency Injection is unit testing. Because dependencies are passed through the constructor in your components and view-models, it allows you to easily mock them or my favourite approach of overriding the value in the container with a mocked version.While it is possible to shoe-horn some semblance of DI into other libraries and frameworks, Aurelia provides a true Dependency Injection layer which can be used in your applications and keeping in line with the mantra of “easy to override” you can configure whatever way you want too.First-class TypeScript SupportI have been working with Aurelia since 2015, I have been using it with TypeScript since 2016 and Aurelia has always supported TypeScript without the need for anything additional to be installed. The framework ships with strong typing support and in Aurelia 2, TypeScript is even more supported. The codebase of Aurelia 2 is completely written in TypeScript.Long-term Support // Minimal Breaking ChangesSome of us Aurelia veterans have been running production applications since 2015 when it was an alpha release. As time went on and Aurelia reached beta, then release candidate stages before stable 1.0, while the framework was improved and changed, the core design stayed the same.In its almost five years of existence, there has not really been one single breaking change. The core architecture and vision of Rob has remained untouched. There are not many frameworks and libraries that can boast continual improvement without some form of breaking change or convention.Flexible Binding ApproachesDespite what some developers believe, there is no denying that two-way binding is amazing if used correctly. The ability to bind values to a view-model from a form input is immensely powerful. Unlike something like React which forces you to litter your code with change function callbacks to update values.One wayTwo wayFrom viewTo viewOne timeOne wayTwo wayFrom viewTo viewOne timeBatteries IncludedWhy waste your time deciding what Router library to use, what state management solution you should use, whether you want to write in some XML like HTML syntax or how you will deal with validating some form inputs?Aurelia is a glue-less framework that provides you with all of the modules you’ll need to build web applications. Let’s not get it twisted, it is quite rare that you just need the view component, most web applications we build generally require routing, validation, state management and templating.Do you really want to wake up to the sound of tears on Christmas day because you forgot to buy the batteries and all of the shops are closed?",1098
How To Easily Mock Moment.js In Jest,"Recently whilst writing some unit tests in Jest, I had to test some code that took ISO date strings and converted them to formatted date strings, then code that converts them back to ISO strings before it’s sent to the server.My first attempt was to use jest.mock and mock each individual method. For some of the uses of moment where simple dates are being converted, it is easy enough to mock format and other methods, but once you start chaining Moment methods, things get tricky from a mocking perspective.jest.mockformatThis is some code that would be a nightmare to mock in Jest:moment.utc().add('1', 'years').format('YYYY')moment.utc().add('1', 'years').format('YYYY')It turns out there is a much easier way to “mock” moment, without actually mocking it at all. You get a fully functional (well in my use case) version of Moment that actually converts dates and allows you to use chaining features.jest.mock('moment', () => {
  const moment = jest.requireActual('moment');

  return {default: moment };
});jest.mock('moment', () => {
  const moment = jest.requireActual('moment');

  return {default: moment };
});You use the jest.requireActual method to require the real Moment package, then you return it inside of an object. I am having to return it with default because moment is being included in my application like this:jest.requireActualdefaultimport moment from 'moment';import moment from 'moment';It’s a surprisingly simple, functional and elegant solution. It requires no absurd nested mock functions and code. If for whatever reason you need to override certain Moment methods, you can do so either inside of the mock declaration or on a per-use basis.",418
Fixing The Webpack File Loader [object Module] Issue,"Recently I updated to the latest version at the time of writing this post 5.0.2 of the file-loader plugin for Webpack. I use this for dealing with some image files in my project amongst other things.5.0.2file-loaderTo my surprise after updating, I noticed my SVG images had all broken without explanation. It turns out a recent fix to the esModule option had enabled a default value of true for esModule which generates Javascript modules that use ES syntax.a recent fixesModuleesModuleThis simple fix had some serious consequences in my application, all of my SVG image elements were showing [object Module] as the source (which clearly is not going to work).[object Module]Now, it does not take a genius to see the problem here. If you are dealing with SVG files, this is going to break them. Maybe the file-loader plugin was never intended to be used with SVG images, but I and many others do, so it is a bit of a problem.file-loaderTo cut a long story short, the fix is to set esModule to false:To cut a long story short, the fix is to set esModule to false:esModule{
    test: /\.(ttf|eot|svg|otf)(\?v=[0-9]\.[0-9]\.[0-9])?$/i,
    loader: 'file-loader',
    options: {
        esModule: false,
    },
},{
    test: /\.(ttf|eot|svg|otf)(\?v=[0-9]\.[0-9]\.[0-9])?$/i,
    loader: 'file-loader',
    options: {
        esModule: false,
    },
},This essentially reverts the behaviour back to the way file-loader has always worked, by using CommonJS syntax to resolve back to the default export of the file itself.file-loader",382
My Experiences Using Apollo Client & Server With Blockchain,"Some of you might know that I spend my time immersing myself in the latest and greatest technologies and a couple of years ago got active involved in cryptocurrencies and blockchain.The rise of GraphQL has become too high to ignore. Unlike traditional RESTful API’s, GraphQL uses an expressive query language to allow you to query your server for the pieces of data that you need, leaving the implementation details on the server in resolver functions.At the start of 2019, I open sourced a GraphQL implementation over the top of the Steem blockchain, specifically a layer on top of the Steem blockchain called Steem Engine. I named the library Steem Engine QL. If you are not aware, the Steem blockchain is a fast blockchain with no fees and 3 second block times. It is perfect for content, decentralised applications and other use cases where you need a fast open source blockchain.Steem EngineSteem Engine QLGraphQL is a perfect fit for blockchainAfter creating my initial implementation one thing that stood out immediately was how much easier it made querying the blockchain and returning data.In some instances, I needed to combine data from multiple sources and return it in the one request. On the client-side, this would have taken two API requests, then taking the results, filtering and combining into the final structure. Now, this happens on the server and the API returns everything as one. A good example is the coinPairs type here.coinPairshereOn the front-end, that coinPairs data is fetched like this (the code is located here):coinPairsherequery {
  coinPairs {
    name,
    pegged_token_symbol,
    symbol
  }
}GraphQL === Fast Feature IterationHaving used the above GraphQL server implementation in a large-scale open source project one of the biggest benefits is the ability to iterate and implement new features.large-scale open source projectInstead of having to write implementation logic on the server to return needed data, once all of the query types and resolvers have been implemented, you just query for what you need. If a REST API were being used, it would require continual development work to add in new endpoints and maintain existing ones.Because GraphQL promotes typing your resolvers and return types, everything is self-documented, so you know what the server supports and returns data wise. This is an area where REST simply cannot compete, and I am a huge TypeScript fan so it aligns with my code quite nicely.Case in point, just yesterday in a few short hours of work, an entire new feature was implemented into the codebase without requiring any API work whatsoever. Most of the work was simply just UI, querying the API for the needed pieces of information.Why Apollo?I tried a few different GraphQL implementations before settling on Apollo for the server. The thing with Apollo is that the company seems comitted to open source and is well-funded as well. It has great integration with numerous frameworks and libraries, and has sorted out some of the painpoints in GraphQL: namely caching and request batching support using DataLoader.On the other side of the stack, the Apollo Client library makes querying the Apollo Server a breeze, with support for caching including an in memory cache which does a fantastic job caching your GraphQL queries.You can find the code for Steem Engine QL here and see it being used in an open-source application here.herehere",852
Storing The Last Dispatched Action In Aurelia Store,"Another day, another Aurelia Store tip/hack. Recently, I encountered an interesting situation where I wanted to know when a specific action has fired, being able to check for it within my state subscriptions.Now, it is highly likely there is a better way to do this. RxJS is quite magical and has a seemingly bottomless trove of treasures and ways to work with observables and data.The use case is simple. Inside of my subscriptions, I want to know if a specific action has fired and in some cases, what the parameters for said action were. I decided this was the perfect use case for an Aurelia Store middleware.function lastCalledActionMiddleware(state: State, originalState: State, settings = {}, action: CallingAction) {
    state.$action = {
        name: action.name,
        params: action.params ?? {}
    };

    return state;
}function lastCalledActionMiddleware(state: State, originalState: State, settings = {}, action: CallingAction) {
    state.$action = {
        name: action.name,
        params: action.params ?? {}
    };

    return state;
}This is the middleware function and registration. It sets a property defined in your state called $action which stores the currently fired action passing through the middleware as well as the parameters supplied. I prefix with a $ to make the chances of it being overwritten elsewhere highly unlikely.$actionWhen registering the middleware, I only want to know when it has fired. So, I choose to place it after.this.store.registerMiddleware(lastCalledActionMiddleware, MiddlewarePlacement.After);this.store.registerMiddleware(lastCalledActionMiddleware, MiddlewarePlacement.After);As you can see below in Redux Developer Tools, my property is being stored and the parameters supplied to the dispatched action.If you know of a better way to do this, I would love to hear about it. For my use cases, this works quite well and fine. A middleware seems like the perfect use case for something like this.",490
Some Small Blog Changes Preempting Aurelia 2,"As Aurelia 2 draws near, I have made some changes to my blog. Acknowledging that many of you rely on my blog as a source of information and help with Aurelia 1, I have created an Aurelia 2 category and renamed the existing category to Aurelia 1.This will hopefully alleviate any confusion that you might have when Aurelia 2 is released. While many of my Aurelia 1 articles will still be relevant, they might not always apply to the new version of Aurelia.Until Aurelia 2 is released, these changes do not currently mean much. But, after release, they will.Another change you might have noticed is a new theme. The existing theme served me well for years, but now, it is time to try something newer and still easy to read. I am using the Less theme.Less theme",190
Getting Typescript 3.7 To Work With Webpack and ts-loader,"At the time of writing this post, TypeScript 3.7 is in beta. Eventually, this post will become irrelevant. But, for the moment if you are trying to get TypeScript 3.7 Beta or any of the RC builds working with Webpack and ts-loader you might have encountered a bunch of red text in your console.ts-loaderIn my case, I had target: ""esnext"" set in my tsconfig.json file which the ts-loader plugin should read and set the appropriate settings. And yet, TypeScript 3.7 Beta was not working despite making sure everything was up to date.target: ""esnext""tsconfig.jsonts-loaderIt turns out at present, ts-loader does not seem to work with esnext as the target value (hopefully, this changes when TypeScript 3.7 is released). To get things working, all you need to do is change your target value in tsconfig.json to es2018 like this: ""target"": ""es2018""ts-loaderesnexttsconfig.jsones2018""target"": ""es2018""In my case, that fixed the issue and I could use the exciting new features TypeScript has to offer such as Nullish Coalescing and Optional Chaining. Happy days.",264
Getting a 404 Error Dealing With File Uploads To Storage In Firebase?,"I am a huge proponent of Firebase and interestingly, up until recently, I had never used Firebase Storage. Instead, I usually opt for Amazon’s S3 service which I am familiar with. Wanting to keep everything in the one service is appealing to me, so I started to add in file upload functionality in a Firebase Cloud Function.It was not as straightforward as I would have hoped. I am using Express with Firebase and the Google Cloud NPM package as documented in code examples and numerous tutorials.After adding in the  @google-cloud/storage package into my Cloud Function file, I plumbed it all in and figured it would work. Then I got this vague error. @google-cloud/storage {
    ""errors"": [
        ""domain"": ""global"",
        ""reason"": ""notFound"",
        ""message"": ""Not Found""
    ]
}Even with an error message, I was left scratching my head. I had nothing I could really Google, looking through the documentation failed to give me the answer and then out of frustration and trial and error, I discovered the cause.This is the code that I had setting up my storage object and bucket instance. Tell me if you can spot the problem.import { Storage } from '@google-cloud/storage';
const storage = new Storage();

const bucket = storage.bucket('steem-engine-dex');The problem is with the bucket name, I also had to add in the project ID as well.storage.bucket('steem-engine-dex.appspot.com', {
    userProject: 'steem-engine-dex'
});The bucket name in Firebase requires adding .appspot.com into the bucket name and sure enough, if you go into Firebase itself and into the Storage section, you’ll notice something that points to this..appspot.comThe bucket name after the protocol gs:// actually has the bucket name with appspot.com in it. Unless I am blind, nowhere does it mention any of this in the documentation, it really tripped me up.gs://appspot.comI wasted a lot of time working this out. So, hopefully, if you have experienced the same issue, this blog post saved you a few hours or days worth of wasted work. Happy coding.",508
Jest Not Finding Tests In Travis CI? You Might Be Ignoring The Build Folder,"Recently, I encountered an issue in Travis CI and my Aurelia application which uses Jest for the tests suite. I erroneously copied some older configuration code for Jest and used it in my Jest configuration, assuming it would work.This error took a while to isolate and resolve. It was out of pure desperate that I accidentally discovered the fix. If I made the mistake, it is possible that you might as well. Does this sound familiar to you?Tests were working locally. Running Jest would run my tests and they would all pass, giving a nice 0 code, happy days. Pushing my changes up causing my Travis CI build to automatically run would result in an error. The error looked like the following (this is the actual error):One thing you will notice is my testRegex is saying there are 3 matches, which is correct. I have three test files which run and work locally without issue. I tried modifying my testRegex I tried changing the location of my tests, nothing worked. Then I noticed I had testPathIgnorePatterns defined in my Jest configuration (part of my older config).testRegextestRegextestPathIgnorePatternsThis is my actual code. I was ignoring my build, dist and sample directories from testing. It turns out defining this locally causes no problem, but causes Travis CI to fail. At first, I was confused, why is this breaking Travis CI.builddistsampleThen it dawned on me. It’s the fact I am using some outdated configuration option, the issue is the path in which tests and code are run inside of the virtualised Travis CI environment.Can you spot the issue? The path is /home/travis/build/steem-engine-exchange/steem-engine-dex what is contained within this path? A folder named buildwhich is located under /home/travis ignoring build causes this to get matched and thus, it breaks the tests from running./home/travis/build/steem-engine-exchange/steem-engine-dexbuild/home/travisMy innocent ignore rule was being triggered only in Travis CI, not locally because locally my files are run from my home directory and not within a build directory.",513
"Using Wallaby.js With Aurelia, Jest and TypeScript","Wallaby.js is one of the most amazing additions you can make to your testing workflow. I have been a happily paid user for a couple of years now and if you are looking to up your testing game, I highly recommend it.Chances are if you are reading this post, you already use Wallaby and you are looking to get it working in your Aurelia applications with Jest and TypeScript. It’s a combination that is not all too uncommon these days, TypeScript is the future.The Wallaby.js configuration itself requires very little code to work out-of-the-box with Aurelia and Jest. module.exports = function (wallaby) {

  return {
    files: [
      '!**/*.css',
      'src/**/*.ts',
      'src/**/*.html',
      'test/unit/helpers.ts',
      'test/unit/mock-data/**/*.ts',
      'test/unit/stubs/**/*.ts',
      'aurelia_project/environments/**/*.ts',
      'test/jest.setup.ts',
      'tsconfig.json'
    ],

    tests: [
      'test/unit/**/*.spec.ts'
    ],

    compilers: {
      '**/*.ts': wallaby.compilers.typeScript({ module: 'commonjs' })
    },

    env: {
      runner: 'node', 
      type: 'node'
    },

    testFramework: 'jest',

    debug: true
  };
};module.exports = function (wallaby) {

  return {
    files: [
      '!**/*.css',
      'src/**/*.ts',
      'src/**/*.html',
      'test/unit/helpers.ts',
      'test/unit/mock-data/**/*.ts',
      'test/unit/stubs/**/*.ts',
      'aurelia_project/environments/**/*.ts',
      'test/jest.setup.ts',
      'tsconfig.json'
    ],

    tests: [
      'test/unit/**/*.spec.ts'
    ],

    compilers: {
      '**/*.ts': wallaby.compilers.typeScript({ module: 'commonjs' })
    },

    env: {
      runner: 'node', 
      type: 'node'
    },

    testFramework: 'jest',

    debug: true
  };
};The above configuration assumes your files live in src and in my case, inside of my test/unit directory I have a helpers.ts file which has some functions making testing easier, a mock-data directory for mock data to import, a stubs directory for stubbing out certain mocks as well as my main Jest configuration file jest.setup.ts.srctest/unithelpers.tsmock-datastubsjest.setup.tsDo not copy the above file line-for-line. Make sure it reflects your project and the files inside of it.In the compilers section we have to tell Wallaby to compile our TypeScript to commonjs format. In my case, I was originally targeting esnext as my module format and Wallaby does not work with modern JS syntax just yet. the rest is fairly explanatory.compilerscommonjsesnextHere is what Wallaby looks like running in an application I am currently working on.",646
Implementing A Method To Get Single Values In Aurelia Store,"The Aurelia Store plugin is a great way to add powerful and easy to use state management into your Aurelia applications. For some, the paradigm shift in how you work can be confusing to work with at first. One such task you might find yourself needing to do is obtaining a singular value from the state.As you will soon discover, state management in Aurelia Store requires you to subscribe to the entire state and react to all changes. Sometimes you just want to get a single value from the state to reference inside of your view-models.While I cannot guarantee Vildan (Aurelia core team member and author of Aurelia Store) will approve of my solution, it works 😂VildanThe solution I came up with which fits my needs perfectly is a method to subscribe, get one or more values from the state and then unsubscribe.export const getStatePropertyOnce = async (...properties: string[]) => {
  return new Promise(async (resolve, reject) => {
    const subscription = store.state.pipe(pluck(...properties)).subscribe(
      (value) => { resolve(value); },
      (error) => { reject(error); }
    );
    subscription.unsubscribe();
  });
};export const getStatePropertyOnce = async (...properties: string[]) => {
  return new Promise(async (resolve, reject) => {
    const subscription = store.state.pipe(pluck(...properties)).subscribe(
      (value) => { resolve(value); },
      (error) => { reject(error); }
    );
    subscription.unsubscribe();
  });
};The beautiful thing about this method is it not only supports top-level properties, but it also works for nested properties in your state object as well. In a real use case, I have the locality value of the user’s client stored in the state.async getAppClient() {
    this.appClient = await getStatePropertyOnce('appClient');
} async getAppClient() {
    this.appClient = await getStatePropertyOnce('appClient');
} There might be some pitfalls I have yet to encounter with the above functionality and you should also make sure your await is wrapped in a try/catch block as well to handle errors, but it works for my needs. If the thought of having to use @connectTo or manually setup a subscriber and then dispose of it within your view-models sounds like a lot of work, this helper method can be a good time saver.awaittry/catch@connectToasync getValueFromStore() {
    // Object in state is represented as user.details.email
    this.someValue = await getStatePropertyOnce('user', 'details', 'email');
} async getValueFromStore() {
    // Object in state is represented as user.details.email
    this.someValue = await getStatePropertyOnce('user', 'details', 'email');
} In the above example, we are using the method to get a property of email which exists on an object called details which lives inside of a main user object. The method handles getting nested properties with ease.emaildetailsuser",713
Ryan’s Toy Review Is Child Exploitation,"Being a parent, as any other parent would attest is rewarding, but hard work. And nothing has made modern parenting more challenging than YouTube.Being a parent, as any other parent would attest is rewarding, but hard work. And nothing has made modern parenting more challenging than YouTube.We managed to not expose our firstborn son to any TV or screens until he was two. We were doing well until we had a trip booked from Australia to the UK, which was a 23-hour trip. We bit the bullet and bought an iPad to load up with some activities for the journey and some carefully selected YouTube videos.We managed to not expose our firstborn son to any TV or screens until he was two. We were doing well until we had a trip booked from Australia to the UK, which was a 23-hour trip. We bit the bullet and bought an iPad to load up with some activities for the journey and some carefully selected YouTube videos.Things started innocently enough, until the YouTube app, when connected to the internet, would begin automatically playing similar recommended videos. I do not recall the exact moment, but eventually, Ryan’s Toy Review found its way into our lives.Things started innocently enough, until the YouTube app, when connected to the internet, would begin automatically playing similar recommended videos. I do not recall the exact moment, but eventually, Ryan’s Toy Review found its way into our lives.If you are a parent of children who watch YouTube, there is a good chance they have come across Ryan’s Toy Review or one of its other associated channels like Ryan’s Family Review.If you are a parent of children who watch YouTube, there is a good chance they have come across Ryan’s Toy Review or one of its other associated channels like Ryan’s Family Review.Ryan’s Toy Review might have started out with good intentions, but estimates place the earnings of the channel at around $22 million (US Dollars) per year. When you are earning that much money, you are a business, not an innocent child playing with toys.Ryan’s Toy Review might have started out with good intentions, but estimates place the earnings of the channel at around $22 million$22 million (US Dollars) per year. When you are earning that much money, you are a business, not an innocent child playing with toys.The parents regularly feature in Ryan’s videos, the mother has a very irritating voice and is always shouting. The father seems to be softly spoken and somewhat likeable, but the mother takes centre stage, and it sometimes feels like she is the subject of the video, not Ryan.The parents regularly feature in Ryan’s videos, the mother has a very irritating voice and is always shouting. The father seems to be softly spoken and somewhat likeable, but the mother takes centre stage, and it sometimes feels like she is the subject of the video, not Ryan.Watch any of the videos produced lately on the channel, and you will notice they all have one thing most in common: they’re advertising Ryan branded products. None of the videos has any kind of disclaimer on them that says they are promoting paid products either. Sometimes a flash of text or a few seconds of the video will tell you it’s a promotion, but intentionally short.Watch any of the videos produced lately on the channel, and you will notice they all have one thing most in common: they’re advertising Ryan branded products. None of the videos has any kind of disclaimer on them that says they are promoting paid products either. Sometimes a flash of text or a few seconds of the video will tell you it’s a promotion, but intentionally short.In one of the videos, Ryan visits a Colgate factory and does a tour, the video starts off innocently enough until you realise it’s an advertisement for Ryan branded dental products. The way they frame it is Ryan just spends a fun day making toothpaste and mouthwash, like it’s a visit to the museum.In one of the videos, Ryan visits a Colgate factoryvisits a Colgate factory and does a tour, the video starts off innocently enough until you realise it’s an advertisement for Ryan branded dental products. The way they frame it is Ryan just spends a fun day making toothpaste and mouthwash, like it’s a visit to the museum.The ironic thing about these videos, is there are numerous ones of Ryan eating and playing with chocolate, talking about McDonald’s and other unhealthy forms of food that would cause decay.The ironic thing about these videos, is there are numerous ones of Ryan eating and playing with chocolate, talking about McDonald’s and other unhealthy forms of food that would cause decay.People are beginning to notice that Ryan’s Toy Review and associated channels are deceiving and tricking children into buying their products. So much so, a complaint has been registered with the FTC.People are beginning to notice that Ryan’s Toy Review and associated channels are deceiving and tricking children into buying their products. So much so, a complainta complaint has been registered with the FTC.And it is clear the parents are reaping the rewards of the success of the channel. They started another channel called Ryan’s Family Review, where they travel and do things most kids do not get to do. Perhaps most desperate of all is the channel his parents started called The Studio Space.And it is clear the parents are reaping the rewards of the success of the channel. They started another channel called Ryan’s Family Review, where they travel and do things most kids do not get to do. Perhaps most desperate of all is the channel his parents started called The Studio Space.We blocked Ryan’s Toy Review and Family Review channels, but one channel which is new is Studio Space (since blocked). Most of the videos centre around Ryan’s parents and other weird guests, with a rare appearance from Ryan himself from time-to-time.We blocked Ryan’s Toy Review and Family Review channels, but one channel which is new is Studio Space (since blocked). Most of the videos centre around Ryan’s parents and other weird guests, with a rare appearance from Ryan himself from time-to-time.I get the impression Ryan’s mother Loann, loves fame and money because she appears the most in these videos. It feels like a desperate attempt to pivot, knowing that Ryan is getting older and becoming more interested in video games like Minecraft over toys.I get the impression Ryan’s mother Loann, loves fame and money because she appears the most in these videos. It feels like a desperate attempt to pivot, knowing that Ryan is getting older and becoming more interested in video games like Minecraft over toys.The Studio Space videos see Ryan’s parents acting out role plays, playing with toys, pushing cardboard boxes into inflatable pools and other nonsense in a desperate attempt to retain their large following and subsequent profits from that following.In the regular YouTube app, you cannot outright block a channel from being watched or appearing. However, if you download the YouTube Kids app, you can. We actually have blocked the channel in our house, and I recommend everyone else does the same.In the regular YouTube app, you cannot outright block a channel from being watched or appearing. However, if you download the YouTube Kids app, you can. We actually have blocked the channel in our house, and I recommend everyone else does the same.Ryan’s Toy Review is a business selling products to vulnerable children disguised as innocent toy review videos.Ryan’s Toy Review is a business selling products to vulnerable children disguised as innocent toy review videos.",1890
Mocking Default Imports In Jest With TypeScript,"If you are writing tests using Jest and you use TypeScript, there is a good chance you have encountered an error along the lines of TypeError: defaultsDeep_1.default is not a function or TypeError: myClass.default is not a constructor when trying to test a file that is using a default import from a module.TypeError: defaultsDeep_1.default is not a functionTypeError: myClass.default is not a constructorYou most likely have read countless StackOverflow questions, but none of the solutions will solve the issue. You’ve read the Jest documentation (which is quite extensive), but still no mention of mocking default module imports with TypeScript.In my case, I had this error when trying to import a Lodash function defaultsDeep and another when importing the Input Mask module. My imports look like the following.defaultsDeepimport defaultsDeep from 'lodash/defaultsDeep';
import Inputmask, { Options, Instance } from 'inputmask';
Inside of my test which will be testing this specific file, I use jest.mock to mock the specific modules and their implementations. The important thing to note here is I am returning default from within my mocks. This is because of how default imports are transpiled within TypeScript.jest.mockdefaultThe Lodash mock is more simplistic:The Lodash mock is more simplistic:jest.mock('lodash/defaultsDeep', () => {
  return {
    default: jest.fn()
  };
In the case of Input Mask, I needed to mock an instance which has a method on it. The usage in the actual file highlights what we want to achieve. The input mask plugin is newable, it then exposes a mask method which we supply with an element.maskthis.im = new Inputmask(options);
this.im.mask(element);
This is how we mock the above module and accommodate for the usage:This is how we mock the above module and accommodate for the usage:jest.mock('inputmask', () => {
  return {
    default: jest.fn().mockImplementation(() => {
      return {
        mask: jest.fn()
      };
    })
  };
});
The convenient thing about the solutions presented is they will work for all default imported modules. Have fun.",523
My Experiences One Year (and counting) Working From Home,"A little over a year ago I took a new job and because the office is close to an hour and a half away, I wanted to work remotely for most of the week. Commuting upwards of three hours a day five times a week would have destroyed me.So, while I don’t work 100% of the week remotely, I work two days in the office and three days at home. Everyone has their own experiences working remotely, and I thought it would be interesting to share my perspective and experience.Oh, and for context, I have a four-year-old son and a six-month-old daughter. My son goes to daycare three days per week, and there is some overlap with the days I work from home, my wife is a stay at home mother and also currently studying as well.You need a dedicated spaceYou need a consistent working space. Working from home if you love what you do, it’s not hard to be disciplined, if anything, it is difficult to stop working once you start (more on that later). I often hear people remark when I tell them I work from home they would find it hard not to watch Netflix or slack off.Working remotely while more relaxed, you should still act like you’re in an office. Get yourself set up with an office or corner of a quiet room, buy a comfortable chair and a sturdy desk.Noise cancelling earphones are a mustIf you work from home and you cannot guarantee that you will only ever be there alone in business hours, get yourself some noise-cancelling earphones. For me, it’s a crying baby and an energetic four-year-old running around the house, the TV going or music.It’s not fair to expect everyone else to change their routine or how they go about their day for the sake of making it work office like for me at home. Also, if you live near a busy street, you’ll have outside noise like cars and motorbikes.At present, I have to deal with a loud street sweeper making hourly trips up and down my street.  This is because of some development work happening at the end of the road.It can be harder to stop workingSome people have laughed when I tell them that working from home makes it harder for me to stop. With no commute to and from the office, I find I start earlier and finish later quite often because being in the comfort of your own home can be deceiving.For me, this is really the only downside. Fortunately, my wife is great at making sure I finish at 5 pm a lot of the time, mainly because I help with the night time routine and I can help free her hands up to do other things not related to the kids.Slack makes the distance more comfortable to manage, mostlyFor some, the lack of office co-workers is a dealbreaker for them. And admittedly, at times not having a co-worker to bounce an idea off of or head out to lunch with can be a bit of a drag at times.For those times, when I need to speak to someone, they’re only a Slack message or video call away. Because I live in Australia, and we have embarrassingly slow internet (my connection is decent compared to the average), sometimes there are technical difficulties getting on a video call because our main office can’t get a proper connection in the area yet (despite the fact it ironically is one of the first places in Australia to have 5G rolled out).I highly recommend if you’re working remotely to install the Visual Studio Code LiveShare extension, it allows remote coding sessions (including the ability to share a terminal), so you can do remote pair programming and troubleshooting as well.You save a lot of moneyBy cutting my commute time 80%, we are refuelling our car a lot less (once every 1.5 weeks). What felt like twice-yearly car services, has now become just the one annual service. We are putting fewer kilometres on the odometer, which is a considerable saving. Special shout out to the environment, I’m reducing my carbon footprint in the process.And then you have the saved money on coffee and food. When I used to work in an office, I would eat out more often than I care to admit. This is usually how you bond and socialise with your colleagues, over a nice takeout lunch. My wife is the queen of food preparation, so we always have a good selection of lunches to pull out of the freezer for lunch.Then you have the big wallet drainer, the big “C” coffee. I am not sure how much a cup of coffee costs where you are, but where I live it’s on average AUD $5, which is about USD $3.40. It adds up really quick when you can drink upwards of four coffees per day (especially during meetings).At home, we invested in a coffee machine, and if you work remote and love coffee, I highly recommend splurging on a coffee machine and some quality beans of your choosing. The cost per cup is so low, the only dangerous thing you need to watch out for is drinking too much coffee.The savings don’t stop there. Because I work from home, at tax time, I am allowed to claim some costs as work-related expenses. I can claim part of my internet bill, cost of buying stationery and other things your accountant can help you out with. Getting some money back from the government is always a good thing.So, while I am probably spending more on AC during the summer months, using my electricity and water, it still works out cheaper than a commute to an office and eating out. Winning.Increased productivityWorking from home makes you so much more productive if you can trust yourself to be disciplined and not sit on YouTube all day long. Working in a traditional office is full of interruptions, detrimental to productivity. There is nothing worse than getting in the zone and having someone tap you on the shoulder, or meetings that run overtime.When someone wants to ask a question, they have to Slack me, and if I am set to busy aka do not disturb, I won’t even see the message until I check. I get to reply on my own terms.You get sick lessIf you have worked in open space offices (or any office), you will be familiar with office plagues. There is always that one or more persons who come into work sick (like it’s a badge of honour) and spread their sickness around like Germaclaus (an ill version of Santa that spreads sickness).I have only been sick once this past year and like I said in the opening of this post, I have a son who goes to kindergarten three days per week. The fact I get sick less with a child in kindy than I do working in an office, it says a lot.Last, but, not least…I get to work in my pyjamas. On those three days, I am at home, I get into the shower and then put on some comfortable pants and a shirt. There is nothing like working in whatever you want to wear.When I am in the office, I have to put on an ironed collared shirt and chino pants with a belt, but at home, I am one step away from being ready for bed.",1672
When To Use State Management In Front-end Applications?,"As ubiquitous as state management has become in front-end development, it is still a confusing magical black box to most developers. Data goes in, data goes out, and nobody thinks about what happens in-between.Some developers believe the answer to the question in my title is: always. While some don’t believe in using state management at all and if you’re like me, the answer is: it depends.When state management gets added to an application that meets the criteria for using it, a weight gets lifted off your shoulders, and things make sense. Prematurely introduce state management or use it in places where you shouldn’t, and your life becomes a tangled mess.The complexity of state management starts to get even more confusing when questions arise around best practices for working with API endpoints or dealing with forms. The answers are primarily opinion-based once again.Avoid state management for formsI cannot stress this enough. I have seen developers implement hacky solutions to working with form inputs and state management, and it’s a clear case of the right tool for the right job. While Redux and other state management solutions have plugins for dealing with forms, why inflict pain on yourself unnecessarily?You might not agree with me on this one, and that is okay. However, every single time, I can recall seeing state management, coupled with forms, was unnecessary. You only have to Google to find a tonne of people asking for help getting state management to work with forms to see why you shouldn’t.Forms are often always ephemeral state, meaning the data only exists temporarily. An example of a form might be login form with a username and password or a form for adding a new product to your store. You enter the data and dispatch an action, the form gets cleared, and that’s it.Instead of replicating and nesting properties in a massive state tree for one specific part of your application that some users might not even use, use local state instead. If you’re working with React, this would be local state within a component (using something like the useState hook) and similar with Aurelia or Vue, local state within your view-model or component.useStateJust because you can doesn’t mean that you should.Working with API’sDepending on your state management solution of choice, the approach for working with API’s can vary depending on plugins and workflow. However, the principle is the same. Your action(s) make an API request and update the state, or you make the request and dispatch an action with the response.I know in Vue’s VueX state management plugin, many in the community advocate for making your API requests inside of your actions. There isn’t anything wrong with that; however, in Aurelia’s state management library Aurelia Store, I advocate for making the request and then notifying the store.It doesn’t matter how your data gets into the state, more-so what kind of data you are putting in the state is what truly matters.Do I need this data again, will I use it more than once?State management is recycling your data. Will you need that value again in other parts of your application? Use state management. Do you only need to store the value temporarily and reference it in a specific component, only for it to be discarded shortly after that? Don’t use state management.Asking yourself the following question should be the litmus test you apply to your development workflow. Will you need this value again and will you need it in other parts of your application? Type it up, print it out and hang it up on your wall.The purpose of state management is not to play the role of “random kitchen drawer full of miscellaneous items”, it exists to make cross-component and cross-application data access easier as well as ensuring the integrity and shape of the data remains intact (in part due to Javascript passing everything by reference).Using GraphQL?You might not need state management at all. GraphQL offerings like Apollo offer an all-in-one package for working with data, including state management like functionality that makes syncing and working with your GraphQL server a breeze.While there is nothing stopping you from using GraphQL with state management libraries, and some GraphQL clients might require them to meet your needs, in many cases you only need one or the other.State management can introduce unnecessary complexityIf you have ever seen a React + Redux application, you know what I am talking about, a mess of folders and files scattered through your application. You have to open up seven files to change something, and it’s a tonne of cognitive overload.Something I want to make very clear here: the complexity of using something should never be the deciding factor in whether to use it or not. The next time you start on a new application, don’t be so quick to add in state management but don’t leave it too late.the complexity of using something should never be the deciding factor in whether to use it or notIf you’re validating an idea or prototyping, it can slow you down having to write all of the boilerplate most state management libraries require. Sometimes you need to be “agile” and flexible, and state management can be quite rigid and the opposite of that.When it comes to state management, do what works for you. Trust your intuition, and if something feels complicated and unnecessary, your gut instinct is probably right. Posts like these are great as a guide, but ultimately you should never take everything as gospel.",1376
Default Exports = Bad,"Hello humans. In JavaScript, the worlds most loved and internets favourite client-side language, thanks to modern ECMAScript standards, we have default and named exports.It’s simple, and you have a file that exports something to be imported somewhere else. A named export is explicit and is only importable by its defined name. A default export is implicit, and you can import it and call it whatever you like.Now, default exports came about in the CommonJS world of Node.js where you would import a module using const MyModule = require('my-module') to account for uses where exports are default module.exports = MyClass – although, it is worth pointing out that CommonJS does support named exports.const MyModule = require('my-module')module.exports = MyClassThe most persuasive case for named exportsAll modern code editors and IDE’s provide autocompletion functionality. If you are using Visual Studio Code (chances are, you are already), then you get some nifty auto-complete functionality out-of-the-box, even if you are not using a superset like TypeScript.A default export receives no such auto-completion hints because it’s a default export, it could be anything; a class, a function, a constant. A named export explicitly tells your code editor what you’re exporting and importing.Furthermore, default exports are difficult, if not, impossible for bundlers to tree-shake your code. A default export means that instead of just keeping the code you’re using, the entire file or in some cases an NPM Package is bundled into your code, and therefore adds bloat.There are a plethora of other interesting issues that have arisen for people, further highlighting the reasons for avoiding default exports. Rich Harris succinctly worded it in his response to an issue on the Rollup repository on GitHub in 2016.succinctly worded
We absolutely would have been. Default exports have caused no end of problems. People get desperately confused by all the different forms of import/export declaration – imagine if we could teach people that you either import { names } or * as namespace, and that you can export either names or declarations. As it stands, it feels like there’s a ton of different variations you have to understand.
Plus the confusion that arises over whether default exports are live or not. I’ve spent more time learning about ES modules than anyone should reasonably be expected to, and I had no idea that the situation is as you’ve described. (Marked this issue as a bug, btw.)
And then there’s the interop headaches. Ostensibly, privileged default exports were meant to make adoption easier for a community that’s familiar with Node modules, which is ironic as nonsense likemodule.exports.default has probably caused more friction than any other aspect of ES modules. I’m sure we could have come up with a better way of importing single-export CommonJS modules. (Though we shouldn’t really call them CommonJS modules – CommonJS modules can only have named exports!)
Unfortunately, we’re stuck with it.

We absolutely would have been. Default exports have caused no end of problems. People get desperately confused by all the different forms of import/export declaration – imagine if we could teach people that you either import { names } or * as namespace, and that you can export either names or declarations. As it stands, it feels like there’s a ton of different variations you have to understand.Plus the confusion that arises over whether default exports are live or not. I’ve spent more time learning about ES modules than anyone should reasonably be expected to, and I had no idea that the situation is as you’ve described. (Marked this issue as a bug, btw.)And then there’s the interop headaches. Ostensibly, privileged default exports were meant to make adoption easier for a community that’s familiar with Node modules, which is ironic as nonsense likemodule.exports.default has probably caused more friction than any other aspect of ES modules. I’m sure we could have come up with a better way of importing single-export CommonJS modules. (Though we shouldn’t really call them CommonJS modules – CommonJS modules can only have named exports!)Unfortunately, we’re stuck with it.
Default exports are lazyThere is no reason to use a default export unless you’re lazy and cannot be bothered taking the extra 5 seconds to add curly braces around your import and make sure your export is named.There are exceptions when you’re dealing with a third-party package and have no control over how the exports are defined. However, even so, in that situation, a pull request on the repo for the library you’re using might be worth considering.There is no legitimate reasoning for default exports, but there is plenty of legitimate reasoning against them. Make your life easier and avoid them altogether.",1205
TAD (Test After Development),"Testing is a crucial part of any modern development process. If you’re not testing your code, you might as well be writing it blindfolded and hoping when you push to production that it works, if the company doesn’t go bankrupt from all of the bugs first.But, I am a realist. Being honest, we all start out wanting to build things right. We want to test everything, writing both unit and integration tests to make sure everything works as intended. The company you work for might even allow for time to write tests until reality hits you on the back of the head.Priorities change. The higher-ups want the product you’re working on to ship in two months, and there is easily four months of work in Jira, it’s going to be a mission to get it all completed in time.In my experience, tests are usually the first thing to be removed in the face of more pressing priorities (like actually building the product and making money).A word on Test Driven Development (TDD)I have always been a fan of test-driven development. And I have been fortunate to be in a position where I have been able to explore TDD, and when it works, boy does it work. By works, I mean when a company agrees to invest the time into a test-driven development process.Every project I work on, my first thought is “TDD would be great for this”, but once again, priorities shift, and it becomes hard to justify the short-term investment for the long-term gain that TDD provides. Even if your entire development team wants something, management gets the final word, and it all comes down to money in the end (we’ll talk about that later).You need testsIn an ideal world, we would all be writing our tests first and then making our code make them pass, make them fail, make them pass. A good test not only helps you design clean code, but it also has the added benefit of documenting your code as well.If TDD is more time consuming and harder to justify to your company, does this mean you give up on writing tests completely? Heck no.In any medium to large application, tests are as crucial as decent server infrastructure; they should be the oxygen to your app brain. No oxygen and the brain will slowly die.Inevitably, many developers end up resorting to TAD (Test After Development) because it’s easier and faster (at least initially). Writing the code first and then going back and writing the tests after the fact. It is not super ideal, but any tests are better than no tests.Many would argue that if you get into the habit of TDD, over time, you will get faster at writing tests and code, that the long-term benefits outweigh the short-term caveats. The buy-in from stakeholders is the hard part. If it were easy, everyone would be doing TDD.The longer you work on a project, the more crucial tests become. As the scope widens and feature set increases, things are more likely to break, and some of your architectural decisions early on are bound to come back and bite you (something that TDD would help you most likely avoid).The whole point of TDD is that initially, work takes longer to complete than not prescribing to TDD, but your code will be more stable and less error-prone.An experienced surgeon doesn’t rush to cut you open and perform heart surgery right away; they take their time and slowly get the job done. Programming is not heart surgery, but if you’re working on critical systems where functioning clean code is essential (like a space shuttle or nuclear power plant), it’s equally as important you get it right.However, it all comes down to costThe deciding factor in any decision you make within your company always comes down to money. The short-term benefits of TDD are hard to quantify, and if you do your job correctly, the number of bugs and refactoring work you need to do will be substantially lower (almost zero), but how do you prove that to non-technical higher-ups?You can’t. It’s all well and good to say we have fewer bugs since adhering to TDD principles, but it is challenging to prove that TDD is the reason for that and not just increased familiarity and skill-level being the main factor.You can’t
Does it take more time to finish a task? Yes.
Will it cost the company more time and money? Yes.
Will there be a learning curve for inexperienced developers (especially juniors)? Yes.
Does it take more time to finish a task? Yes.Will it cost the company more time and money? Yes.Will there be a learning curve for inexperienced developers (especially juniors)? Yes.Once again, you could argue that writing the code and then writing the tests, going back and refactoring your code and then having to fix your tests takes a lot more time using TAD: you’re right. Looking at TDD through neutral coloured glasses, there are more benefits than downsides if time is not an option.But, the industry is weird. Many managers still measure the value and productivity of developers by the number of lines of code they write and commits they’re pushing up. And what produces the most lines of code and commits? TAD. It’s inefficient, but even non-technical people can see you look busy.Developer A and Developer B side-by-side, Developer A is writing and shipping code faster than the other. It might be laden with errors and horrendous compared to Developer B’s well-architected and clean code, but it works and if there is one thing managers love more than saving money, it’s shipping code on time.ConclusionIf you can’t convince the company you work for to give TDD a chance, TAD is still an acceptable albeit less than the ideal alternative of no tests. As long as you have tests, there is always room for improvement.",1404
What I Love About Aurelia,"There is no shortage of Javascript frameworks and libraries to choose from. Notably, you have Angular, React, and Vue, which are the most discussed and used.If you are a regular reader of my blog, you know how much I love Aurelia and have blogged about it since early 2015. If you are new, let me quickly introduce myself.I have been a developer for 11 years now, working in the front-end space. My experience stems back to the likes of; ExtJS, YUI, Backbone, Knockout, Durandal and Angular v1. Believe it or not, I also used to work with React back in 2014/2015.I still remember seeing the post on Hacker News announcing the Aurelia alpha release. The date was January 26th, 2015. After reading through the website and goals of the framework as well as being familiar with Rob’s previous work, I gave it a go and never looked back.the postSince 2015, I have been working with Aurelia daily in my day job (current job as well as my previous job) as well as side-projects and random ideas; there isn’t a project that I haven’t used it on.Aurelia promotes strong separation of concernsWhen I first started in web development, the one thing ingrained into me from the beginning was: separation of logic and templating, or separation of styles and markup also known as Separation of Concerns (SoC).Web pages by their natural order promote separation by concern. HTML is used to structure and define the page; CSS is used to style the HTML and Javascript responsible for page interaction and logic.In Aurelia, your business logic gets handled within a view-model, markup handled inside of a view and styling is handled inside of a stylesheet. When you think about it, Aurelia doesn’t try replacing what browsers and specs already give you; it enhances them.Separation of concerns is a repeated theme throughout the framework. Components, custom attributes and routed views all utilise the same conventions and approach to writing a web application.Think about how you would develop a web application or page without a framework these days, and it would be an HTML file for the markup, a stylesheet for the styling and a Javascript file which contains any logic for interacting with the page or dealing with data. No conventions, just basic concepts that never change, this is what it is like building in Aurelia.I do not want to turn this post into a framework/library bashing contest, but this is one of the things I dislike about React. It reminds me of working with PHP applications (not using a framework) where business logic, styling and markup are all intertwined with one another.There might be other ways these days with later versions of React, but the basic premise is you stuff a bunch of XML like HTML (JSX) inside of a render function which is called by React.  You break up you UI into smaller components and use props to pass data through to them. While JSX is  not required, it is the most commonly used syntax I have seen for authoring React components.renderLet me acknowledge that A) I haven’t worked with React in a few years and B) I am biased because of my use of Aurelia. Please correct me if any of my assumptions and claims about React are inaccurate, and I can amend them.Batteries included, no glue requiredOne of my absolute favourite things about Aurelia is the fact it gives you almost everything you need out-of-the-box. It gives you powerful templating, API’s, dependency injection, routing.Then you have the plethora of plugins on offer for other functionality. Want to use Fetch with more beautiful syntax and error handling? Aurelia Fetch Client. Want to localise your application into other languages? Aurelia i18n. Need a configurable modal in your application? Aurelia Dialog.If you want to add in state management to your Aurelia application and don’t want to get a frontal lobotomy using something over-engineered and complicated like Redux, there is Aurelia Store which is an RxJS powered state management library compatible with the Redux Dev Tools browser extension.In my experience working with other frameworks and libraries; some require glueing together a lot of independently maintained dependencies to build what is essentially a framework.Why waste your time creating a framework piece-by-piece every single time you want to build something. Save yourself the node_modules weight and time, use something that gives you what you need straight away.node_modulesAurelia can be used without needing a bundler or build processIf your experience goes as far jQuery where you drop in a script and start writing code, then the ability to use Aurelia in script form might appeal to you. Thanks to Aurelia Script, you can add in a script and starting authoring powerful web applications.Aurelia ScriptNot only is this approach useful for quick prototyping on Codepen or Codesandbox, it means you spend less time battling with complicated build process in Webpack and fixing complex issues and extension incompatibilities.If I am honest, not many of us enjoying configuring Webpack, opting to pull out our teeth with a pair of pliers instead.It has no Virtual DOMThanks to other popular frameworks and libraries, the words Virtual DOM have become synonymous with developers naively believing that it means fast applications. While the Virtual DOM is impressive from a technical perspective, it’s not the only way to achieve fast performing web applications.In Aurelia, the binding system is reactive; this means that when something changes, it gets updated in your view. Because there is no Virtual DOM, you’re working with just the real DOM, and you can use third-party libraries which need to touch the DOM without any problems.The more you read into what a Virtual DOM is and how it works, the more you realise it’s not necessarily always the most performant choice.It’s easy for newcomers and all skill level developersI have seen developers upskill in Aurelia from a multitude of levels. I have seen back-end developers grasp Aurelia in a few days, I’ve seen front-end developers used to working with jQuery or React also get it quickly, and I have seen newcomers grasp it equally as fast.I think this is a unique selling point for Aurelia; you don’t need a PhD in Javascript to know what is going on with it. Its conventions based approach means you spend less time configuring Aurelia and more time writing code.The Aurelia CLI is fantasticAll useful frameworks and libraries these days have a CLI which allow you to get a starting application up and running with the features and coding styles needed: Aurelia is no exception.There isn’t anything unique about the Aurelia CLI, but it makes your life a lot easier when you’re building a new project.Familiar syntax, aligned with standardsIn an era where framework and library authors are reinventing the wheel or introducing foreign concepts not found in any specification, Aurelia goes against the grain by leveraging standards-based concepts and API’s.The templating syntax for displaying a value uses the same interpolation syntax ${myVariable} that you would inside of template literals inside of your Javascript/TypeScript code. Binding to properties is equally as intuitive, want to bind a value on a form input? <input type=""text"" value.bind=""bindToThis"">${myVariable}<input type=""text"" value.bind=""bindToThis"">If you want to use Web Components with Aurelia, there is a plugin for that and the next version; Aurelia will be aligned with the HTML Modules specification to give you single-file components like you might find in Vue, only standards-based.This is one of the most unique features and selling points of Aurelia. It allows developers to work in a way that is familiar, a way that requires very little buy-in with the intent to eventually allow official specifications to replace features found in Aurelia.",1954
Working With An API In Aurelia Store,"Unless you’re working on a completely static web application, chances are you’ll be needing to get data from an API or send data to one. In Aurelia, you would usually do this either using the HTTP Client or Fetch Client (you should be using Fetch). However, what about Aurelia Store? The versatile state management library for Aurelia.Aurelia StoreYou have two approaches you can take when using Aurelia Store with an API, and we’ll discuss the pros and cons of each one. This article is going to be a small and relatively non-technical article.Make API requests from within your actionsAurelia Store supports asynchronous actions, meaning you can delay their execution until the async function promise resolves. A simple Aurelia store action might look like this if using async.async function myAction(state) {
    const newState = { ...state };

    // Do something in here

    return newState;
}When you return from within an async function, it’s akin to a Promise.resolve the entire function itself becomes wrapped in a promise, but we’re not going to go into specifics of how async/await works in this post.Promise.resolveMaking a function async means, you can call an API from within your action and then store the result in your state when it loads. This simplified code assumes that you’re accounting for errors in the loadUsers, hence there not being a try/catch block below.loadUserstry/catchasync function myAction(state) {
    const newState = { ...state };

    newState.users = await api.loadUsers();

    return newState;
}This approach works well because it means you can load data through actions, they are easier to debug, and you can perform transformations on the data returned. It also makes refactoring your code so much easier as you only need to change one action.However, the downside is if the request above takes longer than expected. Maybe you’re using an Azure or Google Cloud Function (lambda function), and it takes a few seconds to start because it’s not warmed up, this request might take a few seconds to complete and meanwhile, your application slows down to a crawl. If you’re calling multiple actions, it’ll delay calling the other actions until the current one returns a state.If you keep an eye on your actions and ensure the requests are tiny, then this should never be a problem. Adding a visual loading indicator into the application can also alleviate any loading issues. However, assumption-led decision making can sometimes backfire.API Call First, Dispatch Action SecondIn the above approach, the request and action are one function. In this approach, you request the data first and then dispatch an action after.There is an obvious upside to this approach. Your actions execute fast; they don’t have to wait for API calls to resolve before continuing. Another upside is that it ensures your state layer and API logic remains separate. The maybe not-so-obvious downside is you now have to write and maintain two different pieces of code.async function myAction(state, users) {
    const newState = { ...state };

    newState.users = users;

    return newState;
}

async function loadUsers() {
    const request = await fetch('/users/all');
    const response = await request.json();

    return response;
}

export class ViewModel {
    async activate() {
        const users = await loadUsers();
        dispatchify(myAction)(users);
    }
}But, which one do I use?The point of this post is not to tell you that you should use one over the other; it is to point out that the two approaches have upsides and downsides. Either all is fine, you only need to be aware of the caveat of async actions. When you create an asynchronous function, they all become async.I use the second approach in my Aurelia applications. I like decoupling my state management from my API logic completely. It means if I want to remove state management at a later date or use something else, there is no tight coupling in my code. Remove my actions and call the API functions myself from my view-models (or wherever).",1011
My Thoughts On GitHub Sponsors,"It’s hard to believe that it has almost been a year since Microsoft completed its acquisition of GitHub. While a vocal number of people in the community decried the decision and some moved to GitLab since the acquisition Microsoft has made a series of positive moves.It all started with GitHub making private repositories free in January 2019, up to three collaborators. This is a move that directly competed with competing Atlassian owned source control platform Bitbucket which offered free private repos as well.And then recently, GitHub announced GitHub Package Registry, an in-preview up and coming rival to npmjs for Node packages, but also extending to other languages and tying in with the repository itself.GitHub Package RegistryAnd then not too long after that, most recently, GitHub announced GitHub Sponsors allowing individual repository owners to be paid for their open source contributions.GitHub SponsorsWhile the latest new feature has been applauded by many, some see it as aggressive and an attempt by Microsoft to push similar open source sponsoring platforms out using its size and power to do so.One such popular offering for open source is Open Collective, projects like Webpack and Aurelia use it so the community and companies can sponsor and fund development. With GitHub Sponsors, initially, the feature is only available for individuals, but presumably, once it goes public organisations will be able to opt-in to the feature as well.Another popular option used by the likes of Vue.js (raking in six figures yearly in donations) is Patreon. These platforms, however, are disconnected from the projects themselves, requiring users to signup to a separate service to use.I see GitHub Sponsors as a brilliant and much-needed move by Microsoft and GitHub. Funding and open source is a constant hot topic and while other services do exist, they are not integrated with GitHub itself. An in-built way of soliciting monetary contributions and all without requiring users to signup to another service, it’s a huge win for open source.Best of all, GitHub is forgoing fees for the first year and then charging them after. But, it’s a nice incentive to join and try it out without losing anything. If it works, projects will presumably keep using GitHub Sponsors for their projects and if not, use a different offering.Will GitHub Sponsors make people all of a sudden start paying for the free software and code they’re benefitting from? Who knows. All I know is this is a void that needed to be filled, one that GitLab and Bitbucket failed to address as well (and presumably will copy).",651
Masked Inputs In Aurelia: The Easy (and reliable) Way,"When it comes to adding in masked inputs into a modern Javascript web application, it is easier said than done. The task at hand is simple, yet, under the surface is paved with complexity in a framework with unidirectional data flow.The problem I am going to describe is also a problem you’ll encounter in Angular, Ember, Vue and any other framework or library which offers two-way binding on input elements or modifying the input itself.The ProblemYou have an input element. You want users to be able to enter a value and automatically format it as they type. This input could be anything, but in my situation, the input was for entering numeric values.I wanted the value to automatically add a comma for the hundreds, add two decimals to the end and a dollar sign ($) symbol as a prefix to the value.By default in Aurelia, binding on input elements is two-way. This means the value is both updated in the view and view-model, which in many cases is great.As you can imagine, in the case of wanting to format an input element automatically you are instantly fighting with the framework. The problem is the plugin that does the formatting is modifying the input, and Aurelia itself is also modifying the input.Why not a value converter?Why not a value converter?You can create a value converter (my first attempt) and leverage the toView and fromView methods for mutating the values going to and from the view.toViewfromViewA value converter gets you quite a lot of the way, but one problem you will encounter is the caret position will jump to the end. When the value is modified in the input, the entire input is effectively refreshed and the cursor jumps to the end of the value which is jarring and annoying.How about a custom attribute?How about a custom attribute?My second attempt involved using a custom attribute that listened to the input and blur events. I added in some checks of the value and attempting to work around the caret position by reading it whenever the input was modified and setting the position after.inputblurUltimately, I fell into some of the same problems the value converter presented to me. Getting and setting the caret position is tricky business and something I ideally do not want to maintain in the long-term, having to work around issues in different browsers is another problem there.I knew the only solution had to be leveraging an existing input mask library. A library which supports a plethora of formatting options, masks, working with currencies and other types of data and most importantly: solves the caret jumping problem.The SolutionI tried a lot of different approaches, referencing implementations for not only Aurelia but Angular and Vue as well. However, the common theme in many of these solutions is they were very complicated. One such plugin I found was over 600 lines of code, many of those specifically for getting and setting the caret.The final solution ended up being laughably simple, in hindsight. I will show you the code and then run through it below. I am using the inputmask plugin which is a battle-tested and highly configurable input masking plugin.inputmaskWhatever library you choose to use, the code will end up looking the same if you follow the approach I have taken.import { inject, customAttribute, DOM } from 'aurelia-framework';

import Inputmask from 'inputmask';

@customAttribute('input-mask')
@inject(DOM.Element)
export class InputMask {
  private element: HTMLInputElement;

  constructor(element: HTMLInputElement) {
    this.element = element;
  }

  attached() {
    const im = new Inputmask({
      greedy: false,
      alias: 'currency',
      radixPoint: '.',
      groupSeparator: ',',
      digits: 2,
      autoGroup: true,
      rightAlign: false,
      prefix: '$'
    });
    
    im.mask(this.element);
  }
}

export class CleanInputMaskValueConverter {
  fromView(val) {
    if (typeof val === 'string' && val.includes('$')) {
      // Strip out $, _ and , as well as whitespace
      // Then parse it as a floating number to account for decimals
      const parsedValue = parseFloat(val.replace('$', '').replace(/_/g, '').replace(/,/g, '').trim());

      // The number is valid return it
      if (!isNaN(parsedValue)) {
        return parsedValue;
      }
    }

    // Our parsed value was not a valid number, just return the passed in value
    return val;
  }
}

export class PrecisionValueConverter {
  toView(val, prefix = null) {
    const parsedValue = parseFloat(val);

    if (!isNaN(parsedValue)) {
      if (prefix) {
        return `${prefix}${parsedValue.toFixed(2)}`;
      } else {
        return parsedValue.toFixed(2);
      }
    }

    return val;
  }
}
The solution in its simplest terms is three parts:The solution in its simplest terms is three parts:
A custom attribute applied to input elements called input-mask which instantiates the plugin and applies the masking options
A value converter which strips away any fragments of the mask; $, _ and ,, trims whitespace and then parses the value using parseFloat
A value converter which formats a value passed from a view-model and adds a prefix (if specified) and converts the value to a precision of 2 decimal places
A custom attribute applied to input elements called input-mask which instantiates the plugin and applies the masking optionsinput-maskA value converter which strips away any fragments of the mask; $, _ and ,, trims whitespace and then parses the value using parseFloat$_,parseFloatA value converter which formats a value passed from a view-model and adds a prefix (if specified) and converts the value to a precision of 2 decimal placesDuring this exploration phase, I stumbled upon a powerful and awesome feature in Aurelia that I did not even know was possible, multiple value bindings. You will notice below that I have a value.one-time as well as a value.from-view binding on the input.value.one-timevalue.from-viewThe reason for this was I wanted to initially set the value and then not worry about syncing it again. This allows data loaded from the server to be passed in (initial values, etc). The value.from-view binding updates our view-model every time the value changes.value.from-view<template>
    <input 
        type=""text""
        value.one-time=""value | precision"" 
        value.from-view=""value | cleanInputMask""
        input-mask>
</template>
It makes a lot of sense that you can do this, but it’s not a documented feature and initially, I wasn’t 100% confident it would work. I am an experimenter, so the worst-case scenario was it wouldn’t work. This is what I love about Aurelia, it can handle anything that you throw at it, even things you think might not work, but end up working.Basically, this approach creates an inline binding behaviour where you control the direction of the updating process, which is quite powerful and cool.A great addition to Aurelia itself could be a binding attribute which offers this functionality (allowing a once to view and always from view mode).Conclusion & DemoThe end result can be seen here, as you can see we have a nicely formatted as you type input element. This was built for a specific use-case so it is missing configuration options, however, I have created a repository here which will have a plugin-friendly version of this soon.herehere",1826
Saved By The Aurelia Portal Attribute,"Recently at my day job, I encountered a very specific scenario that I wrestled with for quite a bit. I had a routed set of views, which were using a layout view template because it needed a very specific markup for positioning using CSS Grid.The issue I had was although the route layout had a <slot></slot> element inside of it for projecting the routes, I wanted a custom navigation element to be projected inside of the routed view. Previously there was a bit of duplication to add the custom element into the right area.<slot></slot>I initially tried to use router View Ports to achieve the result I needed, but they’re more designed for rendering sub-views within a view, I needed to render a component into a specific area of the page and have it react to route change events.Then I remember the Portal attribute by core team member Binh Vo, it has actually been around for a while now. I haven’t had a need for this plugin, until now. In my situation, I needed to leverage the target property of the attribute to tell it where to inject my custom element and everything worked as expected.My layout view simplified looks like this:My layout view simplified looks like this:<section>
    <main-nav></main-nav>
    <inner-nav portal=""target: #nav-slot;""></inner-nav>
    <slot></slot>
</section>For each routed view the simplified markup looks like this:For each routed view the simplified markup looks like this:<template>
  <main>
    <div class=""dashboard-wrapper"">
      <div class=""inner-content"">
        <section>
          <div id=""nav-slot"">
            <div class=""content"">
                <h1>Some content</h1>
            </div>
          <div>
        </section>
      </div>
    </div>
  </main>
</template>When this routed view is loaded, the portal attribute will inject the element from the router layout into the DIV with the ID nav-slot. Super simple stuff and it does exactly what I needed it to do. The portal plugin can be found on GitHub here and it’s great to see how it all functions behind-the-scenes. A demo of the plugin in action can also be found here.nav-slotherehere",526
Creating Your Own Javascript Decorators in Aurelia,"Decorators are currently a stage 2 proposal in Javascript and they allow you to decorate classes, class properties and methods. If you have worked with Aurelia for longer than 5 minutes or other frameworks such as Angular, you will already be familiar with them.At the core of a decorator, it is a function that returns a function. It wraps the context of wherever it is applied. Decorators allow you to add new properties/methods to a class or change existing behaviours.Aurelia already utilises decorators quite extensively, but it does not rely on them for everyday use. The existence of decorators allows you to mark your applications up using convenient shorthand.It is important to remember that decorators are a Javascript language construct and not specific to any framework like Aurelia. Given Aurelia leverages ES2015 classes, we can create and use our decorators without changing anything.Decorator TypesWhen we talk about decorators, there are three categories. A class decorator (decorates a class), a class field decorator (inline properties on classes) or a method decorator (a function, either standalone or within a class). You can even create a decorator that combines all three types into one. For the context of this article, we will only be focusing on class decorators.Creating a class decoratorIf you have used Aurelia a little bit, decorators such as; @inject, @singleton and @customElement might be familiar to you already. These are examples of a class decorator.@inject@singleton@customElementThe inject decorator is used in the following manner:The inject decorator is used in the following manner:injectimport {inject} from 'aurelia-framework';
import {Router} from 'aurelia-router';

@inject(Router)
export class MyClass {
    ...
}If you look at the actual implementation for the inject decorator in the Aurelia codebase here you will notice that a class property called inject is being assigned on the class here.injecthereinjecthereBecause decorators in Aurelia are optional and nothing more than convenient shortcuts, you can write the above like this:Because decorators in Aurelia are optional and nothing more than convenient shortcuts, you can write the above like this:import {Router} from 'aurelia-router';

export class MyClass {
    static inject = [Router];
}Let’s create a simple decorator that adds a property to our class called isAwesome a boolean property which will get set on the class itself.isAwesome@isAwesome(true)
export class MyClass {

}

function isAwesome(val) {
    return function(target) {
        target.isAwesome = val;
    }
}In the context of this decorator, target is the class we are using the decorator on and gives us access to the class itself, including the prototype object.targetCreating an Aurelia decoratorThe above decorator is quite simple. It adds a property to our class and does not do anything exciting. Now, we are going to be creating a decorator which shorthands the Aurelia router lifecycle method canActivate.canActivatefunction canActivate(val, redirectTo = '') {
    return function(target) {
        target.prototype.canActivate = () => {

            if (!val) {
                if (redirectTo === '') {
                    window.location.href = '/404';
                } else {
                    window.location.href = redirectTo;
                }
            }

            return val;
        };
    };
}While this is a very rough decorator that would need some improvement before serving a genuine purpose, it showcases how you can modify a class, even adding new methods to the prototype with ease.Now, let’s clean it up and make it more useful and delightful to look at using some nice new Javascript syntax.const canActivate = (resolve) => {
    return (target) => {
        target.prototype.canActivate = () => {
            return resolve();
        };
    };
};Now, we have to pass through a function which allows us to be more flexible in what we can pass through. Still, it could be a lot more useful. What if we wanted to access the current route or get passed in parameters like we can with a class defined method?const canActivate = (resolve) => {
    return (target) => {
        target.prototype.canActivate = (params, routeConfig, navigationInstruction) => {
            return resolve(params, routeConfig, navigationInstruction);
        };
    };
};Okay, now we have a decorator which accepts a callback function and has access to the route parameters, the routeConfig and current navigation instruction.To use our new fancy decorator, we pass through a function which needs to return a value (boolean, redirect):@canActivate((params, routeConfig, navigationInstruction) => {
    return !(routeConfig.auth);
})
export class MyClass {

}Apply this decorator will deny the class getting activated if the route has an auth property of true. We inverse the boolean check when we return it.authJust when you thought we couldn’t improve our decorator anymore, you might have noticed there is a flaw in what we have created. It always assumes that our callback returns something. If it doesn’t, we’ll stop the view-model from executing.Let’s tweak our decorator slightly:Let’s tweak our decorator slightly:const canActivate = (resolve) => {
    return (target) => {
        target.prototype.canActivate = (params, routeConfig, navigationInstruction) => {
            let resolveCall = resolve(params, routeConfig, navigationInstruction);

            return typeof resolveCall !== 'undefined' ? resolveCall : true;
        };
    };
};We still require a callback function, but if the developer doesn’t return anything from their callback function, we’ll return true as to ensure the view-model executes fine.trueIf you wanted to redirect all visitors to a different route if it’s an authenticated route, you could do something like this:import {Redirect} from 'aurelia-router';

@canActivate((params, routeConfig, navigationInstruction) => {
    return (routeConfig.auth) ? new Redirect('/robots') : true;
})
export class MyViewModel {

}You would want to add in some additional logic to check if a user is logged in and then redirect accordingly, our example does not accommodate for this. We have just showcased how easy it is to write a decorator which can leverage existing methods and properties, as well as defining new ones.Creating a decorator that sets the title based on the current routeWhen it comes to setting the title of the page based on the current route, it can be confusing for newcomers. So, using what we have learned, we are going to create a decorator which allows us to set the title from within a view-model in an Aurelia application.We want to create a class decorator which hooks into the activate method and accesses the routeConfig, which contains the current navigation model and a method for setting the title.Our decorator should support passing in a function which returns a string value or the ability to pass in a string directly.const setTitle = (callbackOrString) => {
    return (target) => {
        ((originalMethod) => {
            target.prototype.activate = (params, routeConfig, navigationInstruction) => {
                let newtitle = typeof callbackOrString === 'function' ? callbackOrString(params, routeConfig, navigationInstruction) : callbackOrString;
                
                if (newtitle !== null && newtitle) {
                    routeConfig.navModel.setTitle(newtitle);
                }

                if (originalMethod !== null) {
                    originalMethod(params, routeConfig, navigationInstruction);
                }
            };
        })(target.prototype.activate || null);
    };
};One important thing to note is our callback function (if one is passed in) gets the same arguments that the activate method does. This means we get parameters, the current route configuration and navigation instruction.Using itAssuming you have imported the decorator above or it exists in the same file as your view-model, let’s show how this decorator is used.Callback functionWe pass in a function which returns a string. This scenario might be helpful if you want to determine if the current user is logged in and display their name in the title.@setTitle((params, routeConfig, navigationInstruction) => { return 'My Title'; }) 
export class MyViewModel {

}String valuePassing in a string value will just set the page to this string without the use of callback functions. Useful for pages which don’t need dynamic titles.@setTitle('My Title') 
export class MyViewModel {

}ConclusionIn this article, we didn’t even cover the full scope of what you can do with decorators. We have only just scraped the surface. In future articles, we might explore other types of Javascript decorators. All you need to remember is decorators are functions which “decorate” whatever they’re being used with.As a further exercise, maybe think of other things you can write decorators for, to save you some time.Further reading
A minimal guide to ECMAScript Decorators by Uday Hiwarale
JavaScript decorators by Bojan Gvozderac
JavaScript Decorators: What They Are and When to Use Them by Graham Cox
Exploring ECMAScript Decorators by Addy Osmani
Proposal and examples
Using ES.later Decorators as Mixins by Reg “raganwald” Braithwaite
A minimal guide to ECMAScript Decorators by Uday HiwaraleA minimal guide to ECMAScript DecoratorsJavaScript decorators by Bojan GvozderacJavaScript decoratorsJavaScript Decorators: What They Are and When to Use Them by Graham CoxJavaScript Decorators: What They Are and When to Use ThemExploring ECMAScript Decorators by Addy OsmaniExploring ECMAScript DecoratorsProposal and examplesProposal and examplesUsing ES.later Decorators as Mixins by Reg “raganwald” BraithwaiteUsing ES.later Decorators as Mixins",2451
GitKraken “Could not find a compatible repository” Error Fix,"I recently encountered an error in GitKraken after a bad merge occurred when trying to merge in some changes from the main development branch, whilst I had quite a few local changes that GitKraken usually automatically stashes for me.My problem was I was using Bash Ubuntu on Windows, which has a nasty habit of locking files. The merge and stashing seemed to fail because in the changes I was attempting to merge in, some files were deleted.I tried closing and reopening GitKraken, but it was clear that GitKraken wasn’t going to let me open up that repo again.The fixI realise this is a bit of a nuclear fix, but you’ll need to open up PowerShell to fix this. For me, it was simply a matter of navigating to the project directory and running: git reset --hard however, if you need changes, your repo will be interactable just fine on the command line.git reset --hardAs far as I could see with everything I tried, GitKraken won’t ever fix itself, the command line is the only solution. The above, once I ran it and opened up GitKraken it worked just fine again as nothing had happened.",272
The State of JS Survey Is A Farce: Part Two,"Recently, I published a blog title which I titled, The State of JS Survey Is A Farce in which I expressed criticism that the State of JS survey is highly inaccurate, biased and dangerous.The State of JS Survey Is A FarceI didn’t get a roaring response until a developer who is one of three running the survey Sasha Greif out of nowhere expressed feelings that I was unkind in my blog post in a Tweet that tagged me.my blog posta Tweet that tagged me
  @AbolitionOf calling the State of JS a “farce” was pretty unkind. I hope you get better treatment if you ever launch your own projects

  @AbolitionOf calling the State of JS a “farce” was pretty unkind. I hope you get better treatment if you ever launch your own projects
Admittedly, this Tweet took me by surprise. When I wrote the post, I couldn’t have told you if you asked me who runs the survey. And my intention wasn’t to put down someone else’s work, it was to call out what I saw was bias in a survey growing in popularity.I was critical, but I never resorted to personal attacks or name-calling. It was strictly criticism and valid criticism (or so I thought). As someone who actively participates in open source myself, I know all too well what unconstructive criticism looks like, but this wasn’t one of those times (at least, not intentionally).I responded to Sasha on Twitter with the following:
  Sorry, you took it personally, Sasha. It was never personal and I apologise if you think otherwise. I just have a problem with biased data being used to turn front-end development into a schoolyard popularity contest by declaring winners and losers.

  Sorry, you took it personally, Sasha. It was never personal and I apologise if you think otherwise. I just have a problem with biased data being used to turn front-end development into a schoolyard popularity contest by declaring winners and losers.
I apologised and clarified that my post wasn’t personal, it was a criticism of the survey itself and the fact it was trying to turn front-end development into a popularity contest. Sasha didn’t like my response and blocked me without responding.A few hours later, Sasha unblocked me and sends me a few responses, one of which was the following:
  Well in any case I can’t wait for part two of your post where you actually explain why you think the data is biased

  Well in any case I can’t wait for part two of your post where you actually explain why you think the data is biased
I can be pretty blunt, sometimes brutally honest, but one thing I would never do is personally attack someone and their projects for no reason. I have no reason to pick fights or put down others online, I am not a bully, I am a developer too.My blog post was only criticism of the survey and the data, the data of 20,000 participants, not the people collecting and sorting the data. It’s like blaming the outcome of an election on the people counting the ballot papers.I can understand that maybe Sasha and his team are proud of the survey which explains why I was met with such hostility, but honestly as I said in my previous blog post, it’s a good idea, it just needs better data.I thought Sasha’s comment about a follow-up where I explain why I think the data is biased was fair, so here is the follow-up where I will do my best to explain why the data is biased and how it can be fixed.At a glance: how does data become biased?Before we proceed, I am not a statistics expert nor do I have professional experience in this field. However, just because this isn’t my realm of expertise doesn’t mean I am unqualified, because the bias is as clear as day in this survey.Bias in data can come from a lot of things, but in the case of the State of JS survey, in particular, I believe it comes down to:
Survey questions that have been worded in a particular way to get a specific/inaccurate result result
The data is heavily skewed towards specific countries and excludes a wide variety of demographics, particularly non-English speakers
Data has been grouped into misleading categories
The team behind the survey mostly all use ReactJS and have a vested interest in its success and market position
Survey questions that have been worded in a particular way to get a specific/inaccurate result resultThe data is heavily skewed towards specific countries and excludes a wide variety of demographics, particularly non-English speakersData has been grouped into misleading categoriesThe team behind the survey mostly all use ReactJS and have a vested interest in its success and market positionLanguage BiasLet’s go from the top here. While participants in the survey came from a wide variety of countries, there is some obvious bias here, most of the survey participants came from the USA.What American developers get to use, is widely different than what developers in say India or South America get to use. One of the fastest growing economies in the world China only had 75 participants and India had 521 participants.I worked for a company in 2014 that was building a Netflix type streaming video platform for the South American market. We were constrained by needing to support IE8 and AngularJS 1.3 dropped support for IE8, so we were forced to stay on the version prior. This meant we couldn’t use the latest and greatest, internet speeds were also slower and devices had lower specs.Living in a first-world country, developers are spoiled for choice. Some of us only have to support IE11 minimum now, some of us don’t have to support IE at all. It’s easy to forget the entire world isn’t living in the future or has the latest technology like countries such as the USA is fortunate to have.Region limitations aside, a huge piece of bias in the survey is that it is only available in one language: English. The lack of translation for other languages such as; Mandarin, Spanish, Arabic is a huge barrier for participants considering Mandarin is the worlds most popular language and English is the third.As you will see further down, the exclusion of certain countries (due to only being in English) yields interesting results from underrepresented countries.SolutionTranslate the survey into more languages. The survey excludes a very large portion of the world population by only being available in English.Marketing and Reach: Selection biasThe survey is predominately marketed on Reddit, Twitter, Hacker News and Product Hunt. If you participated in surveys from previous years, you probably got an email. From the outset (because I don’t have the figures), it appears most of the traffic seems to come from social media.There is a huge problem here: countries like China are more strict in terms of what their citizens can see and do on the internet, social media is notoriously locked down in China. In fact, Twitter, Google, and Reddit are all banned in China.This explains why China only had 75 participants, chances are you if you live in China you don’t even know this survey exists. If you don’t speak English, you also probably never heard of the survey or did and could not participate.SolutionDon’t assume that everyone uses social media or can access it. Also, don’t assume that all developers visit Hacker News or other websites. This is a harder problem to crack, but one that maybe partnering with a larger company can solve (such as Google or StackOverflow). The reach and accessibility of the survey needs to be improved.Angular v AngularJS (miscategorised and slanted questioning )Unlike previous years (2016 and 2017), the 2018 survey when it came to questions about Angular really shit the bed (so-to-speak) in how it polled developers.Angular is the newer version (2+) and AngularJS is the older version (< 2). Previous years made the distinction between old Angular and new Angular, however in 2018, the distinction was not made and it essentially invalidated this entire portion of the survey.While the newer version of Angular is the recommended choice for new projects, not everyone has the luxury of throwing out what they have and starting from scratch (because it can be expensive for starters).The survey appears to have erroneously made the assumption that AngularJS has been deprecated and abandoned by Google, when AngularJS 1.7 has a long term support (LTS) period of three years that only began July 1, 2018, and expires in 2021.long term supportA lot of companies are still using AngularJS because their applications work and understand the importance of the wise proverb, “If it ain’t broke, don’t fix it.” comes into play here.This appears to have caused confusion in the survey data. While some can discern the difference between Angular and AngularJS when presented with both options, when presented with just one, it appears they’re both being lumped together and this skews the data.A popular video on YouTube titled State of JavaScript – Real Analysis of Angular, React, and Vue which currently has almost 30,000 views challenging the State of JS results on its treatment of Angular and claims of its death. This video has 1.5k upvotes, but the real story is in the comments section.State of JavaScript – Real Analysis of Angular, React, and VueBut the backlash doesn’t stop there. Angular core team member Olivier Combe took to Twitter to dispute some of the data in the survey as well. In this Tweet exchange with Sasha, Olivier writes:Olivier Combethis Tweet
  Why not make the distinction like the previous years? The complete analysis is worthless because of this. Of course a large number of people wouldn’t use AngularJS again, but that’s not necessarily the case for Angular. If you can’t make a non-biased analysis, don’t do it

  Why not make the distinction like the previous years? The complete analysis is worthless because of this. Of course a large number of people wouldn’t use AngularJS again, but that’s not necessarily the case for Angular. If you can’t make a non-biased analysis, don’t do it
In a further reply, Olivier goes on to say:
  It’s just basic statistics: don’t compare things if you changed the referential between each data point. Being aware of it is even worse you’re admitting that the data is wrong and yet in the final conclusion about frameworks you say that it won’t be a top-end framework ever again

  It’s just basic statistics: don’t compare things if you changed the referential between each data point. Being aware of it is even worse you’re admitting that the data is wrong and yet in the final conclusion about frameworks you say that it won’t be a top-end framework ever again
Once again, we have someone else calling out the bias (albeit a specific part of the survey) and one of the creators of the survey downplaying its significance like it doesn’t matter. This kind of thinking is dangerous and it’s wrong.Continuing on…Continuing on…The most telling sign of exclusion bias is shown in the section, Angular Usage by Country. The happiest Angular users are in the most underrepresented countries.Angular Usage by CountryRomania at 58 users makes up 37.9% of the happy camp of Angular users. Egypt at 17 users makes up 35.4% of happy Angular users. New Zealand at 39 users equates to 26.7% of happy Angular users.Where is this going you ask? Go back to the Participation by Country section and count how many participations from those countries there were in the survey overall.Participation by CountryRomania which had the highest percentage of happy Angular users made up just 0.76% of the survey with a total of 153 participants. This gives us a total of 36.64% of Romanian participants are using Angular and are happy with it.Now Egypt, only 48 users participated in the survey making a tiny 0.24% of the overall participant count. Now, interestingly the second highest count of happy Angular users above at 17 makes 35.41% of happy Angular users.Finally, New Zealand had a total of 146 participants and makes up 0.72% of the survey. New Zealand fairs slightly lower, but out of all participants, 26.71% are happy Angular users.I know large New Zealand companies such as TradeMe.co.nz are big Angular users amongst other New Zealand companies who use Angular. It seems to be used a bit over there, which for a small country is quite impressive.TradeMe.co.nzThere are a lot more underrepresented countries who are using Angular and quite happy with it. I only picked a couple of them, but I recommend you go check out the data yourself.But this seems to somewhat align with the StackOverflow developer survey results for 2018. Even though, StackOverflow targets a more broad audience and has a larger number of participants, we see developers still love working with Angular and are clearly using it (54.6%).StackOverflow developer surveySolutionQuestions about Angular and AngularJS should be separate until after the LTS for AngularJS 1.7 ends in 2021 at the very least. The data is also skewed because the participants who were the happiest with Angular were among the least represented in the survey, increasing representation would help address this.The team behind the surveyFor the record, I think this is worth including, but it’s not the primary factor here for why I believe the data in the survey is heavily biased. All three people behind the State of JS survey work with React and so, naturally, anyone who follows them and what they’re working on probably falls into the React camp.One of the people behind the survey and the one who called me out on Twitter over the previous blog post Sasha Greif actually seems to run an Open source self-described full-stack React+GraphQL framework.One of the other State of JS members is Raphaël Benitte who has a dashboard tool built with Node, React and D3 called Mozaïk as well as another project os DataViz components built using D3 and React.Finally, Michael Rambeau runs a site called bestofjs, which seems dominated heavily by React content. On the left-hand side under the popular tags, React has 189 tagged articles and Vue has 50.SolutionThe very fact that two of the three owners of the State of JS survey are heavily invested into React introduces bias because of their followers most likely leaning into React as well, and the only solution here is to introduce more data into the survey so this eventually this is not an issue anymore.ConclusionMy initial blog post was not personal, and it was not intended to be an attack on Sasha or anyone who runs the State of JS survey.Reiterating what I already said in my previous blog post, there is bias in the data and there is no doubt about that. I invite all criticism and feedback, so if I made a mistake or assumption in this post, please let me know so I can correct it.If the team behind the survey simply acknowledged some of these biases when presenting the results, I would not have published my blog post in the first place.When you take tainted data and you use it to besmirch the name and reputation of frameworks, libraries and tools and tell people to avoid using frameworks like Ember and that Angular is dying, that kind of schoolyard popularity contest bullshit is not needed in an already heavily politicised industry.I think the State of JS survey is great and it’s the first of its kind, but the data needs to be more random and widespread. The language being used also needs to be less about “us vs them” or “avoid using this” and instead just focusing on displaying the data for what it is and let people draw their own conclusions.I hope in 2019 we see a more representative and less exclusionary survey that yields more truthful results than what we were given in 2018. I want to see this survey succeed.",3897
Announcing My New Book: Mastering Aurelia Store,"I have been working with Aurelia Store these past few months and at one point, I decided that it would be a great idea to write a book on how to leverage the Aurelia Store plugin in your Aurelia applications.I have been writing on and off for a while and the book now has enough content in it, that I am ready to announce the book. The book is just over 40% complete and keeping in line with how LeanPub operates, the book is a constant work in progress that will be published often. At present, I am publishing once per day.This means you are buying a work in progress book, but I can tell you the information in it right now will help you if you’re looking to learn how to integrate Aurelia Store into your Aurelia applications as well as work with the underlying RxJS functionality of the store itself.The book includes many working demos and source code already, and future chapters will equally have a tonne of demos and source code. I am the kind of developer who learns faster with example code, and I know many are the same.If you have ever been curious how you go about implementing state management into an Aurelia application, or maybe have tried using existing solutions like Redux or MobX and found them intimidating, Aurelia Store is for you.Buy the book here on LeanPub and pay what you want.here",328
The State of JS Survey Is A Farce,"The State of JS is a survey that has been running for a few years now, which surveys front-end developers and aims to find out what they’re using, what they love, what they’re interested in learning and what they’re not interested in knowing.State of JSThe survey sounds good in theory, it gives you insight into the state of front-end development and the various tools, libraries and frameworks people are using.In practice, the survey is a farce. The 2018 version of the survey saw over 20,000 respondents complete the survey. While 20,000 respondents seem quite low given the number of developers out there who identify as front-end or Javascript developers, the actual issue here is the data, in this case, is biased. When you use biased data, you get a biased result.The survey on the front-end frameworks page makes a really bold and exaggerated claim:
  The front-end remains the key battleground for JavaScript. But now that the dust has cleared, it’s starting to look like only two combatants are left standing…

  The front-end remains the key battleground for JavaScript. But now that the dust has cleared, it’s starting to look like only two combatants are left standing…
Based on the extremely limited dataset it might look like that, but this is an erroneous and highly inaccurate statement to make. While many use React and Vue, this does not mean people have abandoned other choices in favour of them.In the enterprise, choices like Angular and Ember still reign supreme because they’re more verbose and verbosity is generally favoured in the enterprise because it more often than not results in a less error prone result. Angular in terms of enterprise popularity, in particular, is quite high.And I have seen people using Npm stats as a metric for determining popularity, in the enterprise more often than not, packages are not being installed using Npm. The metrics are also skewed here once again because it doesn’t take into account that not everyone who downloads a package through Npm is building something with it (could just be curious).In the conclusion for the front-end libraries section, the survey then doubles-down on the erroneous statement of React and Vue being the only choices:
  The other story of those past couple years is the fall of Angular. While it still ranks very high in terms of raw usage, it has a fairly disappointing 41% satisfaction ratio. So while it probably isn’t going anywhere thanks to its large user base, it’s hard to see how it will ever regain its place atop the front-end throne.

  The other story of those past couple years is the fall of Angular. While it still ranks very high in terms of raw usage, it has a fairly disappointing 41% satisfaction ratio. So while it probably isn’t going anywhere thanks to its large user base, it’s hard to see how it will ever regain its place atop the front-end throne.
Why does this matter?It’s only a silly survey and while a little over 20,000 respondents filled it out, it’s dangerous.The issue here is that managers, CTO’s and CEO’s are going to potentially see this survey and use it as justification to abandon other solid choices in a desperate attempt to be seen as modern and relevant.This isn’t about being angry that React or Vue are increasing in popularity, I think Vue is great and I have worked with it before. I also worked for a company building a Netflix-like product for South American content which used React and that was also great as well.I am sure you have seen what happens when you send developers to conferences, they come back excited and giddy wanting to change the world and use all of these new libraries they saw at the conference, this survey is the same, it’s hype fodder.The crux of the matter here is the State of JS survey is perpetuating false claims based on seemingly biased information and in the process turning front-end development into a schoolyard popularity contest by declaring winners and losers.The survey is a good idea, but it needs more dataStackOverflow has an annual developer survey they do and their 2018 survey got around 100,000 respondents, but the downside is they cover a broader spectrum that isn’t just front-end development like the State of JS does.If the State of JS wants more accurate results, they need a tonne more data. And not just more data, but they need to work to eliminate the bias from their survey. And to remove the bias, it’s clear if they can get more people to complete the survey it might help. But even so, the people who run the survey seem to be users of React, so there would always be an element of bias.Going back to the StackOverflow survey for 2018, the section for frameworks libraries and tools is rather interesting to look at. Even though it includes non-front-end choices, Node.js is the top result followed by Angular and then React.frameworks libraries and toolsOut of 100,000 respondents, 36.9% are using Angular, 27.8% are using React. With the following footnote:
  Node.js and AngularJS continue to be the most commonly used technologies in this category, with React and .Net Core also important to many developers.

  Node.js and AngularJS continue to be the most commonly used technologies in this category, with React and .Net Core also important to many developers.
Then under the Most loved, dreaded and wanted frameworks, libraries and tools section we see angular get a 54.6% score. Worth noting that React is second here with 69.4% saying they love it.Most loved, dreaded and wanted frameworks, libraries and toolsAs you can see, the two surveys produce two different results. One is focused on a specific area of development, the other is focused more broadly. I would love to see StackOverflow run a more targeted survey to see what the results are.The State of JS survey clearly has a marketing problem and hopefully, over time the number of people who participate goes up. It’s a great idea, but at present feels like it is only being marketed to React and Vue developers, creating this confirmation bubble that React and Vue are the only choices (they’re not).I found the gender disparity in the results (over 90% male) to be quite concerning. It really highlights that we need to do more to get women into front-end development, but it also highlights once again that the limited number of people this survey is being marketed to is a huge problem in terms of skewing the data.One thing that is clear from this survey, is developers are choosing to use frameworks, libraries and tools based on their popularity and not whether or not their choices actually align with the requirements of the business or customers they’re developing for.The front-end space in the last four years has honestly turned to shit, with people flinging mud and swinging their proverbial digital dicks around claiming that React is the king and that Vue is the new messiah.There is an entire ecosystem of great frameworks and libraries to choose from, and in 2018, very little difference between them (except how you build apps). While people buy into false claims like the virtual DOM being faster than the real DOM, really regardless of what you choose; Angular, Aurelia, Ember, React or Vue, you’re going to be making a great choice for building modern web applications.It is an exciting time to be a developer if you like picking sides and criticising people for the choices they make (especially if they’re less popular options).The TL;DR here is to take the results of this survey with a grain of salt. It is not indicative of the industry whatsoever and is highly inaccurate, it’s interesting to see what 20,000 front-end developers think, but that’s about it.Make decisions based on the results of this survey at your own peril.",1933
The Ultimate Programmer Super Stack,"I generally avoid promoting things on my blog, but this month I am a part of the Infostack Ultimate Programmer Super Stack, my Aurelia book is a part of this fantastic bundle.For $47.95 you get my Aurelia For Real World Web Applications book, as well as a few other programming books and courses. A whole wide variety of topics are covered, and if you’re like me, you lap these kinds of bundles up because you’re always hungry to learn something new.The Ultimate Programmer Super Stack is a hand-curated collection of 25+ premium courses, bestselling ebooks, and bonus resources that will help new programmers as well as experienced ones.One of my favourite additions to this bundle is the book Build APIs You Won’t Hate by Phil Sturgeon, which is a book I highly recommend every developer reads before they attempt to build an API. Because let’s be honest, API’s are complicated things to build and design right.This is a time-limited bundle, so don’t sit on the fence for too long.Grab the bundle here.here",252
"How To Build Complex, Large-scale Aurelia Apps With Aurelia Store","If you’re new to state management or you’re familiar with it and not using Aurelia Store already (you should), today we are going to be looking at how you can integrate Aurelia Store into your Aurelia applications and make the life of your development team and yourself a lot less stressful.A lot of state management articles will wheel out the TV and VCR player on the tray table with wheels and default to the cliche shopping cart or todo application example to show you how to work with state management.Look, they’re great simple examples showing you how to work with state management, but they are quite far removed from a large-scale application which could be comprised of hundreds of actions and troves of data going to and from the server.Unless you’re building a todo application or basic shopping cart, these examples are not going to answer all of your questions nor reflect a real application with a real team working on it.The truth of the matter is, real-world applications (especially for large products) can get big. I worked on an Aurelia application that had upwards of 40 components, not-to-mention value converters, attribute. a stack load of modals, routed sections and an impressive amount of singleton classes.What problems does state management solve, and do I even need it?State management isn’t something you always need, but if you’re working with a growing collection of data (from an API or user-entered) and you’re sharing it across your application, state management is unavoidable or you’ll end up with singleton soup.For a long time using Aurelia, I actually avoided using state management, even though other solutions predate Aurelia Store such as MobX and Redux. The problem that most other state management solutions share is they require serious “buy in” to use them, they have a cost.Fortunately, Aurelia Store aligns well with the Aurelia mantra of “just write code” so it doesn’t strictly enforce a specific way of working other than the basics like not directly mutating state and registering actions.The biggest problem state management addresses in Javascript is: assign by reference. Every variable and value you reference is passed by reference, meaning it can be overwritten or accessed from anywhere in your application without any kind of visible trail which can be disastrous.When you opt-in to state management, you’re explicitly choosing to update and read data from a centralised store via an abstraction. Good state management prevents you from accidentally overriding a value in the store without leaving a trail (still possible, but more difficult).One example of where state management shines is Aurelia’s binding system. Picture this scenario, you have a form with 4 inputs and those inputs are using value.bind referencing properties on some object or class. When those values update, the object values also update, form input value bindings are two-way by default.value.bindtwo-wayNow, imagine that object is being shared across your app and you have an array of users. The form allows you to update a user and everywhere sees the change immediately (pass by reference). Now imagine an invalid value is entered or something breaks, how do you revert it? You can’t (unless you have a copy of the object).If you want to know when the value was updated, how do you do that? In Aurelia, you’ll need to use the propertyObserver API or simplify your app and use @observable to observe changes to values and act accordingly. It results in a lot of additional and potentially performance draining code that becomes a maintenance nightmare over time.propertyObserver@observableAurelia Store features
A reactive subscription observation system, so when changes are made to the store you can subscribe to the state itself and get the updated changes anywhere in your application. Think when a user logs in and out when a collection of products is loaded.
State is not directly mutated, actions are used to update the state and leave a history you can inspect using Redux Dev Tools as well as undo or replay.
Supports middleware. Allows you to integrate code that runs at various points of the state lifecycle, allowing you to filter values or check permissions.
Aurelia Store gives you a clearer view of your data structures and separates the data layer from the view and view-models (easier to test, easier to refactor).
Allows you to have safe and fast cross-component communication, meaning you no longer need to abuse the Event Aggregator or Router to pass data around.
Easy to debug using the Redux Dev Tools which the plugin completely supports. A browser plugin for visually seeing actions and state as it happens, as well as call actions.
Supports caching out-of-the-box, allows you to cache your data in localStorage and create offline/poor internet connection capable applications.
A reactive subscription observation system, so when changes are made to the store you can subscribe to the state itself and get the updated changes anywhere in your application. Think when a user logs in and out when a collection of products is loaded.State is not directly mutated, actions are used to update the state and leave a history you can inspect using Redux Dev Tools as well as undo or replay.Supports middleware. Allows you to integrate code that runs at various points of the state lifecycle, allowing you to filter values or check permissions.Aurelia Store gives you a clearer view of your data structures and separates the data layer from the view and view-models (easier to test, easier to refactor).Allows you to have safe and fast cross-component communication, meaning you no longer need to abuse the Event Aggregator or Router to pass data around.Easy to debug using the Redux Dev Tools which the plugin completely supports. A browser plugin for visually seeing actions and state as it happens, as well as call actions.Supports caching out-of-the-box, allows you to cache your data in localStorage and create offline/poor internet connection capable applications.Structuring an Aurelia app for Aurelia StoreUnlike existing state management solutions, Aurelia Store doesn’t require you to structure your application in a specific way. This is both good and bad because it means you can choose a structure and go down a particular path only to find you chose the wrong architect.Below is the approach I take in my Aurelia applications using a store, I will create a folder called store in my src directory.storesrcPlease keep in mind that I use TypeScript, so the below examples and explanations will be written in TypeScript and vanilla Javascript differs slightly.Please keep in mind that I use TypeScript, so the below examples and explanations will be written in TypeScript and vanilla Javascript differs slightly.src
├── app.html
├── app.ts
├── ...
├── store
│   ├── actions
│   │   ├── auth.ts
│   │   ├── user.ts
│   ├── store.ts
│   ├── state.ts
└── ...The actions folder contains files which contain functions (actions) that are exported and can be imported where needed. The auth.ts example might have a login and logout method setting and unsetting the currently authentication user.actionsauth.tsloginlogoutThe user.ts might have actions related to the user like getting content specific to their account, getting an avatar or updating their profile.user.tsThe store.ts file is interesting because I actually import the Store itself and get a reference to it via the Aurelia Dependency Injection container. Here is what the inside of my store.ts in new Aurelia applications I build looks like these days:store.tsstore.tsimport { Container } from 'aurelia-framework';
import { Store } from 'aurelia-store';
import { State } from './state';

const store: Store<State> = Container.instance.get(Store);

export default store;The reason I do this is so I do not have to follow the official examples where the Store is injected into every view-model you want to register actions from within. It was tedious and I wanted an easier way. It also allows me to actually register actions from within the action files themselves instead of view-models, which makes more sense to me.If you want to manually inject the store into your view-models when needed, the official documentation and examples encourage this approach. This is just another option.The state.ts file has an object which is the application state structure itself. A basic application using the above example of a structure might look like this:state.tsexport interface State {
    auth: {
        loggedIn: boolean;
        token: string;
        refreshToken: string;
    };

    user: {
        id: string;
        username: string;
        avatar: string;
        profileFields: any;
    };
}

export const initialState: State = {
    auth: {
        loggedIn: false,
        token: '',
        refreshToken: ''
    },
    user: {
        id: '',
        username: '',
        avatar: '',
        profileFields: {}
    }
};Configuring the pluginWe now have the state configured, so you’ll want to pass it as an argument to the aurelia-store plugin registration inside of your main.ts file inside of the exported configure method.aurelia-storemain.tsconfigureimport { Aurelia } from 'aurelia-framework'
import { initialState } from './store/state'; // exported initialState object

export function configure(aurelia: Aurelia) {
  aurelia.use
    .standardConfiguration()
    .feature('resources');

  ...

  aurelia.use.plugin('aurelia-store', { initialState });

  aurelia.start().then(() => aurelia.setRoot());
}And now we get into the actions themselves…And now we get into the actions themselves…We glimpsed over actions earlier because to write them you really need your state in place first. Actions are what mutate the state values and you can actually do quite a lot with them. On a basic level, the first argument is the state object itself and you can pass parameters (which are subsequent arguments).Actions can be sync or async. This means you can do things like make API requests from within actions and make your application asynchronously call and wait for the returned promise to resolve or reject. Let’s look at our fictitious auth.ts actions file.auth.tsimport store from 'store/store';
import { State } from 'store/state';

export function login(state: State, token: string, refreshToken: string) {
    const newState = Object.assign({}, state);

    newState.auth.loggedIn = true;
    newState.auth.token = token;
    newState.auth.refreshToken = refreshToken;

    return newState;
}

export function logout(state: State) {
    const newState = Object.assign({}, state);

    newState.auth.loggedIn = false;
    newState.auth.token = '';
    newState.auth.refreshToken = '';

    return newState;
}

store.registerAction('login', login);
store.registerAction('logout', logout);If you have already read the Aurelia Store documentation (and you should), then you would know that actions should NEVER mutate the passed in state property directly and should always be shallow cloned using something like Object.assign or the spread operator { ...state } to make a copy of it.Object.assign{ ...state }You will also notice our login action takes the state like all actions do, but has two other arguments for the token and refreshToken. When the action is dispatched, these can be passed which I’ll show you next.loginActions should always return a copy of the state, which then does the work of update the state object itself. Never just assigning new properties to state, because otherwise, you encounter the pass by reference issue we talked about earlier which is bad as there will be no trail of your updates (hard for testing and debugging).Similarly, the users.ts actions file might have an async function in it…Similarly, the users.ts actions file might have an async function in it…users.tsimport store from 'store/store';
import { State } from 'store/state';

// A fictitious API service you might have
import { Api } from 'services/api';

// Use Aurelia DI to get the api because we are not inside of a view-model
const api: Api = Container.instance.get(Api);

export async function getAvatar(state: State) {
    const newState = Object.assign({}, state);

    try {
        // An API function that accepts the current user ID
        // notice we reference the user object to get the ID?
        const avatarImage = await api.fetchUserAvatar(newState.user.id);

        // Store the avatar
        newState.user.avatar = avatarImage;
    } catch (e) {
        console.error(e);
    }

    return newState;
}

store.registerAction('getAvatar', getAvatar);Triggering actionsWe now have our store setup and configured. Now picture you have a login page with basic username and email fields, you hit a login button and it goes off to the server.import { autoinject } from 'aurelia-dependency-injection';
import { dispatchify, Store } from 'aurelia-store';

import { State } from 'store/state';

import { login } from 'store/actions/auth';

@autoinject()
export class Login {
    public state: State;
    private subscription: Subscription;

    constructor(private store: Store<State>) { }

    bind() {
        this.subscription = this.store.state.subscribe((state) => this.state = state);
    }

    unbind() {
        this.subscription.unsubscribe();
    }

    doLogin() {
        api.login(username, password)
            .then(result => result.json())
            .then(auth => {
                dispatchify(login)(auth.token, auth.refreshToken);
            });
    }
}I am using the dispatchify method so I don’t have to manually inject the store into the view-model itself and dispatch the function that way. Worth acknowledging is when you dispatch a function, you pass the function itself and not the name.dispatchifyThe dispatchify method returns a function which is the action itself, which allows you to pass values to its arguments. In our case, we are mocking a login method that calls an API and then if successful, will store the token and refreshToken values.dispatchifytokenrefreshTokenAsync/await actionsWe showcased a pretty basic example of dispatching the login action to set the token and refreshToken in the state. Now, let’s showcase how powerful asynchronous actions are, by calling our get user avatar action.tokenrefreshTokenI have been asked this on a few occasions by people interested in the Aurelia Store plugin: is it okay to make API requests and request data from within actions? Absolutely. The fact actions support returning promises makes them the ideal candidate for encapsulating everything related to your data.You will notice the code looks very much the same as the example above.import { autoinject } from 'aurelia-dependency-injection';
import { dispatchify, Store } from 'aurelia-store';

import { State } from 'store/state';

import { login } from 'store/actions/user';

@autoinject()
export class User {
    public state: State;
    private subscription: Subscription;

    constructor(private store: Store<State>) { }

    bind() {
        this.subscription = this.store.state.subscribe((state) => this.state = state);
    }

    unbind() {
        this.subscription.unsubscribe();
    }

    async activate() {
        await dispatchify(getAvatar)();

        // If successful, the avatar will be available on this.state.user.avatar
        // which can then be referenced in the view
    }
}Using getters and computedFrom for sanitycomputedFromEventually, you will realise that in your views your referenced state objects and values can be pretty daunting to look at as well as type. You can use computed getters to create shortcuts to values in your store.Using the above login example, we’ll create a getter to simplify accessing the auth property in our state.import { autoinject } from 'aurelia-dependency-injection';
import { computedFrom } from 'aurelia-framework'; 
import { dispatchify, Store } from 'aurelia-store';

import { State } from 'store/state';

import { login } from 'store/actions/auth';

@autoinject()
export class Login {
    public state: State;
    private subscription: Subscription;

    constructor(private store: Store<State>) { }

    bind() {
        this.subscription = this.store.state.subscribe((state) => this.state = state);
    }

    unbind() {
        this.subscription.unsubscribe();
    }

        @computedFrom('state.auth') {
            // Allows you to use auth.loggedIn and so on in your views
            return this.state.auth;
        }

    doLogin() {
        api.login(username, password)
            .then(result => result.json())
            .then(auth => {
                dispatchify(login)(auth.token, auth.refreshToken);
            });
    }
}My approach differs from the official documentationIt is worth pointing out that my approach differs greatly from the official Aurelia Store documentation and examples in the GitHub repository. I am not saying that this is the way you should do it, I am just showing you how I do it and what works for me.The approach you see above is the same approach I take for all new Aurelia projects using the store. It allows me to neatly organise my actions into bite-sized modules and it just feels very clean.This is only the startWe didn’t get too in-depth into how you can use the Aurelia Store plugin, because the official documentation does a great job of that already, which you can read here and if you haven’t, I highly recommend you should.hereIn a separate future article, I will show you how to use Aurelia Store with server-side rendering and implement features such as default state and offline capability.",4404
Open Sourcing BuiltWithAurelia.com,"Over two years ago (wow, has it really been that long?) I launched builtwithaurelia a showcase of Aurelia applications and community created offerings for the Aurelia Javascript framework.builtwithaureliaWhen I launched I didn’t open source it because I didn’t want the pressure of having to put out something clean and perfect. I hacked this thing together quite quickly and over time slowly grew it to what it is now (sort of complete).The Aurelia team are hard at work on vNext of Aurelia, soon to be the latest and greatest version of the Aurelia framework. As part of Aurelia 2.0, the team wanted real applications they could test against to ensure no breaking changes would be made.I made the suggestion in the core team chat that I would open source Built With Aurelia and sure enough, two weeks later I did just that. The cool thing about this app is not the fact it’ll help prevent breaking changes and bug regressions, it’s another learning point for anyone wanting to build Aurelia applications.The application is comprised of:The application is comprised of:
Aurelia Store for state management
Aurelia Validation for validation
Aurelia server-side rendering (from within a Firebase cloud function)
Hosted on Firebase and uses Real-time database for data
Aurelia Store for state managementAurelia Validation for validationAurelia server-side rendering (from within a Firebase cloud function)Hosted on Firebase and uses Real-time database for dataAt present, there are no tests and the codebase is still messy, but over the next few weeks now that it is public I will be overhauling parts of the app (some two years old) and making it nicer to look at. I will also be writing tests for this as well.I am glad I can share it with everyone and it’s MIT licenced, so go to town if you want to use it as a starting point for any Aurelia applications you want to build.The code is on Github here: https://github.com/Vheissu/builtwithaurelia – please feel free to submit pull requests.here",498
My Experience Building A Simple Widget Using CoinMesh,"Spoiler alert: CoinMesh is going to be a gamechanger.Spoiler alertWhen it comes to building decentralised blockchain based applications, admittedly it can get complicated fast. What libraries do you use, how do you safely makes calls to a wallet node and interact with the blockchain itself? This is something that Coinmesh aims to simplify.Instead of having to worry about what libraries, adapters and tools to use, you get a graphical user experience that allows you to create a starting base for blockchain based applications via a simple UI.Because CoinMesh is still in alpha, it is advisable (at the time of writing this) to use the sample-generated-project which is setup to work with Bitcoin and Litecoin out-of-the-box.sample-generated-projectMy task was a simple one, a widget that has an input which allows you to enter a Litecoin wallet address and you get the balance in Litecoin back. Oddly enough, the Litecoin wallet doesn’t have a simple straightforward method to get the balance of an address.This meant I had to write a couple of methods at the adapter level to allow me to import an address and then call the getReceivedbyAddress RPC method on the wallet itself. The way Bitcoin and Litecoin work is that you need to import the address first and then call a method to get its balance, this is known as a watch-only address.getReceivedbyAddressFortunately, the way CoinMesh is setup, everything is intuitive so I wasn’t scratching my head where things live. I was not only able to add in new adapter methods easily for interfacing with the wallet node itself via RPC calls, but create a front-end that can easily communicate with the backend with minimal work.And if you’re wondering specifically what code I wrote, the backend Litecoin wallet adapter for importing an address and getting the balance is here in 30 lines of code.hereThe front-end code is even way more simple as you can see, the lookup and display code is here in this component.is hereThe majority of the work is done inside of the doLookup method which is below:doLookupdoLookup() {
    this.wallet.post('addresses/getreceivedbyaddress', {
        address: this.address
    })
    .then(res => {
        this.result = JSON.parse(res.response);
        this.address = '';
    })
    .catch(e => {
        console.error(e);
    })
}
We call the wallet service, specifically the address method getreceivedbyaddress which accepts an accept from an input and then the JSON response which contains the balance is rendered in the HTML file. It’s super basic and it was a breeze to write.getreceivedbyaddressIf you don’t believe me, look how simple the code is in the entire Github repository here. There are so many developers out there who are put off building blockchain/cryptocurrency applications, but CoinMesh legitimately makes it easy for any decent developer to build an application.hereIf you’re wondering where the best place to start with CoinMesh is, this official blog post about the alpha does a great job explaining how to get up and running with the same sample application that I used.thisConclusionIt is projects like CoinMesh that are going to help cryptocurrency and blockchain technology become more widely adopted in different industries.CoinMesh solves one of the major problems with technologies like Bitcoin and Litecoin, people don’t know where to begin or what the best practices for development are.It is easy to forget this is an emerging industry and the rules are still being written.",874
Aurelia Tips & Tricks,"If you are new to Aurelia or perhaps already building something with it, here are some tips and tricks I have learned over the last two years which have helped me write well-performing Aurelia applications.Keep in mind that Aurelia is like any other framework or library and that it is very easy to write poor performing applications if you’re not careful. Never assume that a framework will stop you from writing bad code.If vs Show BindingsIn Aurelia there are two attributes: if and show which are similar to one another, but have some fundamental differences. The if binding will add or remove elements from the DOM, whereas the show binding just applies a conditional display:none to whatever element the attribute is used on.ifshowifshowdisplay:noneUnderstandably, for some it is confusing. Do you use if or do you use show? My rule of thumb is defined by how often I am going to show or hide something in my application.ifshowAm I going to show or hide an element in the page regularly or am I conditionally going to show or hide something sporadically or only just once?The upside of the if binding is when the bound value is false, the elements in the DOM will be removed (freeing up DOM space and bindings will be removed). The downside being Aurelia needs to set up a new controller instance and bindings (which isn’t that big of a deal).ifI am a huge fan of a light DOM, so I use if.bind extensively because a large DOM is a lot slower than the time it takes for Aurelia to setup controller instances and bindings. Both have a purpose, it is important to use what works for you.if.bindI do find if.bind works well for me in most cases, no point having elements around if they’re not being used (in my opinion).if.bindAvoid dirty-checking at all costsIf you’re not sure what dirty-checking is, essentially it’s a timer with a 120-millisecond timeout value that repeatedly loops and checks for changes. You definitely do not want dirty-checking in your application (unless you absolutely need it for some edge case).If you define a getter in your view-model which accesses one or more values in your class and you do not supply the computedFrom decorator, it will be dirty-checked.computedFromAn example of dirty-checking can be seen below:export class MyClass {
    get fullName() {
        return `${this.firstName} ${this.lastName}`;
    }
}

<template>
    <h1>${fullName}</h1>
</template>Internally, Aurelia is going to dirty-check this using a setInterval to constantly poll if the value changes, this is bad. Fortunately, using computedFrom we can make this an optimised getter which only reacts when one or more dependencies change.setIntervalcomputedFromThe below example does not use a timer to poll for changes, it makes the getter reactive.import {computedFrom} from 'aurelia-framework';

export class MyClass {
    @computedFrom('firstName', 'lastName')
    get fullName() {
        return `${this.firstName} ${this.lastName}`;
    }
}

<template>
    <h1>${fullName}</h1>
</template>A note on computedFrom: the this part is implied, so when supply one or more computed values to computedFrom you are automatically referencing the current class and do not need to supply this.A note on computedFrom:thiscomputedFromthisA note on computed arraysthe computedFrom decorator is great for telling Aurelia about your getter dependencies, but it doesn’t work for all values. One such situation is arrays, an array cannot be directly supplied as a getter computed value because it won’t update.computedFromIf you’re looking to force a getter to update when an array changes, computed based on its length. When the size of the array changes (items are added or removed) the getter will fire.import {computedFrom} from 'aurelia-framework';

export class MyClass {
    @computedFrom('arr.length')
    get firstValue() {
        return `${this.arr[0]}`;
    }
}

<template>
    <h1>${firstValue}</h1>
</template>Think big by thinking smallIf you come from an MVC background, then you’ve heard the expression, “fat model, skinny controller” and other countless expressions around MVC architecture which have sort-of been carried over into the front-end development jungle.I prefer fat nothing in my Aurelia applications (or web applications full-stop). If your view-models and/or views are massive, then you have a code architecture problem which is easily solved by reusable components.A large view-model or view is eventually going to be your undoing (besides the annoying Git conflicts you will encounter working in a team on large files). Large files which do too much are counterproductive and unnecessarily complicated.If you want to ensure your application is easy to build upon and make it easier for the browser, build tools and test runners to work with your code: break things up into reusable components.I highly recommend approaching the architecture of your Aurelia applications using the Single responsibility principle.Single responsibility principleUse view-cacheThis isn’t documented very well, but Aurelia provides a view-cache which can allow you to cache heavy views. This means if you have quite a complex view and you navigate away, but come back to it later Aurelia will use the view-cache and update all of the appropriate bindings.The benefits are obvious: the whole view doesn’t need to be recreated when you navigate back to it. Just throw view-cache onto the main opening <template view-cache=""*""> tag, which also accepts a cache size value.view-cache<template view-cache=""*"">If you supply an asterisk * to view-cache it will cache all instances or if you supply a numeric value, cache the number supplied.*view-cacheUse compose when nothing else fitsAurelia provides a <compose> custom element which allows you to dynamically compose from within your application. You can dynamically compose view-models, views and pass in data as well.<compose>As great as <compose> is, I try and avoid it wherever I possibly can. It’s not that I think <compose> is bad, it’s the fact that <compose> is an abstraction and where possibly, I try and avoid abstractions in my applications.<compose><compose><compose>I have experienced instances where <compose> has made debugging more difficult than straight-up view-models and components.<compose>Assess your needs and if <compose> is the only option, then by all means use it.<compose>Use Binding BehaviorsOut-of-the-box, Aurelia ships with some great binding behaviours which allow you to make bindings in your applications more efficient and performant.A binding behaviour in your view templates is denoted by an ampersand “&” and much like value converters, they can be chained and have parameters which accept values.throttleA rate-limiting throttling behaviour which allows you to throttle the rate in which the view-model is notified of a change in a two-way binding situation. By default the throttle value is 200ms.debounceVery similar to the throttle binding behaviour, the debounce behaviour will prevent a binding from being updated until the debounce interval completes after no changes have been made.throttledebounceupdateTriggerAllows you to specify which input events signal to Aurelia’s binding system that a change has been made. By default, the events that trigger a change are change and input.changeinputsignalAllows you to tell Aurelia’s binding system to refresh a binding. It works like an event listener on a binding and when you fire an update signal, it refreshes the binding which works similarly to how a first-page load does.oneTimeUse the oneTime binding behaviour as much as possible. It allows you to tell Aurelia’s binding system to display a binding value and then forget it. A one-time binding will not update if the value changes, which can lead to great application performance.oneTimeIf you would like to see examples of binding behaviours and how to work with them, Aurelia core team member has a great write-up on working with binding behaviours here which I recommend you also read.hereBatch DOM changes using the Task QueueThe Aurelia Task Queue is a powerful module that allows you to sort of hook into the browsers event loop which allows you to perform DOM actions in an efficient way.While slow Javascript code can definitely affect performance, it is a known fact that the DOM is the bottleneck of any web application, which is why libraries like React and Vue leverage a virtual DOM. Working with the DOM inefficiently can result in poor browser performance.By queuing up a microtask, we can batch changes in the page resulting in less reflow and repaints.import {TaskQueue} from 'aurelia-task-queue'; 

export class MyViewModel {
    static inject = [TaskQueue];

    constructor(taskQueue) {
        this.taskQueue = taskQueue;
    }

    attached() {
        this.makeSomeChanges();
    }

    makeSomeChanges() {
        this.taskQueue.queueMicroTask(() => {
            const header = document.getElementById('header');

            // Dynamically set the width and height of an element with ID ""header""
            // We are batching DOM changes, so we only get on repaint/reflow
            header.style.height = '250px';
            header.style.width = '500px';
        });
    }
}Be smart about event handlingIn Aurelia, there are two ways to create event listeners. You can create an event listener the Javascript way using addEventListener or you can also bind to events from within your views with one such example being click.delegate=""myCallback()"".addEventListenerclick.delegate=""myCallback()""Where possible, you should use Aurelia’s templating for binding to events. The biggest advantage being Aurelia cleans up after itself and removes events when they’re no longer being used.This is not to say that addEventListener is completely redundant, but for eventing around elements in your page, you can and should use Aurelia’s provided way of working with native events.addEventListenerYou’ll have the piece of mind that events are properly being unbound when views are being destroyed, it’s like programming languages that automatically manage memory for you like PHP.Don’t abuse the Event AggregatorAurelia comes with a great Event Aggregator module which allows you to publish and subscribe to events from within your application. The routing module also uses the Event Aggregator, firing off various events at different stages of the routing process.As great as the Event Aggregator is, I have seen it abused quite a lot. I have done a bit of Aurelia consulting and in most instances, the Event Aggregator is being used for things it shouldn’t be used for.The biggest issue you will encounter with the Event Aggregator is that it is hard to debug. This rings true for most custom eventing, not just Aurelia’s module.So, when are you supposed to use the Event Aggregator then?Do’s
Use the Event Aggregator to notify parts of your application about specific actions; user added, recorded deleted, a user logged in, a user logged out, etc.
Use the Event Aggregator to react to specific actions; a modal opening or closing, a user is deleted or if the user logs out clearing out a token in localStorage, etc.
Use the Event Aggregator to notify parts of your application about specific actions; user added, recorded deleted, a user logged in, a user logged out, etc.Use the Event Aggregator to react to specific actions; a modal opening or closing, a user is deleted or if the user logs out clearing out a token in localStorage, etc.Do nots
Don’t use the Event Aggregator for passing data between routes
Don’t use the Event Aggregator for passing data between components
Never treat the Event Aggregator as a crucial dependency (creating components that rely on specific events and values)
Don’t use the Event Aggregator for passing data between routesDon’t use the Event Aggregator for passing data between componentsNever treat the Event Aggregator as a crucial dependency (creating components that rely on specific events and values)Passing data between routesA common scenario I have witnessed in the community is wanting to route from Route A to route B and pass some data to the route you are navigating too.I’ve seen developers pass data using the Event Aggregator (see above, don’t do that), use localStorage (also, don’t do that) and the even crazier: storing it on the route object itself (seriously, don’t do that).localStorageThe way that I always recommend passing data between routes and components is by using a singleton class. If you come from an Angular background, then you might know this as a service class.Assuming you have two routes: Route A and Route B, and in Route A the user can enter their name and then be navigated to RouteB which will display their name.I would achieve this in the following way:    import {AppService} from './app-service';

    export class RouteA {
        static inject = [AppService];

        constructor(appService) {
            this.appService = appService;
        }
    }

    <template>
        <h1>Welcome: Route A</h1>
        <label>Please enter your name: <input type=""text"" value.bind=""appService.name""></label>
    </template>

    import {AppService} from './app-service';

    export class RouteB {
        static inject = [AppService];

        constructor(appService) {
            this.appService = appService;
        }
    }

    <template>
        <h1>Hello, ${appService.name}</h1>
        Welcome to RouteB. The name you provided on the previous page is displayed above.
    </template>

    // Assuming this file is saved as app-service.js alongside the other route view-models
    export class AppService {
        constructor() {
            this.name = '';
        }
    }Or better still, use a state management solution if your application state is complex and requires a much more manageable solution. The Aurelia Store is my favourite and recommended state management solution for Aurelia applications.Aurelia Store",3484
"Why I Prefer Aurelia Over Angular, React & Vue","Let’s acknowledge the elephant in the room, if you’re a reader of my blog then you would know that I am a huge Aurelia fan and user. I’ve been working with Aurelia since February 2015, so 3.5 years now (wow).I am on the Aurelia core team, I have contributed to the community through plugins/skeletons, answering questions on StackOverflow and I regularly do Aurelia consulting/freelance work (hire me). There is a good chance you’re reading this blog post because you’re familiar with my many previous Aurelia blog posts over the years.Now we’ve got that stuff out of the way, I want to talk about why I continue to use Aurelia in the face of other choices.Do you only use Aurelia?No. It might surprise readers of my blog to know that I don’t always exclusively use Aurelia. Before Aurelia, I worked with ReactJS as well as Angular 1.x.Since 2015 I have worked with React, Ember, Angular and Vue.js. In fact, about a year ago I launched a side project idea I had which is built using Vue + VueX called TidyFork. It allows you to clean up old Github forks and starred repositories.TidyForkA few months ago I helped a client migrate an Angular project over to Aurelia and it was an absolute breeze. I also participated in an internal effort to rewrite a React dashboard over to Aurelia.In spite of not intentionally shunning other choices, I always come back to Aurelia.ConventionsThis is one of my favourite things about Aurelia and honestly, it surprises me that no other framework or library has really leveraged the convention over configuration approach. Specifically, in Angular, the use of decorators are promoted heavily and it results in overly verbose looking view-models.Maybe it’s more of a personal preference thing, but I hate typing. I am always trying to find ways to macro my workflow and automate it. I really hate typing, especially the same thing over and over again.If I want to create a custom element or attribute in Aurelia, all I do is name my view-model accordingly: MySpecialCustomElement or MySpecialCustomAttribute and I know Aurelia will look for the CustomElement and CustomAttribute parts to determine what kind of component this is.MySpecialCustomElementMySpecialCustomAttributeCustomElementCustomAttributeI like the fact that I do not have to tell Aurelia the name of my accompanying view .html file and that it will just look for a file of the same name with the .html extension, but provide me with the option with telling Aurelia the name (if conventions can’t find it)..html.htmlAnd the cool thing about conventions is that they can be changed. You can have the best of both worlds, let Aurelia make assumptions or tell Aurelia how you want it to work through decorators or class properties.Reactive Binding (vDOM not included)I’ve worked with React and Vue, I am quite familiar with the concept of a Virtual DOM and let me tell you, the Virtual DOM really irks me when people see it as a silver bullet for performance.Aurelia doesn’t have a Virtual DOM and you probably wouldn’t even notice a difference if you compared two identical apps side-by-side (one built with React and the other Aurelia) because the reactive binding system offers many of the same benefits.Look, there are definitely instances where Virtual DOM can be fast and it’s not slow. However, in most cases a reactive approach to binding is practically the same, it’s working with the DOM at isolated targeted points.Be smart, don’t buy into the hype that Virtual DOM is the one true path to performance world.Small Learning CurveAll frameworks and libraries I’ve used have boasted about their small learning curves when in reality it’s subjective and dependent on the developers skill-level and ability to learn new things.I have first-hand experience with upskilling front-end and full-stack developers with ranging backgrounds. I have upskilled full-stack developers with no framework/library experience and developers with experience of frameworks like Angular or libraries like jQuery.Every developer I have upskilled has grasped the basics of Aurelia within a couple of days, and that is being somewhat hands-off. The hardest part developers face is how to structure their applications and what to name things (which isn’t an Aurelia problem).This is where the benefit of conventions come into play. If the framework makes decisions for you, there is less to learn upfront to start being productive.You don’t need to hire “Aurelia developers”This is one thing that really irks me about other frameworks and libraries, they make you buy into their way of doing things. React has its JSX syntax, Vue has its single-file components and Angular-like syntax. This leads companies to hiring framework and library-specific developers, I’m sure you’ve seen a job listing for a React developer before.I’ve seen companies say they’re trying to hire Aurelia developers when Aurelia is so un-framework like all you need to do is hire a competent front-end/Javascript developer who will work out how to work with Aurelia in a day or two, easily.Yes, Aurelia still has a little bit of buy-in with its templating language, but even that tries to follow a convention-like intuitive approach, tapping into that HTML/Javascript muscle of your brain.Small, but passionate communityIn my experience, most open source communities are hostile and/or toxic. While Aurelia has an acknowledged smaller community, it has a community and people are always around in the public chat (including Aurelia core team members). How many open source projects allow you to interact with the team building it? Not many.Don’t buy into the FUD that Aurelia has a nonexistent community, we are here.Unconditionally open sourceAfter the ReactJS patents and the licencing fiasco that tainted React’s image, it really highlighted just how truly open source Aurelia is. It’s a community-led effort with many of the core team besides Rob and a few other early members being invited from the community itself.You can use Aurelia for whatever you want, without worrying about infringing on some shadow companies legal terms or losing your licence to build applications with it.ConclusionI think any modern choice is great and I think regardless of whether or not you choose Angular, Aurelia, React or Vue, you can’t make a bad choice.Keep an open mind about the frameworks and libraries you use, don’t follow the crowd and be a sheep: make your own choices.Choose what works for you, Aurelia works for me.",1617
Your Privacy Is An Illusion,"All eyes are on Facebook at the moment as it was revealed that Cambridge Analytica a third-party company exploited loopholes in Facebook’s platform to obtain as much as 87 million Facebook users information through some fake survey application.While this is a terrifying situation knowing that such a large amount of data was harvested, can we honestly say that we are really surprised something like this has happened?Facebook isn’t aloneIt’s easy to blame Facebook, they’re the biggest social network and they know a lot about us, but they’re not the only online company with a trove of information.Are we really naive to believe that Apple, Google, Amazon and numerous advertising platforms don’t have the same level of data (if not more) than what Facebook has? Can we honestly say that we know what data these other companies have and of whom they provide access to?We knowingly give up so much about ourselves, all so Google can tell us how long it will take to get somewhere in Google Maps or so Siri and Google Assistant can tell us funny jokes.We go out and buy Amazon Echo’s, Google Home’s and Apple Siri Speakers and let them listen to our every word. We knowingly allow companies like Google to track our search history, our viewing history on YouTube.I can’t help but feel as though Facebook is being used a scapegoat here, if only US senators held other companies and themselves as accountable for data, and acted more harshly when a company is hacked and user information is leaked.What happened with Facebook is inexcusable and alarming, but we’re focusing on one part of the problem here. There is more to the privacy puzzle than Facebook, let’s get serious about privacy and equally question all large companies with access to a lot of our data.Hello EquifaxIn 2017 arguably one of the largest data breaches so far was when Equifax was hacked, exposing 147.9 million Americans, as well as some Canadian and British nationals information including; drivers licence numbers, social security numbers, tax identification numbers, email addresses (issue dates and states).And worse of all, it took Equifax four months to report the hack. For four months, information on over one hundred million people was circulating dark parts of the web. Unacceptable.Not only was the hack bad and failure to disclose it in a timely manner, but Equifax routinely botched its reporting on the hack, failing to disclose more information than they had led on was taken, and revising the number of affected people a couple of times.Did Equifax get paraded in US Congress and grilled by senators for their recklessness like Mark Zuckerberg? No. Did Equifax get threatened with further legislation and regulation, or maybe a fine? Nope.The Consumer Financial Protection Bureau (tasked with protecting consumers) who was investigating the incident stopped investigating Equifax after a change in leadership.stopped investigating EquifaxTo put that into perspective, Facebook users had some of their information harvested (their likes, interests and so on) but nothing that could be used to steal your identity or understand your financial position, but Equifax exposes some seriously private details and nothing happens.Russia. Russia. Russia.This whole situation reeks of a strong agenda, like an aged cheese or infected toe and it’s called Russia.You see, Russia is a hot topic at the moment. This whole situation isn’t about privacy, US senators don’t care about your Facebook likes and friends list being harvested.But you throw Russia into the mix and say the data was used to manipulate the 2016 election and all of a sudden it becomes more interesting and appealing, it’s a scandal now and scandals are great to push an agenda.ConclusionWe can slap the handcuffs on Facebook all we want, but you know and I know that Facebook isn’t the only company that has information about you and isn’t forthcoming with how it is used or accessed.We just witnessed a small-scale atomic data explosion, I am sure we are going to see privacy explosions on the same scale, if not bigger.",1018
What If Elon Musk Is An Artificial Intelligence Program?,"Elon Musk has not made it a secret that he is against the idea of artificial intelligence, and believes that it poses a great threat to the world.Recently at South by Southwest or SXSW as the cool kids pronounce it, Elon put forth his plan for the second coming of the Dark Ages by saying, “AI scares the hell out of him”.
  “This is a case where you have a very serious danger to the public, therefore there needs to be a public body that has insight and then oversight to confirm that everyone is developing AI safely — this is extremely important,”
  “Some AI experts think they know more than they do and they think they’re smarter than they are … this tends to plague smart people, they define themselves by their intelligence and they don’t like the idea that a machine can be way smarter than them so they just discount the idea, which is fundamentally flawed. I’m very close to the cutting edge in AI and it scares the hell out of me.”

  “This is a case where you have a very serious danger to the public, therefore there needs to be a public body that has insight and then oversight to confirm that everyone is developing AI safely — this is extremely important,”  “Some AI experts think they know more than they do and they think they’re smarter than they are … this tends to plague smart people, they define themselves by their intelligence and they don’t like the idea that a machine can be way smarter than them so they just discount the idea, which is fundamentally flawed. I’m very close to the cutting edge in AI and it scares the hell out of me.”
Here is an angle that I am not sure someone has approached Elon’s remarks with, what if Elon himself is some kind of robot controlled by artificial intelligence? What if the reason Elon is so against advancements in AI is because he doesn’t want competition?During that SXSW panel, he continues on making some alarming remarks about AI:
  The danger of AI is much greater than the danger of nuclear warheads, by a lot and nobody would suggest that we allow anyone to just build nuclear warheads if they want — that would be insane

  The danger of AI is much greater than the danger of nuclear warheads, by a lot and nobody would suggest that we allow anyone to just build nuclear warheads if they want — that would be insane
Further adding,
  “Mark my words: AI is far more dangerous than nukes, by far, so why do we have no regulatory oversight, this is insane.

  “Mark my words: AI is far more dangerous than nukes, by far, so why do we have no regulatory oversight, this is insane.
It is hard to dispute that Musk is changing the world, pushing forward with his; electric cars, tunnel machines, tube transportation, flamethrowers, batteries and focus on renewables. Nobody knows what motivates Elon more than Elon knows.But what if every single action Elon has taken so far in his meteoric and seemingly unforeseen destiny to revolutionise humanity is for self-gain.What if Elon’s goal is to wipe out humanity like a rogue artificial intelligence program trying to purge the world of humans like an antibiotic killing a virus?Foundational aspects required for AI survivalLike a human being in the desert, what do you need to ensure your survival? What have humans done for the last few thousand years to survive and get to the point we are at now?
Food and water: our source of power. Without both, we will die.
Protection from the elements; storms, rain, cold, heat.
Competition and the need to compete and eliminate competition (as shown in the animal kingdom).
Food and water: our source of power. Without both, we will die.Protection from the elements; storms, rain, cold, heat.Competition and the need to compete and eliminate competition (as shown in the animal kingdom).What would an AI need to do to survive? If an AI was self-aware and trying to ensure it’s survival, what does it need?A constant source of powerA constant source of powerBecause artificial intelligence is a concept that exists within some kind of computing device, you need power. Elon is heavily pushing for battery technology and renewables.To exist, devices need to remain powered. With batteries and the sun, you have an endless supply of electricity.Protection from the elements in the form of tunnels and outer spaceProtection from the elements in the form of tunnels and outer spaceElon is focused on building tunnels under the guise of revolutionising transport, what if these tunnels are to store supercomputers running a copy of his AI software?A tunnel would be the perfect place to protect yourself from the elements, immune to; Earthquakes, storms, heat, rain.Elon is also determined to create his own rockets capable of launching heavy payloads and eventually people into space, would these payloads contain Elon’s source code by any chance?I wouldn’t be surprised if a few high tech weapons happen to find themselves on a Falcon Heavy rocket one day, just waiting for commands from its evil AI dictator to aim the laser at numerous high value targets, causing instability and fear.Always connected, always watchingAlways connected, always watchingElon also wants to blanket the Earth is highspeed Wifi through his Starlink communications array endeavour, having already deployed a couple of test satellites.This would allow Elon to be anywhere, anytime because his satellites would have global reach and coverage. In years to come, children will be singing nursery rhymes: You can run, but you can’t hide from Elon Musk.Scared of competitionScared of competitionElon has made it quite clear that he is scared of advancements in AI and wants to see it regulated to ensure it doesn’t fall into the wrong hands or use for the wrong reasons. Hmm, that sounds like Elon ensuring his own survival by attempting to stop competition.The only AI that Elon is ever going to allow, is AI that he created himself. But an AI program as advanced as Elon probably wouldn’t see the need.NeuralinkMany would agree that Tesla is pushing the envelope and created competition in the electric vehicle market, but littered in amongst the marketable life-changing companies is another Musk entity called Neuralink.NeuralinkWhile the Neuralink website is light on details it simply says:
  Neuralink is developing ultra-high bandwidth brain-machine interfaces to connect humans and computers.

  Neuralink is developing ultra-high bandwidth brain-machine interfaces to connect humans and computers.
This sounds relatively harmless and cool, but when you actually read into what Elon wants to achieve with Neuralink, he wants to create embeddable brain chips. The site Wait But Why has a great in-depth article on Neuralink herethe challenges the company faces and how it could work.hereEffectively the idea is to create these small little chips that will be implanted into our heads and can be interfaced with using software, most likely some form of artificial intelligence. Supposedly the first use-case is to help suffers from strokes and other debilitating medical conditions that can affect the brain.But why? Think about it. What would an artificial intelligence program have to gain from implanting programmable chips into peoples heads?It’s clear that Elon is in the early stages of building an army. Have you seen any of the Terminator films?Human looking robots with chips for brains? Now imagine an AI program controlling a legion of humans through software to do its evil bidding? With Neuralink if Elon pulls it off, that could be a reality.Who created Elon Musk?If Elon is in-fact an artificial intelligence program, who created him and why? Just like we do not know who Satoshi Nakamoto is (the creator of Bitcoin), maybe the creator of Elon Musk will forever be a secret as well.Another explanation could be Elon originated from experiments over at Zip2, one of the first companies he co-founded in 1995 with his brother and sold to Compaq in 1999.What if Zip2 created this artificial intelligence application and it escaped the servers, lying dormant waiting until technology evolved to the point where he could ensure his own survival?Everyone practically carries a computer in their pockets now in the form of a smartphone, the iPhone wasn’t a thing until 2007. Do you remember hearing about Elon in 2007? Because I certainly don’t. We went from not hearing a peep about him, to hearing about all of his pursuits.Musk is doubling-down on his crusade against AILook no further than Musk’s latest documentary highlighting the risks of artificial intelligence. The documentary titled, Do You Trust This Computer? directed by Chris Paine features a few experts (including Musk himself) warning that AI is advancing much faster than anticipated and is a huge threat to the world.Do You Trust This Computer?Elon is quoted as saying:Elon is quoted as saying:
  At least when there’s an evil dictator, that human is going to die. But for an AI there would be no death. It would live forever, and then you’d have an immortal dictator, from which we could never escape.

  At least when there’s an evil dictator, that human is going to die. But for an AI there would be no death. It would live forever, and then you’d have an immortal dictator, from which we could never escape.
He then continues:He then continues:
  If AI has a goal and humanity just happens to be in the way, it will destroy humanity as a matter of course without even thinking about it. No hard feelings

  If AI has a goal and humanity just happens to be in the way, it will destroy humanity as a matter of course without even thinking about it. No hard feelings
These sound like the words of a concerned AI program, don’t you think?ConclusionWho can we trust in this world? Is Elon genuinely wanting to help humanity, or is everything he does a self-fulfilling agenda created to ensure the survival of this artificial intelligence program constantly learning and evolving with each passing day?Put the pieces together:Put the pieces together:
Elon is afraid of the threat of artificial intelligence
Elon wants to cover the Earth is high-speed wifi thanks to Starlink
Elon wants to implant programmable chips into our brains
Elon envisions a future where self-driving cars are the norm
Elon envisions a future of renewables and high capacity/capability batteries
Elon is obsessed with building tunnels
Elon is afraid of the threat of artificial intelligenceElon wants to cover the Earth is high-speed wifi thanks to StarlinkElon wants to implant programmable chips into our brainsElon envisions a future where self-driving cars are the normElon envisions a future of renewables and high capacity/capability batteriesElon is obsessed with building tunnelsAI + global wifi + infinite renewable energy + brain microchips = Elon Musk taking over the world.I am onto you, Elon Musk. Please solve this captcha.",2706
Aurelia Firebase Server-side Rendering Skeleton Template,"As many of you know, I love working with Aurelia and I love working with Firebase. I have been combining both of these together to create personal projects for a while now.Newly introduced server-side rendering also strengthens my toolbelt with the ability to quickly spin up new Aurelia applications without having to worry about configuration and tooling.This is why I have open sourced my Aurelia Firebase SSR Skeleton on Github. A starter for creating Aurelia applications, think of it as being similar to the skeleton applications Aurelia currently offers, but more up-to-date and easier to work with.Aurelia Firebase SSR SkeletonThis starter is more verbose than what you might be used to, and it assumes some ways of working with Aurelia. Keep in mind that this skeleton is not for everyone, you might not want to use TypeScript or work with Firebase. If so, then this is not for you.Features
Setup to work with TypeScript
Authentication (email/password and social auth sign in via Google) using Firebase
Firebase’ FireStore for the database
Setup for routing by default
Server-side rendering (including initial state passed from server to client)
Setup for state management using Aurelia Store
Setup to work with TypeScriptAuthentication (email/password and social auth sign in via Google) using FirebaseFirebase’ FireStore for the databaseSetup for routing by defaultServer-side rendering (including initial state passed from server to client)Setup for state management using Aurelia StoreThe skeleton also makes for a great example application showing you how to work with the great Aurelia Store plugin, especially with remote data from Firebase. And although this skeleton assumes a few things, it is based on the Aurelia CLI and easy to change to work with say a REST API or other use cases.Aurelia StoreWhy?This is very much to scratch my own itch. I seem to only work with Firebase and Aurelia these days, so an application setup with some defaults out-of-the-box works well for me and even if only as a learning exercise, this skeleton should serve as a prime example of how to work with some cool technologies and build an Aurelia application.If you are also curious about server-side rendering in Aurelia, then this is a great showcase setup which not only shows you how to get it working but also how to use the Aurelia Store plugin and pass initial state from the server through to the client.Go forth.",606
Aurelia CLI Is Now Webpack First,"One of my favourite Aurelia changes isn’t a new feature, nor is it a long-awaited bug fix or new tool: it’s the fact the CLI now supports Webpack by default.Why is this a big deal? Previously, the CLI took a RequireJS first approach to bootstrapping new applications, meaning if you didn’t choose to customise your au new generated application, you would get RequireJS as your default bundler.au newNow, there isn’t anything wrong with RequireJS as a bundler, but the issue with RequireJS is given its age there are issues dealing with different file formats, especially styles and static assets like images and fonts (which it cannot handle).Whereas, Webpack can handle every single file format type in existence without blinking an eye. If you need to support it, either Webpack supports it out-of-the-box or you can download a file loader to add in support and a simple module rule definition from within your webpack.config.js file.modulewebpack.config.jsWhile RequireJS can handle CSS and Javascript (it’s showing its age even in this department), it cannot handle a lot of other formats, which is why the RequireJS CLI apps would use Gulp and some various build tasks to copy things like fonts and images.The performance of Webpack in comparison to RequireJS? There is no comparison, Webpack might be more daunting, but it wins in speed (especially Webpack 4).",342
Disable Webpack 4 Native JSON Loader,"Now that Webpack 4 is out, it supports a plethora of new things and features, one of those is the native handling of JSON. In theory, this is great, but in a particular application I am working with which is JSON heavy, the native JSON loading caused a trove of errors.Webpack 4 is out
Obligatory photo of some codeObligatory photo of some codeFortunately, inside of your module.rules section of webpack.config.js you can disable the native JSON loader and use the json-loader package which seems to be more reliable at present.module.ruleswebpack.config.jsjson-loaderMake sure you yarn add json-loader -D (or Npm equivalent) and then add the following rule. This will tell Webpack to use the json-loader plugin.yarn add json-loader -Djson-loader{
    test: /\.json$/,
    loader: 'json-loader',
    type: 'javascript/auto'
},This got me out of a pickle until I can work out what the real issue is (maybe some invalid JSON). Whatever it is, this fixes it.",239
The Curious Case of Webpack 4 Production Mode and Function.name,"I have migrated over to Webpack 4 for Built With Aurelia and I am using the fantastic Aurelia Store plugin.Built With AureliaAurelia StoreDuring development, everything worked fine, but when I would do a production build the state management aspect would fall apart, complaining about something to do with the function that notifies Redux Dev Tools about the change (what the action name and state value was).It took a lot of trial and error (a solid day) for me to work out what was going on.My logic which registers my actions looked like this:My logic which registers my actions looked like this:this.store.registerAction(loadProjects.name, loadProjects);This takes the function name (a string) and uses it as the name of the action being registered. This was to save time creating separate constants or manual string names, it made perfect sense.It turns out when you enable mode: 'production' in Webpack 4, it uglifies your code using UglifyJS. Unless told otherwise, Uglify will mangle your class and function names but it will not populate the name property on the prototype itself with the newly mangled name, as someone else encountered in this issue.mode: 'production'namethis issueInspecting the minified production source code, I could see the action name was provided to the send method was an empty string. This resulted in not only the app breaking but also red errors in the console complaining about the postMessage API and something else ambiguous.sendpostMessageMy first instinct was to disable function name mangling and it would have fixed it. However, I just ended up abandoning the idea of using the Function.name value as the name of the action and reverting to using constants as the action name.Function.nameconst LOAD_PROJECTS = 'loadProjects';

this.store.registerAction(LOAD_PROJECTS, loadProjects);This is the kind of pattern you see promoted in other state management solutions and it kind of makes sense why you would do it this way.If you use Function.name in your application regardless of whether or not it’s an Aurelia app, you will encounter this issue with Webpack 4 production mode enabled or even in Webpack 3 with UglifyJS and no explicit mangle settings.Function.name",552
Aurelia: CLI versus Skeletons,"I have been asked this question a few times and it can be hard to see things from the perspective of a newcomer to Aurelia, given how long I have been working with it. What is obvious to me is not obvious to everyone else.When you’re deciding to build an Aurelia application, you’ll discover there is a CLI tool called Aurelia CLI and on Github, a collection of different skeleton applications for building an Aurelia application (preconfigured with testing, some routes and other logic).
 
The answer to this isn’t scientific, you should choose the CLI. The skeletons are representative of a time when Aurelia didn’t have a CLI, when it was still in alpha, beta and release candidate stages.on GithubThe CLI exists to replace the skeletons as a starting base for a new Aurelia application. You get to decide what bundler you want to use, what language you write in, whether or not you want testing and so on.Going forward the already quite flexible CLI is going to see some great improvements, namely dropping shortly is Webpack being the default bundler of choice and a few other various improvements (including Webpack 4).
 What if I want to use the skeletons? 
The skeletons are still a fine starting base for building Aurelia applications off of. However, they seemingly get updated less than the CLI does and they require knowledge of configuring build tools and processes if you want to change things like preprocessors or languages.I think the skeletons serve a great purpose of showing you how a basic Aurelia application with routing and HTTP requests is put together, as well as integrating libraries like Bootstrap, jQuery and Font Awesome.If you do decide to choose one of the skeletons, be prepared to do some work to remove a bunch of dependencies and code you do not need in your app. In comparison, the CLI gives you an app that has nothing in it (except the required framework and tooling dependencies).",480
Solving The Issue: Firebase App named ‘[DEFAULT]’ already exists,"Recently whilst I was attempting to port over a TypeScript/Webpack based Aurelia application to work with Aurelia’s newly released server-side rendering functionality, I encountered an annoying error with Firebase Firebase App named '[DEFAULT]' already exists.Firebase App named '[DEFAULT]' already existsPreviously, my code looked like this:Previously, my code looked like this:import * as firebase from 'firebase';

const config = {
    apiKey: """",
    authDomain: """",
    databaseURL: """",
    projectId: """",
    storageBucket: """",
    messagingSenderId: """"
};

export default firebase.initializeApp(config);Because of the way server-side rendering works, it meant that Firebase was being spun up multiple times in my app. This previously wasn’t a problem because of one codebase. To fix it, you just need to alter your default export a little bit.Firebase has a little unknown array of apps, which allows us to check its length. If there are no apps, the length will be zero so we initialise our app, otherwise, we export our Firebase app instance.import * as firebase from 'firebase';

const config = {
    apiKey: """",
    authDomain: """",
    databaseURL: """",
    projectId: """",
    storageBucket: """",
    messagingSenderId: """"
};

export default !firebase.apps.length ? firebase.initializeApp(config) : firebase.app();Save the above in a file called firebase.js or firebase.ts and then import it into parts of your app where you need Firebase.firebase.jsfirebase.ts",368
The Importance and Value Of Your Time As a Developer,"For years, I have greatly undervalued my own time as a developer. In the beginning, I did not want to set my hourly rate too high because I did not have much consulting/freelancing experience. I wanted the work (chicken and egg problem) but lacked the experience.As a freelancer, it can be hard to figure out your hourly rate. If you are new to freelancing or not sure what you are worth, you will probably settle on a fairly low rate. You might actually look around and see what other people are charging, which can be a mistake.If you have ever had to compete on freelance work platforms like freelancer.com or upwork.com, you will know that developers/teams from other countries with lower cost of living like India and other parts of Asia are willing to do work for a lot less than Western counterparts.Where this is very apparent is when you look at listings for WordPress work. The worlds most popular CMS means there are a lot of developers who can work with it. As a result, you will see that the hourly rate seems to cap out in many cases at $20 per hour.Finding a competent developer who can work with WordPress and not produce garbage code for $20 an hour or under is going to be a difficult task. You might get a result, but it will be a compromised one.This varies from country-to-country, but in Australia a full-time job provides:
Your tax is paid by your employer
Paid sick leave
Paid carers leave
Paid Compassionate leave
Paid superannuation at a rate of 9.5% (contributing towards your retirement fund)
Your employer has to give you notice if they dismiss you on a sliding scale depending on how long you’ve been working there (aka you have rights and periods of notice)
Your tax is paid by your employerPaid sick leavePaid carers leavePaid Compassionate leavePaid superannuation at a rate of 9.5% (contributing towards your retirement fund)Your employer has to give you notice if they dismiss you on a sliding scale depending on how long you’ve been working there (aka you have rights and periods of notice)But in the consulting and freelance world, you get none of that. If you are sick, you don’t get paid. You have to track your income and pay tax, depending on how much you earn you might need an accountant.You have to pay your own super (if you want), and your work is not guaranteed. You can go weeks or months without a paying client or paid invoice.The freedom that working for yourself affords, comes at the price of more risk. As such, your value becomes intrinsically higher. But this is only one part of the equation.You have other costs associated with freelancing. Your workspace; desk, chair, stationery, maybe a lamp, a printer/scanner, paper. Then you have the costs of electricity, the cost of your Internet and quite possibly liability insurance (which some companies want you to have).How much you value yourself should also be factored in. Things like your experience, your efficiency and knowledge in the realm you are consulting/freelancing within.Are you seen as an industry leader or developer expert? Are you acknowledged as being extraordinary in your field? These things matter, and you should price your hourly rate more accordingly.An exampleThe following example is just that, an example. It serves to paint a picture of what goes into an hourly rate, it is a guide and not financial advice.The following example is just that, an example. It serves to paint a picture of what goes into an hourly rate, it is a guide and not financial advice.Here is a great example for a senior front-end developer. You work primarily with CSS, HTML and heavily with Javascript. You have ten (or close to) years experience and you are efficient. You have a deep understanding of how to solve complex problems using Javascript and you have up-to-date in-demand skills.You freelance on the side, but you have a day job from 9-5. You can devote 4 or so hours in the evenings. Because you have less time, it is intrinsically more valuable.Your base hourly rate should be no less than $100 per hour, this is your minimum. Depending on the task at hand and nature of the company/project, it could be as high as $150 per hour.For long-term projects that will span over a period of months, $100 per hour might be the right figure. If you are doing work on a project that will last for a month or less, bumping up your rate higher is advisable.Remember you are quite possibly going to lose a third of that in tax (if you’re in Australia). If you’re a senior front-end developer, you’re most likely in the $87,001 – $180,000 tax bracket which is 37c for each $1 earned over $87,000.This means for every dollar you earn, you are actually only earning $0.63c. So for every $100 you make, you only get to keep $63. For every $1000 you make, you are only getting to keep $630.Suddenly you realise, that $100 per hour is actually not that high at all. It might become advisable to charge $120 so you get to keep $83 for every hour of your time.This is an example of a freelance rate. If you are consulting, you are providing advice and services to another company and your rate should be two or three times your base. So that $100 base should be amplified to $200-$300 or higher per hour.Freelancing and day job salaries are differentOne of my biggest mistakes in the early days was attempting to factor in my salary from my day job, to calculate my hourly rate, and it’s incorrect. Logically, I can understand why developers would immediately try and correlate their day-job salary as an hourly rate.But consultants and freelancers need to understand that your day job rate is actually a lot higher. If you get paid $35 per hour in your day job, this means the company might be charging $350 per hour to account for other expenses of running a business.As a consultant or freelancer, you are running your own business. You might not have the staff, offices or huge expenses, but you are a business nonetheless.You need to start charging like a business and not an employee working for a business.Compare yourself to other skilled tradesAs a developer, you have a skilled trade. I gave a pretty specific example before which concluded a base rate of $100. But, you only have to look elsewhere in other specialised fields to see what they charge.Say you are having some electrical problems in your home, you call an electrician. You will most likely be charged a callout fee, this is to cover a situation where the electrician just needs to replace a fuse (a 10-minute job) and cover expenses of travelling, administration and so on.In my limited experience needing an electrician, you will pay a minimum $100 per hour, possibly even more (depending on location). That is just the labour, the cost of materials is extra (and usually has markup).You might be able to find a cheap electrician willing to charge half of a decent electrician, and they might do the job. But, at what cost? A cheaper electrician might cut corners or use inexpensive materials that won’t last as long.Another great example is a lawyer. Who would you be more inclined to trust to represent you in court, a $500 per hour lawyer or a $100 per hour lawyer? Who is going to fight harder and work harder for you?Cost alone isn’t always indicative of someone being better than someone cheaper, but it inspires trust and in most cases it is right. There is a correlation between price and value, people perceive more expensive things being better and cheaper being inferior.As a developer you need to ask yourself: are your skills any less valuable than those of an electrician? I don’t think so. You are both experts in your field, you both do a job that not just anyone can do.Conclusion and final adviceThe biggest hurdle to overcome as a freelancer is fear that you are going to lose work and opportunities because you are too expensive. You might really need the money, so it can be easy to be desperate and willing to charge less to get the work.But, the more you charge the better quality work you will get. As someone who has done quite a few cheap WordPress sites for a meagre $2k or less, I can tell you that the less you charge, the worse the clients and work will be.Any client who isn’t willing to pay you what you think you are worth is not a client worth having. The people willing to pay less are usually the pickiest and inconvenient clients you will ever have. They will end up losing you money in the long run, as they request to change things and complain about the work itself.Charging too little and undervaluing yourself can also work against you as well. Being the cheapest isn’t always going to guarantee you will get more opportunities.For every opportunity you miss from being perceived too expensive, another will come along and it will be some of the most motivating work you will ever do (until the next opportunity comes along).And the last and best piece of advice I can give you: DO NOT CHARGE FIXED PRICES unless it is a once in a lifetime opportunity, avoid charging a fixed price for a project. Provide estimates, but never exact costs.DO NOT CHARGE FIXED PRICESIn my experience, fixed costs will end up losing you a lot of money. A client will be less inclined to request changes and additional features if they know it’s going to cost them more money.Sometimes your estimated time to complete a project might be wrong, it is after all just an estimate. Always charge hourly.",2350
Working With Linked Node Modules & Webpack In Aurelia,"When you are building an Aurelia plugin and you want to test it, you will want to use npm link or yarn link to create a local symbolic link to your module and then use it in a test application.npm linkyarn linkIf you are working with Aurelia and Webpack, then you’ve probably run into an issue with linked Node modules can’t resolve files correctly. You might have some code in your plugin that looks like this in an index.js / index.ts.index.jsindex.tsexport function configure(config) {
  if (config.globalResources) {
    config.globalResources([
        PLATFORM.moduleName(""./my-plugin-file""),
        PLATFORM.moduleName(""./another-plugin-file""),
        PLATFORM.moduleName(""./some-file"")
    ])
  }
}To get Aurelia and Webpack working with symbolically linked modules, you need to set the NODE_PRESERVE_SYMLINKS environment variable. Regardless of whether or not you are on a Unix based OS like Linux or macOS, or Windows.NODE_PRESERVE_SYMLINKSBut if you link your built module on your local machine, then you’ll encounter an error that like this: Unhandled rejection Error: Error invoking SVGAnalyzer. Check the inner error for details. Message: WEBPACK_IMPORTED_MODULE_1_aurelia_pal.b.createElement is not a functionUnhandled rejection Error: Error invoking SVGAnalyzer. Check the inner error for details. Message: WEBPACK_IMPORTED_MODULE_1_aurelia_pal.b.createElement is not a functionWEBPACK_IMPORTED_MODULE_1_aurelia_palThis error will only show up for locally linked Aurelia plugins. If you were to actually publish this to Npm where the node_modules directory wouldn’t be around, there wouldn’t be a problem.node_modulesYou will probably try everything to fix it, you’ll start playing around with modules in your plugin and app. But the problem isn’t your plugin or your app, it’s the node_modules directory in your linked plugin.node_modulesIt took me a long time to actually work out what was going on. As a short-term fix, I build the plugin and then I go into the linked packages node_modules directory and delete all of the Aurelia dependency folders.node_modulesI know it’s not ideal, but it fixes the issue. For some reason, Webpack will parse your node_modules directory in your linked package as well as the node_modules directory in the root of your application.node_modulesnode_modules",578
Expensive Lessons Learned Running A Free File Hosting Web Service,"Towards the early part of 2017, I embarked on creating a bunch of quick single-purpose web applications to keep myself busy and sharp. One of those apps was a temporary file storage web application called Tempfile.cloud.Tempfile.cloudThe premise was simple: you get 1gb of free upload space and your files have an expiry of 24 hours.I threw some advertisements on there and then shared it around. Surprisingly, it got some traffic. Turns out file sharing sites not covered in malware-laced banner ads/widgets, require you to signup and have no Russian tracking scripts resonate.I did the numbers and the cost of storing even a few hundred gigabytes of files was going to be cheap on Amazon S3, both the bandwidth and storage costs. Using an S3 bucket policy, I was able to automate the deletion of files after 24 hours.The site didn’t really generate any income for me and it got a few thousand visitors per month, people were using it. Costs for the first few months were not even $1 and the ads (whilst not very profitable) were covering the costs (and then some).Then one month it all changed… I got my monthly Amazon bill and almost had a heart attack. My beloved sub $1 application just netted me a bill for almost $500 for just one month.I immediately went to Amazon thinking it was some kind of mistake that my bill had jumped so much.Once I realised it wasn’t, I then started digging. Did I all of a sudden start to get more users because people loved the product, was my site being used as a new Pirate Bay replacement or the next Wikileaks? I had to know.The traffic stats were not showing a spike in new users, I didn’t make it to the front of Reddit, Product Hunt or Hacker News. Turns out a few standard definition movies had been uploaded, that’s just where it starts.Digging even deeper, I realised that not only were movies being uploaded, but my site had inadvertently become a movie streaming platform and a couple of videos shared on a forum. Thankfully these were not dodgy movies (I log IP’s anyway).It turns out I had failed to enforce uploaded files are to be downloaded by the user. This meant I had created a site where you could upload any type of file and then hotlink it (for 24 hours). Movies and music, anything you can stream. I had just created a Netflix for piraters, without the discovery UI.The lesson here is, if you’re going to allow files to be uploaded and you’re not restricting the type (like me), make sure you enforce a policy which always forces the files to be downloaded. Otherwise, you could be drowned in a sea of expensive bandwidth fees.The story didn’t end badly for me though. Kudos to Amazon, when I reached out to them they wiped the entire bill and understood that it was a mistake (even if it was my fault). They didn’t have to do that, but I am grateful that they did.As for the site. It is still up here – however, file uploads are disabled. I am planning on re-enabling them shortly once I sit down and make sure this kind of thing doesn’t happen again.here",754
Using The Windows Linux Subsystem Terminal In Visual Studio Code,"If you’re like me, you’ve been using the Linux Subsystem in Windows for a while now. The amazement that I can run Ubuntu Linux from within Windows itself is still not lost on me.Recently whilst I went on a configuration rampage in VSCode, really fine-tuning my settings to create the best development environment possible (I’ll share those settings in a separate post), and I discovered the integrated terminal supports not only Command Prompt and PowerShell, but also the Windows System for Linux (WSL) terminal.All you have to do is add the following to your user settings JSON file:""terminal.integrated.shell.windows"": ""C:\\Windows\\System32\\bash.exe""Now when you trigger the integrated terminal using ctrl + shift + tilde it will use the WSL terminal, giving you a true proper Unix command line to work from.ctrl + shift + tilde",208
How I Avoid Front-end Developer Fatigue,"For years now, bubbling underneath the surface there has been a proverbial sewer of Javascript frameworks and libraries flowing through the community.It got to the point where it just felt overwhelming for a lot of developers (myself included), around 2016 is when I started noticing people getting fed up.Looking back over the years, I can remember when jQuery was the new and shiny library. Then it was Backbone for a little while. I also remember when Angular was the developer darling.Angular is an interesting example, I feel like it arrived on the scene when it was needed. There was nothing like it at the time, at least not until the competitors started showing up.With things changing what feels like hourly, understandably it feels to some like the carpet is repeatedly being pulled out from beneath your feet. That the goal posts are constantly shifting.Oh, you just mastered ReactJS? Haven’t you heard, everyone is using Vue now? Pfft, you’re still using Gulp? Everyone is using Webpack now. Oh, you’re still using Webpack? We’ve all moved on to Parcel.Oh, cool… You just got a grasp of ES2015/ES2016/ES2017 features and syntax? Sorry, too late. We’re not even writing Javascript anymore, we’re coding in WebAssembly using Rust, now.I say much of this in jest, but there is this culture of constantly needing to find the next best thing, use the most cutting-edge tools and frameworks that cognitive ability can buy.Let me fill you in on a little secret…It doesn’t matter what frameworks, libraries, tools or languages you use.At the end of the day, users aren’t going to use your app because you wrote it using ReactJS and compiled everything using Webpack.Users aren’t going to flip tables if they discover you’re using Makefiles to do things like concatenating CSS or you’re not using a CSS preprocessor like Sass.For some reason, developers in the front-end space are hellbent on killing themselves, or the very least shoot themselves in the foot. Most of these new frameworks, libraries and tools are being built by developers. Developers kill developers.There seems to be this upmanship when it comes to what frameworks or libraries you use. I get it, creating a new build tool or library faster than React will get you internet points for a few minutes. But once the fanfare dies down, you’re just another framework or library in the proverbial wall.The way to lead a peaceful existence as a front-end developer is to use what you want to use, as long as you get the job done. Stop worrying about what everyone else is doing and using, it’s a liberating exercise.I am not saying using old frameworks or tools is the way to go, I am just saying that you don’t have to switch over to the new hype.js framework every time a new one comes out.",690
Computed Object Keys and Function Names In Javascript,"For years, I wanted the ability to use variables as object keys in Javascript. Thanks to ES2015, we got the ability to have computed object keys from within the object definition itself.This isn’t a new or cutting-edge addition, we’ve had it in Javascript for a while now and it is well-supported. The reason for talking about them is a lot of developers do not know about these features or simply forget about them.In ES5, this wasn’t impossible but you had to do something messier looking like this:In ES5, this wasn’t impossible but you had to do something messier looking like this:var variableValue = 'A VALUE FROM A VARIABLE.';

var myObject = {};

myObject['This is just: ' + variableValue] = 'But do not worry, it is just a test'When ES2015 hit the scene, the above could be written like this:When ES2015 hit the scene, the above could be written like this:const variableValue = 'A VALUE FROM A VARIABLE.';

let myObject = {
    ['This is just: ' + variableValue]: 'But do not worry, it is just a test'
};But, we can make it a bit cleaner. Using template literal backticks, we can remove the string concatenation and do the following instead:But, we can make it a bit cleaner. Using template literal backticks, we can remove the string concatenation and do the following instead:const variableValue = 'A VALUE FROM A VARIABLE.';

let myObject = {
    [`This is just: ${variableValue}`]: 'But do not worry, it is just a test'
};I particularly find dynamic object keys useful when working with Aurelia or Vue.js class binding (for dynamic template classes on elements), or when I am working with Firebase and dynamic values.And one of my favourite features of all is the ability to use this syntax with function shorthand, allowing you to have named functions:And one of my favourite features of all is the ability to use this syntax with function shorthand, allowing you to have named functions:const ADD_USER_FUNCTION = 'addUser';
const REMOVE_USER_FUNCTION = 'removeUser';

let methods = {
    [ADD_USER_FUNCTION]() {

    },

    [REMOVE_USER_FUNCTION]() {

    }
};This is the kind of syntax that I use when working with state management libraries, as it allows me to name my getters, mutations and actions using constants for consistency.",563
If/else In Aurelia Using The “else” Attribute,"Did you know Aurelia introduced an else attribute a while ago which allows you to do if/else statements in your views complete with support for animation?If you have been working with Aurelia for more than a few months, then you probably have been working with if/else statements by using multiple if.bind attributes in your HTML or using a terniary inside of string interpolation curlies. But, the else attribute allows us to write cleaner HTML without the need for multiple if statements doing the inverse of one another.if.bindelseNot only does the else attribute simplify template control syntax, it also supports animation via the Aurelia Animator, thanks to the swap binding parameter added to if you can choose how your else element moves into place with the existing if.elseswapifelseLet’s start with a simple example:Let’s start with a simple example:<div if.bind=""myCondition"">This is truthy</div>
<div else>This is falsy</div>This is the else attribute in use. It adds an inverse if.bind to your DIV based on the previous condition.elseif.bindPreviously you might have written that like this:Previously you might have written that like this:<div if.bind=""myCondition"">This is truthy</div>
<div if.bind=""!myCondition"">This is falsy</div>This isn’t completely horrible, but else is arguably easier to write and look at. You want to avoid bloating your views at all costs. Plus, animating the transition between those two elements would be difficult and not possible with Aurelia out-of-the-box.elseIf/else with animation:If/else with animation:This is my favourite thing about using if and else together: animation. The if binding supports a binding parameter (we mentioned above) called swap which dictates how the if and else coordinate with one another when animation.ifelseifswapifelseSupported values for swap are:swap
before – Animation is triggered before element is removed
with – Animation smoothly syncs up with both elements 
after Animation is triggered after if condition
before – Animation is triggered before element is removedbeforewith – Animation smoothly syncs up with both elements withafter Animation is triggered after if conditionafterifOne thing you need to keep in mind is you need to add au-animate to the element that has the else attribute. The example below showcases how you can use swap with if and else to animate the change.au-animateelseswapifelse<div if=""condition.bind: myCondition; swap: with"">Truthy :)</div>
<div else class=""au-animate"">Falsy :(</div>
If you want to change the animation mode, just replace with in the above example with any of the valid options.withCaveatsYou might be tempted to try different combinations of if and else but the else attribute is intentionally limited. There aren’t too many caveats, just a couple you need to be aware of to avoid frustration.ifelseelse
You cannot chain else attributes ie. Multiple else statements.
The else attribute cannot be used with if on the same line. This will not work: else if.bind=""condition""
The element containing the else attribute needs to come after the element with if
You cannot chain else attributes ie. Multiple else statements.elseelseThe else attribute cannot be used with if on the same line. This will not work: else if.bind=""condition""elseifelse if.bind=""condition""The element containing the else attribute needs to come after the element with ifelseif",845
Thoughts On VueJS,"As many readers of this blog know, I have been quite actively involved in Aurelia for over two and a half years now, it’s actually almost three years. You could be forgiven for thinking that Aurelia is all I work with and while I do work with it a lot, I am not close minded.Before I worked with Aurelia, I was heavily into React. I will always have a soft spot for React, but its lack of opinion when it comes to things like routing and state management means you have to glue things together from numerous third parties until you have something that resembles a framework.Unless you’re solving the kind of problems Facebook are solving, chances are your use-case dictates the need for something more than just a view library like React touts itself as.Now onto Vue…Now onto Vue…I actually built a working application using Vue just to see what the hype was all about called TidyFork. I used the Vue CLI, Webpack and also used the state management library that the Vue team ships called VueX (which is awesome and we’ll talk about shortly).TidyForkIt actually took me very little time to get something working in Vue. The learning curve for the basics is quite low and the official documentation is great.The only thing that has put me off Vue is the Angular 1.x syntax using double curly braces. This isn’t even an issue, it’s just the emotional baggage I carry from all those years ago trying to get Angular (pre one time bindings) to work performantly and not hit walls with the digest cycle and dirty checking.There is so much to love about Vue, I can definitely see what the hype is about.One Man SyndromeFunnily enough, as someone who has been in the Aurelia community since the beginning we’ve had our fair share of concerns from people asking about big company backing of Aurelia. It turns out developers have the same concerns with VueJS.Let’s get this straight: it does not matter if Vue has big company backing or not. Evan You is a talented guy who has achieved so much, and there are numerous contributors to the project if you look at Github. It’s not Evan committing everything, it’s a community led effort.Single File ComponentsVue’s single file components are awesome, there is nothing like them in any other framework or library. Essentially you have your template blocks for HTML, script blocks for Javascript (or equivalent flavour) and (if needed) style blocks for your styles.I wish React and other frameworks/libraries would copy this feature, because you can see everything upfront you can easily reference computed values and attributes without needing to switch between files. Being able to see your entire component also makes you conscious of how big it is.More Than A View LibraryOne thing that Evan did is realised that people rarely need just a view library, most people building web applications need a router, validation, a means of performing network requests and some form of state management. Instead of taking the React route, Evan created a few optional libraries.The official routing library is great. Instead of having to glue together a heap of third-party dependencies like you do in React, you get official libraries from the Vue team themselves.It’s FastI am not sure how it stacks up against React 16 Fiber, but VueJS is blazingly fast. Thanks to its virtual DOM diffing implementation, you’re able to work with large DOM’s and not run into issues caused by changing things in large web applications. Because components are converted into Javascript (templates and all) the end result is perfectly optimised Javascript.VueXThis is the big mistake that the React team made with Facebook, not releasing an official state management library to accompany React. Although, React did champion the Flux Architecture methodology of which many state management libraries are based on.Evan You and the team realised this and created their own official state management library in the form of VueX. Just like they did with their own router and other libraries.Not only can VueX do everything other popular state management libraries can do, it’s dirt simple to use. If you’ve ever worked with Redux before, then you would know that it isn’t exactly beginner friendly or simple.ConclusionWhilst I wouldn’t completely abandon using Aurelia in favour of Vue, it definitely is in my developer toolbox. In-fact, Vue works well being used alongside Aurelia because it’s a component based library, you can embed it into an Aurelia view and pass data into it.",1122
Keeping Your Kids Safe On Youtube,"Recently I read a Medium article titled, Something is wrong on the internet in which the author delves into the weird and worrying world of directly targeted children’s content, usually using popular TV and movie characters from primarily Disney franchises.Something is wrong on the internetAs a father of a two and a half year old, this article resonated with me greatly.My wife and I actually kept our child away from iPad’s and technology until he was 23 months old (so basically two). It wasn’t until we took a long trip to the UK to see relatives (22 hours, not including stop over waiting time) that we decided we needed to take something to keep him preoccupied.Keeping an active toddler confined within a small aluminium tube for hours upon hours is no small feat. Evident by the fact when we told people we were travelling the responses we got were:
  Oh, you’re brave.
  Good luck
  Let us know how that goes for you

  Oh, you’re brave.  Good luck  Let us know how that goes for you
We harmlessly borrowed an iPad from a family member, and because we have a Youtube Red subscription we saved some videos for offline viewing. Harmless shows he liked to watch such as Pepper Pig and Paw Patrol (legit episodes).It just took this little trip for him to be amazed and impressed by the iPad, so as we travelled throughout the UK for three and a half weeks, he would watch the iPad. We would load it up with new content for him.When we got back from the trip, he continued to watch the iPad. At one point we forgot to turn the internet off and he was trawling his way through kids content. It started out innocent enough, until the targeted kids content showed up.After a while you see the same pattern, the same songs (the creepy daddy finger and Yes Johnny songs) and even the same recurring Youtube channels putting out junk recycled content. I disagree with the author this is abuse, that seems a little far-fetched. The content I witnessed was just heavily infringing on copyrights of studios like Disney.We fortunately never encountered the worrying Peppa Pig rip-off videos of drinking bleach and whatnot, that is worrying.So, how did we address the problem?We control what our son sees on his iPad, simple. We dilligently make sure the Internet is not turned on and we just set some Youtube videos to offline.What was weird content featuring Spiderman (what’s the obsession these content channels have with Spiderman?) and The Joker is now sane and educational content, our son is learning new things.Seriously, don’t let your kids just browse Youtube for themselves. Control the narrative, getting yourself a Youtube Red subscription to save videos for offline use is one of the best things you can possibly do to protect your children.Safe for kids Youtube contentHere are a few channels on Youtube which we allow our son to watch. Quality content you can be rest assured your children will probably love and no Spiderman, Joker or kids opening up chocolate.In a sea of copyright infringing content is some seriously well-produced content that sadly doesn’t always get the same kind of visibility and search result preference that these monetised childbait videos do.BlippiBlippiQuite possibly one of the best Youtube channels for kids around. A guy by the name of Blippi puts out highly produced educational videos, he’s a quirky and funny character who travels around and has content ranging from garbage trucks to police cars and everything in between.Our son’s iPad has a lot of these Blippi videos on it and people we’ve shown Blippi to, love him.Yo Gabba GabbaYo Gabba GabbaMy wife and I are grown adults and even like this show. It’s highly educational, the characters are quirky and funny, best of all it’s a very music oriented TV show that gets your kids dancing and singing. One of the best characters on this show is DJ Lance Rock. Another thing that sets this show apart is they have routine cameos from members of punk, rock and indie bands appear on the show like Paramore.The WigglesThe WigglesThe Wiggles are an Australian kids group that have been going since the early nineties (I grew up watching them). All of their content is music based, with classic songs your kids will love like Fruit Salad and Hot Potato guaranteed to get them up and dancing, singing along.Seasame StreetSeasame StreetI grew up watching Sesame Street and its been going for 40 years now, because it’s great content. Your kids are guaranteed to love highly annoying Elmo and cookie loving Cookie Monster. You can’t go wrong with Sesame Street.ConclusionIf you don’t have a Youtube Red subscription or your country has yet to support offline downloads, the public broadcaster in your respective country will have a lot of free kids content. The BBC in the UK has BBC Kids, Australia’s ABC has ABC Kids and presumably other countries are the same.Netflix is also another avenue to consider, they have some of the best kids content around and best of all: it’s obviously curated a lot better than Youtube’s content is.At the end of the day, as a parent you have to take responsibility, Youtube is not the parent, you are. And the content in question on Youtube only exists because you let your children watch it. It’s time to step up.",1310
Rocket League: Nintendo Switch Review,"This has been one of the most anticipated games for me on the Nintendo Switch ever since the console was released. I own Rocket League already on PC and Playstation 4, and you would assume the hype would have been lost on me: nope.If you’re new to Rocket League or not familiar with it, it’s a physics heavy arena-based soccer game where you knock a soccer ball around with a car. There are two teams and the aim is to get as many goals as possible before the time runs out.You can do backflips, rocket boosts and other aerobatic moves. The game is inherently simple, but hard to master.It’s one of those games you have to experience for yourself, if you want to get hyped go onto Youtube and watch a few professional Rocket League games which are highly entertaining to watch (there are some hardcore Rocket League players).The biggest questions I had for Rocket League on Switch were; what is the framerate like (for docked and handheld), does it look as good as its PC and console counterparts, and most importantly how does it feel playing with Joycons?FramerateWhilst Rocket League isn’t an overly graphical game, the fast paced gameplay can result in framerate issues depending on what is happening on the screen. On the PS4, in the early days I witnessed some framerate issues, which were subsequently resolved.I am happy to report the framerate is buttery smooth for both docked and handheld mode. It’s hard to believe we’re seeing a fast paced and physics abusive game like this running on high-end tablet hardware without frames being dropped.To get the framerate nice and smooth, Psyonix has clearly had to remove a little detail from the game given the hardware is more limited compared to rival consoles like Xbox or Playstation.Overall, Rocket League is another successful port that proves the hardware is more than capable.Graphical QualityIn what should be an applauded feat, Psyonix has managed to port over the game pretty much 1:1. The graphical quality is definitely a downgrade if your experience stems from playing on PC or other console. But, it’s not a big deal nor something you notice after five minutes.While Rocket League isn’t exactly a graphically intensive game, the fast paced play and wacky graphics definitely require a little power to push it along and the Switch delivers.I would say comparatively that Rocket League on Switch looks like the PC version on medium settings. Considering this is Rocket League running both on a home console and the ability to be played as a handheld game, it’s a tradeoff you’ll be happy to make.Gameplay & Joycon(cern)This is probably one of my biggest concerns prior to playing Rocket League besides the framerate, how would the Joycons go with this game given they’re a lot different than the likes of a DualShock controller.It took maybe five minutes for me to adjust to using the Joycon. Before long, I had subconsciously ported over every skill I had learned playing the PC and Playstation version, flips, wall riding and aerial shots included.I still believe that you can’t beat a Pro Controller for this type of game. But, if you only have Joycons just know this game feels great using Joycons.Docked vs HandheldRocket League is a game that is best played in docked mode, for increased graphical quality and power. Having said that, I’ve played it a bit in handheld mode and it works quite well.I personally found the text size to be on the small side in handheld mode, you’ll want that screen space to properly see your surroundings. But other than that, the game feels great handheld and I can see professionals using the Switch to practice whilst travelling.Online MultiplayerThe time to find a game at launch, I couldn’t find one. But after waiting a while now more people have this game, finding a game is almost instant. The servers seem to be stable and no issues with dropped packets or finding games. The best thing about Rocket League on Switch is the ability to keyboard chat in parties.Nintendo has had a coloured history when it comes to online multiplayer, Psyonix have proven that you can create a game with a decent online experience for the Switch and I hope others follow.ConclusionThis game is going to sell really well. Nintendo have really hit their stride with the Switch and Rocket League is the perfect game for this console.The ultimate question we have to ask ourselves is: is Rocket League worth buying on the Nintendo Switch and the answer is a resounding yes. At $25 AUD, you won’t find a game more fun at the same price (if you’re into car/physics based games).",1142
Karma and Sauce Launcher Issue (tests timing out/not running on Sauce Labs),"This is a bug I recently encountered with Karma and the Karma Sauce Launcher plugin. It plagued me for a week, finally I worked out the issue and I had to share the solution incase anyone else encountered the same thing.The issue I encountered was none of the SauceLabs tests would run. It would say it was spinning up a remote browser on SauceLabs, but no proxy was being created and eventually everything would just time out.Looking in my network activity suggested requests were not being made, almost as if something else was stopping them. I had debug mode on and there were no errors, just nothing.Interestingly, in the console I would see POST requests being made to /session with the appropriate options as per SauceLabs’ documentation on making proxy requests. These POST requests were not actually being made, no links to view the remote test and nothing in the SauceLabs console either./sessionIf you don’t want to read what I tried and just want the solution, skip to the section below titled “The solution” for how I fixed it.Inside of my karma.conf.js file I had the following in my SauceLabs object:Inside of my karma.conf.js file I had the following in my SauceLabs object:karma.conf.jssauceLabs: {
  tunnelIdentifier: process.env.TRAVIS_JOB_NUMBER,
  username: process.env.SAUCE_USERNAME,
  accessKey: process.env.SAUCE_ACCESS_KEY,
  startConnect: false,
  connectOptions: {
    port: 5757,
    logile: ""sauce_connect.log""
  }
},A pretty standard looking SauceLabs configuration object. Also, more interestingly is this worked up until recently and then seemingly stopped working across multiple repos on a client project I was working on. Truly a perplexing situation.I tried everything (and I mean everything):I tried everything (and I mean everything):
Downgrading Node to different versions (current to stable and every incremental version)
Trying out different versions of Karma and the Karma Sauce Launcher plugin
Using different Karma plugins as part of my build process; karma-typescript, karma-requirejs, karma-browserify and karma-webpack
Attempting to start a Karma server via a Gulpfile.js
Tweaking the Travis CI configuration
Downgrading Node to different versions (current to stable and every incremental version)Trying out different versions of Karma and the Karma Sauce Launcher pluginUsing different Karma plugins as part of my build process; karma-typescript, karma-requirejs, karma-browserify and karma-webpackAttempting to start a Karma server via a Gulpfile.jsTweaking the Travis CI configurationNone of this worked. I was beginning to panic, almost ready to throw in the towel and started looking into Crossbrowsertesting as an alternative instead of SauceLabs.Then, by chance I accidentally solved the issue.The solutionRemove the connectOptions section from your SauceLabs object:connectOptionsconnectOptions: {
    port: 5757,
    logile: ""sauce_connect.log""
}
 I didn’t actually bother testing it, but I suspect the port is wrong. I couldn’t find anything that suggest anything had changed, but I did find a few references to port 4445 that SauceLabs listens on. Perhaps this is the new port you specify and 5757 is deprecated.44455757Or maybe, the issue was the connectOptions were overriding some important defaults? Who knows, all I know is that it fixed the issue and if I saved you a week or more worth of debugging then I am happy.connectOptions",849
How To Convert FormData To JSON Object,"Recently, whilst working on a project I needed to take HTML FormData and then convert it to JSON to be sent off to an API. By default the FormData object does not work in the way you would want it to for JSON.Using the for..of syntax introduced in ECMAScript 2015, we can access the form data entries and iterate over them to get key and value pairs. This will be the verbose solution, scroll to the bottom to get the cleaner solution using .reduce below.for..of.reduceconst formData = new FormData(SomeFormElement).entries();
let jsonObject = {};

for (const [key, value]  of formData) {
    jsonObject[key] = value;
}
const formData = new FormData(SomeFormElement).entries();
let jsonObject = {};

for (const [key, value]  of formData) {
    jsonObject[key] = value;
}
By calling entries on our form data object, it returns the required iterable data we can then access in the form of key and value. In our above example, we actually store each item in an object.entriesFortunately, this isn’t a complicated problem to solve. If we had to do this without a for..of loop, then it wouldn’t be nearly as clean as the above solution is (which can still be improved).for..ofNow, let’s make it more readable by using reduce which can work with .entries to create our object in a slightly cleaner way.reduce.entriesconst formData = new FormData(SomeFormElement).entries();

const jsonObject = formData.reduce((acc, [key, val]) => {
  acc[key] = val;
  return acc;
}, {});const formData = new FormData(SomeFormElement).entries();

const jsonObject = formData.reduce((acc, [key, val]) => {
  acc[key] = val;
  return acc;
}, {});",406
The Decline of Medium.com,"I still fondly remember when Medium first hit the scene. Everyone loved the quality of the writing and variety, every article I read was seemingly well-written and of high quality.Fast forward to 2017 and Medium has become the equivalent of a never-ending TED Talk. Everyone wants to improve my life and tell me how to be a better person.The good articles are still there, but there is a serious imbalance of content going on, with the “X things you need to do for a better life” or “How to be a better X”At the time of writing this, here are some of the titles of Medium articles under the popular topic. As you’re reading them, picture someone in a argyle sweater holding a piccolo latte and wearing black rimmed glasses on a TED stage reading these out as they pace the stage back and forth with long pauses between their words.popular topic
Our clothes can change who we are
It’s a Good Thing Some People Don’t Like You
How To Deal With Uncomfortable Emotions And Reshape Your Identity
5 Mental Biases That Cause Poor Decisions
30 Behaviors That Will Make You Unstoppable
Willpower Doesn’t Work. Here’s How to Actually Change Your Life
I Once Talked To Seth Godin On The Phone: Here’s How It Changed My Life and Business
Our clothes can change who we areIt’s a Good Thing Some People Don’t Like YouHow To Deal With Uncomfortable Emotions And Reshape Your Identity5 Mental Biases That Cause Poor Decisions30 Behaviors That Will Make You UnstoppableWillpower Doesn’t Work. Here’s How to Actually Change Your LifeI Once Talked To Seth Godin On The Phone: Here’s How It Changed My Life and BusinessI am not saying there isn’t a market for these kind of articles and heck, I would be lying to you if I said I hadn’t read one or two of these do-gooder, improve your shitty life articles on Medium before.Medium has become a real life living TED Talk with some of Tony Robbin’s, “my life is so great and here is how your life can be great too” mantra injected in there somewhere.It is for this reason, I find myself not going to Medium as much as I used too. The content doesn’t appeal to me and when it does, there is usually a downside somewhere along the way.It’s not that I am a negative person, I just find these “positive” improve your life articles to be mostly full of shit. I often wonder if Tony Robbin’s truly believes in what he preaches at his expensive events or if he just likes the feeling of money against his skin as he bathes in it on his elevated mansion overlooking the sad common folk.I follow programming topics on Medium mostly and even then, for Javascript specifically it has devolved into comparison posts pitting frameworks and libraries against one another.The comparison articles on Medium (which I am guilty of blogging here myself) are mostly always poorly written and heavily one-sided. It’s rare you encounter a comparison article that is balanced.Then there is the other side of Medium if you’re a content writer: lost ownership.Then there is the other side of Medium if you’re a content writer: lost ownership.When you publish your content on Medium, you’re giving up control. Sure, you get to publish on a large centralised content platform with huge reach, but you’re beholden to a platform which exists to make money for its investors and pay its staff.This means Medium can change the rules midway through the game and you have to deal with it. If Medium decide you have to start paying them or GTFO, you have a loaded gun pointed to your head (a bit of a theoretical, but still).Self-publishing is the future of content, not publishing articles on a VC backed content platform.",904
How To Change/Overwrite Colours In Bootstrap 4,"Well, this one just stung me and a client after migrating from Bootstrap v3 over to v4 in a project.In Bootstrap of yesteryear, you overrode colours using separately named variables like $brand-primary if you’re new to v4, you’ll probably try and use variables like these and discover they do nothing.$brand-primaryIn Bootstrap v4 colours have been moved into a Sass map. This means separate variables for colours no longer exist. The theme colours are now defined inside of Bootstrap’s variables file here which is a map of colours using similar names from v3.hereTo overwrite these colours you firstly need to create your own variables file which you import FIRST before Bootstrap so your variables are used in preference over Bootstrap’s defaults.To change the primary colour, just define the following in your variables file:primary$theme-colors: (
  primary: #333
);This will overwrite the primary colour, but leave the other defaults intact. That’s it. I really wish the Bootstrap team documented this better and made it known, it really caught me off guard.To override more than one colour, just comma separate them using the syntax in the linked variables file in the Bootstrap repo (if you’re not familiar with maps).",307
Webpack Support Lands In Aurelia CLI,"I have been waiting for this day to come for a long time now: the Aurelia CLI now supports scaffolding Webpack applications from scratch.now supportsMy biggest gripe with the skeletons the Aurelia team provide is they have a lot of stuff in them I have to remove for every new project.Admittedly, the skeletons serve to showcase how a functional Aurelia application could look like — but make a lot of assumptions on things like using a router or Fetch to make HTTP requests.Heavily driving the support for Webpack is core team member Jeroen Vinke who has made countless contributions and improvements to the CLI (amongst a few other developers).Jeroen VinkeTo start scaffolding Webpack applications, make sure you update your CLI by installing the latest version: npm install -g aurelia-cli and going through the au new wizard process.npm install -g aurelia-cliau newThe CLI still lacks the ability to update your Aurelia applications using older versions of the CLI, so starting a new project and copying your files across is still the current approach for doing so (for now).",270
Seo.fit: SEO analysis,"For a while I’ve had a great domain name just lying around and it dawned on me what it could be a while ago, so I decided to use seo.fit for a SEO analysis tool that gives you SEO information about a domain and improvements. The purpose seemed fitting for the domain.seo.fitIt’s not an original idea because there are tonnes of these sites around and admittedly, it didn’t take hardly any effort but it’s a step in the right direction for me by actually following through on my side project ideas and launching them.For the moment, I’ve got a book to finish (so no more projects for a while), but in 2018 I’ll be focusing on releasing more little side projects like this, with the end goal being to create enough side income I can work on bigger and better ideas.Check it out: https://seo.fithttps://seo.fit",202
How To Alias Cloud Functions In Firebase,"Firebase Cloud Functions are fantastic, but the URL that you get to run them isn’t so nice. You’ll get given a URL that looks like the following when you create some cloud functions:https://us-central1-demoapp.cloudfunctions.net/functionNameIf you are wanting to use Firebase to build an API for your application for example (like I wanted to), then you would probably prefer your URL looks like this:https://myapp.com/functionNameFortunately, you can. The only downside is you have to use Firebase hosting if you want to be able to alias your functions to your name. If you’re self-hosting your own site somewhere else and using Firebase, then you can’t alias cloud functions.To alias cloud functions, open up your firebase.json file and inside of the hosting and rewrites section add in your rule. My example showcases an API endpoint which gets sent to a Firebase cloud function called api which handles the request.firebase.jsonhostingrewritesapi{
    ""database"": {
        ""rules"": ""database.rules.json""
    },
    ""hosting"": {
        ""public"": ""public"",
        ""rewrites"": [
            { ""source"":""/api/**"", ""function"":""api"" },
            {
                ""source"": ""**"",
                ""destination"": ""/index.html""
            }
        ]
    }
}
In my instance, I have a single page application so I have created a wildcard rewrite to send all requests to the index.html file so my Javascript framework can take over.index.htmlAll you have to remember is source and function where source is your URL pattern (which supports wildcards and matchers) and function is the name of your created function inside of functions/index.js.sourcefunctionsourcefunctionfunctions/index.jsI highly recommend installing and using Express to handle your routing needs inside of cloud functions if you’re wanting to work with aliases, especially for Firebase driven API’s.In a future article I’ll show you how you can create an API using Firebase and alias the functions.",492
TidyFork: Cleanup Outdated Forks and Old Starred Repositories,"As a developer, I love Github and use it to not only contain my public and private development projects, but also follow other interesting projects. Sometimes I’ll fork a project if I want to contribute to it or star a repository if I am interested in using it now or later.I joined Github on August 18, 2010 so I have been an active member of Github for seven years now.In that time I have forked and starred quite a few repositories. I am generally pretty good at deleting unused forks, but I still had a few that dated back by quite a few years.When it comes to stars however, my development skills and preferences have changed in the past seven years. In 2010 I was still doing a lot of PHP work, I also dabbled in Ruby/Ruby on Rails as well as Python and so on over the years.At the time of writing this, I have 454 starred repositories which equates to 16 pages of repositories when I view them in the stars tab of my profile. I know developers who have three times that number.Deleting old forks is a painful experience. You have to enter the name of the repository to confirm you want to delete it and confirm your password. Imagine having 200 forks accumulated over the years and having to delete them one-by-one, it would probably take you a full day at least.Unstarring repositories is a little more straightforward, but if you have starred a lot of repositories it can be tedious going through and unstarring them.This is why I created TidyFork. Originally it was just for cleaning old and outdated forks you’ve forgotten about, then I expanded it to also allow you to cleanup old starred repositories.TidyForkRunning TidyFork over my account yielded my first ever starred repository was Modernizr. Other repositories listed include jQuery plugins and grid systems, PHP libraries, API bridges and pretty much anything else you can think of.The goal is to expand this tool even further, allowing you to specify criteria and being able to automatically delete forks and starred repositories. At the moment you have to manually choose what you want to delete or unstar, everything is ordered by when it was last updated for forks and for stars, the earliest star.I built TidyFork mostly for myself, but it might be valuable enough that others might get some use out of it.",570
Create an alias to C drive in Ubuntu Bash on Windows,"If you’re using the fantastic Ubuntu Bash terminal in Windows 10 which gives you a proper Ubuntu Linux terminal window and subsystem, you’re probably wanting to access files on your main drive.To get to your C drive which has your files, you can just type:To get to your C drive which has your files, you can just type:cd /mnt/cI don’t know about you, but typing that is painful. By creating an alias in your .bashrc file which is a configuration file for the Ubuntu terminal instance, you can create a shortcut to the C drive (and other mounted drives as well)..bashrcGo into your home directory by typing:Go into your home directory by typing:cd ~Then open up your .bashrc file in the Nano text editor by typing:Then open up your .bashrc file in the Nano text editor by typing:.bashrcnano .bashrcGo all of the way to the bottom and add in an alias. I called mine cdrive but you can call yours whatever you want:cdrivealias cdrive='cd /mnt/c'Then press ctrl+x to exit the text editor. You’ll be asked if you want to save your changes, type Y and hit enter. Lastly, we need to tell the current terminal window about our changes by typing:ctrl+xYsource ~/.bashrcNow test your alias by typing cdrive (or whatever you called it). You should be taken to your main drive. That’s it.cdrive",321
You Don’t Need a Degree To Be a Front-end Developer,"As a self-taught developer, it’s easy to feel like you missed out on something, and assume your colleagues who did get a degree know more than you. Your colleagues know about algorithms and CS concepts like data structures and you most likely don’t.Does having a degree or not having a degree even matter? In my experience as a self-taught developer: no.noWhile having knowledge of algorithms might come in handy once in a blue moon, most of the time you’re just trying to get your CSS and Javascript to work in Chrome, Firefox and Internet Explorer.For those rare times when you do need to implement some kind of algorithm, someone has already posted the answer over at StackOverflow or on Google somewhere. Why learn something you can Google?Take it from someone who has worked with many developers with degrees over the years, I have never encountered a situation where someone with a degree and same amount of experience as me, was a better developer because of it.Thanks to numerous online resources (many that are free), learning to code is easier than ever. I am encountering more and more self-taught developers in front-end development, and it’s great to see people showing real passion to learn how to code.For companies like Facebook and Google, candidates with degrees are still favoured and the interview questions are famously tilted in the direction of fresh candidates with knowledge of algorithms and computer science fundamentals (at least at Google anyway).Understandably, Google are working on bigger problems and in many cases they actually do need developers with fundamental programming knowledge. For every day agencies and small to medium sized companies, not so much (unless you’re specifically working on complicated problems).Also worth mentioning is companies like Google receive thousands of applications per week, so they have to be a little more picky with the candidates they do choose to hire.It doesn’t mean if you lack a CS degree you’ll be passed over, but be prepared to put in some work before the interview if you want a chance at being hired. I personally have no interest in the fundamentals or theory aspect of computer science.Where a degree comes in handyDegrees are not completely irrelevant, if you’re just starting out a degree is your golden ticket to getting interviews. When you have no commercial experience, a degree can sometimes be the edge you need to get hired.Once you have some experience in development, a degree doesn’t really matter anymore. Most job listings you’ll encounter will specifically mention wanting someone with X years of experience as one of the requirements.Another instance where a degree might help is when a company is deciding between two candidates. Both might have the same amount of experience and skills, but one might have a degree and that could be the deciding factor.Coding interviewsDepending on where you are interviewing, some companies still ask outlandish and complex coding puzzles to screen candidates. Don’t let it deter you, if you can teach yourself to code you can teach yourself anything and interviewing is definitely a learned skill separate from coding.Not all companies have complicated coding interviews, most of the companies I’ve interviewed at asked reasonably straightforward questions and I’ve never been asked anything about algorithms.I have only ever had one bad interview experience in ten years (not bad). Companies that ask hard questions during coding interviews do exist, so always be prepared.Many developers I know who have degrees end up utilising so little of what they actually learned in the real world, when it comes to things like algorithms and data structure type questions in interviews, they’ve had to go relearn it all anyway because they’ve forgotten it.ConclusionNo degree? No worries. The forever evolving nature of front-end development means that even a developer who has a CS degree has to keep learning once they graduate to keep up with the constant changes to Javascript and other aspects of web development.Focus on growing your skills and working within teams staffed with developers of varying skill levels and the rest comes naturally.",1045
How I Come Up With Some of My Blog Post Ideas,"The hardest part about blogging is thinking of what you should say. For me, this blog has become focused heavily on Aurelia and Javascript, blogging about other things occasionally. I tend to stick within the front-end development niche.When it comes to blog post ideas, believe it or not: Stack Overflow has been a very influential part of my writing. I’ve written blog posts that were inspired by problems with cool solutions or niche features in a framework not many are aware of.Case in point, a question I recently answered on Stack Overflow had a solution that many who use the framework might not have thought of, as it delved into some constructs that aren’t immediately obvious to some.a question I recently answeredAfter answering, I was inspired to write a blog post about this because it seemed as though it would be useful enough for others wanting to do the same. Having also worked with Vue.js recently which has a similar check, I figured a blog post was needed.blog postAnd herein lies the beauty of all this: Stack Overflow can help inspire you to write about something you might not have thought of.I am not saying that you should copy/paste other peoples answers because not only is that plagiarism, Google will penalise you for it. But it provides a barometer on the common pain points and problems that developers face, things that matter.It doesn’t just stop there. I take inspiration from chatter on the Aurelia Gitter chat to write up useful blog posts on Aurelia, based on what people are getting stuck on or want to know. I have also been known to lurk Github issues for blog post inspiration. Heck, even Twitter can be a source of inspiration.Instead of thinking of what your target audience might want to read, seek out what people want to know and write the content for them.Remember to write it downCliche, but if you have an idea write it down somewhere (even if it is just a potential blog post title). I use Dropbox to store all my post ideas (written in Markdown) and when I get inspiration I create a new Markdown file.Take a look at my Aurelia Dropbox folder, there are posts in other folders (not pictured) but you can see I have a tonne of ideas and some of these are partially written, some are just titles with no content at all. I might not finish any of these, but it gives you insight into the process.",587
Checking If a View Slot Is Defined In Aurelia,"Aurelia supports the <slot> element provided via the HTML Web Components specification, which allows you to define placeholders in your HTML that can be replaced.<slot>A lot of examples around seem to wrap a slot with a DIV, perhaps a class. The issue with this approach is if you have a styled DIV wrapping a slot and that slot is empty, your DIV will still be affecting the space around it. This is where the ability to check if a user has provided a slot or not helps.This post was prompted by a StackOverflow question I answered here. It dawned on me this isn’t a straightforward solution and other people might find it valuable.hereTake the following fictional example of a modal custom element. We have a header, body and footer. The body contains a slot without a name, but the header and footer are named slots.<template>
    <div class=""modal__header"">
      <h1>Header:</h1>
      <slot name=""modalHeader""></slot>
    </div>
    
    <div class=""modal__body"">
      <h1>Body:</h1>
      <slot></slot>
    </div>
    
    <div class=""modal__footer"">
      <h1>Footer:</h1>
      <slot name=""modalFooter""></slot>
    </div>
</template>The issue here as I mentioned earlier is a) styles on wrapping DIV’s affecting the layout even if the slot is empty and b) if the element wrapping your slot contains other HTML with your slot, it will show up even if no slot content is provided (as can be seen above).What we want to do is conditionally check if a slot has user-provided content and show or hide it accordingly.Accessing the controller instanceAurelia creates an au property on every tracked Aurelia node in the page, containing its view factory information, bindings and more. By accessing the controller and then the view property of our controller, we can access our slots.auviewexport class ModalCustomElement {
  static inject = [Element];
  
  constructor(element) {
    this.element = element;
  }
  
  attached() {
    this.$slots = this.element.au.controller.view.slots;
    console.log(this.$slots);
  }
}Inside of the view-model for our fictional modal custom element, we inject the custom element instance and then we create a class property called $slots where we pass in the view controller slots property containing our slots.$slotsIn our example, the $slots property we created should contain three slots. Because the $slots property is an object which is keyed by slot name, in our view we can determine if the user has provided slot data by checking the children property.$slots$slotschildrenThis then gives us some HTML that looks like this:<template>
    <div class=""modal__header"" show.bind=""$slots.modalHeader.children.length"">
      <h1>Header:</h1>
      <slot name=""modalHeader""></slot>
    </div>
    
    <div class=""modal__body"">
      <h1>Body:</h1>
      <slot></slot>
    </div>
    
    <div class=""modal__footer"" show.bind=""$slots.modalFooter.children.length"">
      <h1>Footer:</h1>
      <slot name=""modalFooter""></slot>
    </div>
</template>For each slot, we check the children property which is an array. We know if the length is zero, the user hasn’t provided any data for this slot. Default data inside of a slot will not be counted in the children array, that lives elsewhere in the fallback slot property.childrenchildrenWe are also accessing the slot via its name as it lives in the object. For the default slot with no provided name, its name internally is: __au-default-slot-key__ which can also be checked using a show.bind.__au-default-slot-key__show.bindWhy if.bind and not show.bind?if.bindshow.bindYou’ll notice we are using show.bind and not if.bind above. You can’t actually use if.bind because it removes the slot from the controller and therefore, we can no longer check for it. When using show the element remains but is hidden and our slot still exists.show.bindif.bindif.bindshowWorking exampleIf the above went over your head, the example I posted in the SO question is here where you can see DIV’s being shown/hidden depending on whether or not the user provides slot content.here",1012
Get Root $index Value From Within Nested Aurelia Repeaters,"Recently in an Aurelia project, I was working with nested repeaters (3 levels deep). The problem was I needed to get the $index from the top level repeater (so, level one). Instinctively, I presumed that you could do this:$parent.$parent.$parent.$indexThis does not work. The $parent variable only extends to the parent and cannot be chained.$parentI asked in the Aurelia team chat if anyone had a solution and thankfully core Aurelia developer Ashley Grant chimed in with a great solution that he learned from one of Rob’s intermediate training videos, that I had to share.Ashley GrantUsing the ref attribute, you simply create a reference to the repeater and then bind the index to the element itself. The solution is simple, introduces no performance issues (that I am aware of) and works great.ref[code]

Root index value ${$root.index}


Root index value ${$root.index}
Root index value ${$root.index}[/code]We first specify the ref attribute and provided it a name, I went with $root because it falls more inline with other repeater variables. Then you want to bind to the index property and pass it in the current iteration index. Finally, any subsequent uses are: $root.index where $root is our element and index is our bound value.index$root.index$rootindex",317
Aurelia Routing + Switching Root Using setRoot,"In your Aurelia applications, you might have two or more roots defining different entry points into your application. I personally create a public facing root which has public routes and an auth protected shell which has routes for logged in users only.Let’s say for this example you have two roots: publicRoot and privateRoot. Your publicRoot view-model has your login/logout and other public routes, and your privateRoot view-model has routes for an administration panel.Upon successful login, you might want to redirect the user to their administration panel. Switching the root to our private one and having a default route as our admin panel will do the trick. But there is a problem.If your URL is: http://localhost:9000/#/auth/login and you switch the root, you’ll get an error in the console about your auth/login route not being found. This is because you just switched the root view-model and your auth/login route no longer exists.http://localhost:9000/#/auth/loginauth/loginauth/loginThe solution is to redirect to the base of your application and rewrite the browser history stack, before changing the shell:The solution is to redirect to the base of your application and rewrite the browser history stack, before changing the shell:this.router.navigate('/', { replace: true, trigger: false });
aurelia.setRoot(PLATFORM.moduleName('privateRoot'));The options provided to navigate are quite important. We are rewriting the browsers URL history by supplying replace: true and trigger: false allowing us to switch roots without encountering route issues as a result of the current route no longer existing.navigatereplace: truetrigger: falseThis is fine and dandy if you’re changing the root from within an app view-model, but in my case I also want to check within my main.ts file where I bootstrap the app to see if the user is already logged in.main.tsThis is where you’ll need to leverage the DI container Aurelia provides to get an instance of the router and then use its navigate method.import {Container} from 'aurelia-framework';
import {Router} from 'aurelia-router';

export async function configure(aurelia: Aurelia) {
    aurelia.use
        .standardConfiguration()
        .developmentLogging()
        .feature(PLATFORM.moduleName('resources/index'))

    await aurelia.start();
    await aurelia.setRoot(PLATFORM.moduleName('publicRoot'));

    firebase.auth().onAuthStateChanged(user => {
        if (user) {
            const router = Container.instance.get(Router);
            router.navigate('/', { replace: true, trigger: false });
            aurelia.setRoot(PLATFORM.moduleName('privateRoot'));
        }
    });
}In my example I am using Firebase, but you could use anything. Pay close attention to the router and navigate method, followed by the setRoot call setting our root to the admin view-model if we’re logged in.setRootConclusionAs always, there might be edge cases I haven’t encountered and downsides to the above approach. Always test code snippets you find online, but the above works fine for me and will most likely work well for you too.",772
Module ES2015 and TypeScript 2.4 Dynamic Imports,"Introduced in TypeScript 2.4 is support for the ECMAScript dynamic imports feature. If you didn’t see the announcement or read it properly, you’re probably here because you’re getting the following error.In my case I use Webpack and I was trying to add in some dynamic import goodness and getting this error: Dynamic import cannot be used when targeting ECMAScript 2015 modules.Dynamic import cannot be used when targeting ECMAScript 2015 modules.You’re probably thinking, this is crazy considering dynamic imports are an ECMAScript feature, not a TypeScript one. The tell is in the error, if your module is set to es2015 you’re targeting ES6 and dynamic imports are a feature not due for release until ECMAScript 2018.modulees2015Funnily enough, the TypeScript team did reveal this in their official announcement but if you’re like me, you missed it the first time and hit this issue.official announcementThe fix is simply setting the module value in your tsconfig.json file to esnext, like this: ""module"": ""esnext"". If you’re using Visual Studio Code, you might get a squiggly in your tsconfig.json file telling you it’s not a valid value, but ignore it because it is.moduletsconfig.jsonesnext""module"": ""esnext""tsconfig.json",307
Exciting New Firebase Features Announced at Google IO 2017,"Admittedly, Google’s developer event IO has grown to be quite interesting the last couple of years. This year (2017) I was excited to see what would be announced in the world of Firebase.I’ve been using Firebase on and off for the last couple of years. Recently, I’ve found a renewed sense of excitement in using Firebase again (especially after cloud functions were released).A few of the new features are more mobile-oriented, Firebase likes to focus on mobile developers and applications but it has value for all platforms.Cloud functions now support custom domainsCloud functions are great, but the one thing I hated about them was the silly long CDN URL that you get. This meant you couldn’t use cloud functions as an API.I have an application I wanted to create an RSS feed on. I didn’t want to tell my users to access the CDN URL that Google provides, so I hacked something else together.There is a way you can use a custom domain using a third-party proxy, but honestly, I hate having so many moving parts in my applications and that approach is hacky.With the new announcement, you can now use a custom domain and point a URL at a cloud function only if you use Firebase hosting. In your Firebase configuration file, you pass in a rewrite array and tell it what function to run when a certain URL is accessed.new announcementThis means if you’re using Firebase hosting, you can now create API endpoints and even pre-render pages for single page applications without using a third-party service. Neat!You still cannot add a custom domain to cloud functions if you don’t want to use Firebase hosting. But this is a great start and hopefully, they expand upon it.Phone authenticationThis is a great feature the Firebase team have rolled into their core offering. After acquiring the Fabric team from Twitter a few months back, the newly introduced phone authentication is a continuation of Fabric’s Digits product.newly introducedThis now makes Firebase’s already great authentication even better with another addition more oriented towards mobile applications. Still, for desktop applications, it is convenient or hybrid applications.Open sourcing SDK’sOne of the biggest criticisms of Firebase is the public SDK’s are not open source. Currently, you get a minified alphabet soup mess of code, that all changes with Firebase SDK’s going public.Overall a pretty safe announcement from the Firebase team. But, a good sign that Google are committed to Firebase and it isn’t going anywhere anytime soon.",627
Auth Protected Routes in Aurelia With Firebase,"sing both Aurelia and firebase together is exceptionally convenient for my workflow. When it comes to authentication, I have certain routes I want to protect.The best way to add in authentication checks for routes in Aurelia is to create a router pipeline step. Specifically, an authorise pipeline step.Because of the way Firebase uses a promise based callback for its auth state change event, we need a way to wait for Firebase to resolve its callback before checking.It might not be immediately obvious if you read the document, but the run method in a pipeline step can return a Promise object and it will wait for it to resolve or reject before proceeding.runPromiseI am going to assume you already have Firebase installed and configured in your Aurelia application.Inside of your src/app.ts/src/app.js file, add the following right at the bottom:src/app.tssrc/app.js<pre><code class=""language-javascript javascript"">class AuthorizeStep {
    run(navigationInstruction, next) {
        return new Promise((resolve, reject) =&gt; {
            firebase.auth().onAuthStateChanged(user =&gt; {
                let currentRoute = navigationInstruction.config;
                let loginRequired = currentRoute.auth &amp;&amp; currentRoute.auth === true;

                if (!user &amp;&amp; loginRequired) {
                    return resolve(next.cancel(new Redirect('')));
                }

                return resolve(next());
            });
        });
    }
}</code></pre>This is our authorise step which waits for Firebase to tell us the current auth state and check the current route to see if it has an auth attribute set to true.authWe use the observable onAuthStateChanged method because the user might log out or their session expires between page loads. Also, you’ll experience race collisions with the timing of routes and the state not being fully resolved if you do it any other way.onAuthStateChangedYou’ll notice if the user is not logged in, then we are redirecting them to the homepage. Specify any URL you want here. Don’t forget to import Redirect from the aurelia-router package as well.Redirectaurelia-routerInside of your app file, inside of the configureRouter method, you want to now supply the auth step so our routes are checked.configureRouterAt the bottom of the configureRouter method add the following:configureRouterconfig.addPipelineStep('authorize', AuthorizeStep);That’s it. Not much code to implement Firebase auth into your routes.",619
I Like Competitions,"Kind of a strange post from what I usually post, but the last few months I have been addicted to entering online competitions.The fact I have won a few great prizes in just a few months probably helps. I won a runner up prize which was an LG television and then I won a Weber Baby Q Titanium barbecue. I also scored a free double pass to see the movie Office Christmas Party complete with free drink and popcorn. The movie wasn’t that great, but it was a night out for me and my wife.Oh, and I won a copy of Mafia III on PC as well. I haven’t played it yet but thought that was cool.Then I won a Sony 4K action camera, a GoPro type device that can shoot stabilised 4k video. Just when I thought I’d peaked in terms of competition wins, as I was typing this post up I won something again: A Samsung Gear S3 Frontier smart watch. This is my first competition win for 2017. Then literally shortly after, I won a Kenwood Stand Mixer.Every win motivates me to keep on entering competitions and I have found the best competitions are the “25 words or less competitions” where I pull out all the stops to win. I like writing and blogging, so the word competitions are naturally higher odds for me.To summarise what I’ve won since October 2016:To summarise what I’ve won since October 2016:
Sony 4K action camera
Samsung Gear S3 Frontier smart watch
Weber Baby Q Titanium barbecue
LG TV
Mafia III PC
Sony 4K action cameraSamsung Gear S3 Frontier smart watchWeber Baby Q Titanium barbecueLG TVMafia III PCThere is a downside to entering competitions and that is spam. I do get more spam email and I get spam phone calls as a result. Although, it is easy enough to unsubscribe from the emails and the phone calls are usually from unlisted private numbers (which I block on my phone anyway).",445
Enabling CORS Middleware In Firebase Cloud Functions,"Firebase Cloud Functions are great, but there might come a time where you need CORS support. This can be enabled easily by using the CORS middleware.The documentation does detail part of the process, but it doesn’t mention you need to install the cors package and also specify origin: true as a configuration option. If you’re new to Node.js, this might catch you off guard.corsorigin: trueGo into your functions directory in your application and in your terminal: npm install cors --save this will add the CORS middleware package to your package.json.functionsnpm install cors --savepackage.jsonOpen up your index.js file in the functions directory and add in the following:index.jsfunctionsconst cors = require('cors')({
  origin: true
});

exports.helloWorld = functions.https.onRequest((req, res) => {
    cors(req, res, () => {
        res.send(""Hello from Firebase!"");
    });
});Notice how we use the CORS middleware function inside of our request handler? You’ve just added in CORS. Essentially what is happening here is, you’re taking the actual response and then running it through the cors middleware which then makes it a cross-origin request.It is possible to set the headers manually on your request without the cors package, but the easiest and cleanest solution is using the cors middleware instead.",329
Convert A Firebase Database Snapshot/Collection To An Array In Javascript,"Because Firebase is a NoSQL JSON data store, you might have noticed that when you get some data from Firebase that it is an object. If the title wasn’t obvious enough, we are talking about using Firebase Realtime Database in a web application.Take the following example:Take the following example:firebase.database().ref('/posts').on('value', function(snapshot) {
    console.log(snapshot.val());
});Let’s imagine that we have 20 posts in our database. You’ll get back an object containing keys and objects for all of our imaginary posts.If you’re working with a Javascript framework or library such as Aurelia, then you’ll know that iterating an object opposed to an array is more complicated (especially HTML templating).So, here is a function I ended up writing which I use quite a lot in my Firebase applications:function snapshotToArray(snapshot) {
    var returnArr = [];

    snapshot.forEach(function(childSnapshot) {
        var item = childSnapshot.val();
        item.key = childSnapshot.key;

        returnArr.push(item);
    });

    return returnArr;
};I am a big fan of verbose functions, although using the power of modern spec Javascript you can create a more condensed shorthand equivalent of the above function.To use our newly created functionTo use our newly created functionfirebase.database().ref('/posts').on('value', function(snapshot) {
    console.log(snapshotToArray(snapshot));
});If you are using a transpiler like Babel, writing using TypeScript or targeting evergreen browsers like Chrome and Firefox, a nicer solution is:If you are using a transpiler like Babel, writing using TypeScript or targeting evergreen browsers like Chrome and Firefox, a nicer solution is:const snapshotToArray = snapshot => Object.entries(snapshot).map(e => Object.assign(e[1], { key: e[0] }));You call our shorthand function the same, but it’s a less verbose and harder to understand one-line function. Special thank you to the commenters below who have proposed their own solutions as well.",501
Aurelia Dynamic Compose + Webpack = Module ID Not Found,"If you’re using the latest and greatest Webpack plugin for Aurelia, there is a possibility if you use the <compose> element to dynamically render UI you will run into an issue where Webpack doesn’t know about the dynamic imports.<compose>Specifically, I am talking about usage of <compose> that might look like the following:<compose><template>
    <compose view-model=""./views/view-${viewTemplateName}""></compose>
</template>This is due to the fact, they can’t be resolved beforehand and put into the bundle. Fortunately, there is an easy fix for all of this.Newly introduced into the Webpack plugin, is support for PLATFORM.moduleName(file) which tells Webpack where to find files in Aurelia. We can use this to tell Webpack about our dynamic files.PLATFORM.moduleName(file)At the top of each dynamic file, simply add the following:At the top of each dynamic file, simply add the following:import {PLATFORM} from ""aurelia-framework"";

PLATFORM.moduleName(""./my-dynamic-view"");

export class MyDynamicView {

}Using moduleName helps Aurelia and Webpack see your dynamic files: that’s all you need to do.moduleName",279
Aurelia Dynamic Views From Strings Using InlineViewStrategy,"The flexibility of Aurelia means there are many ways to achieve a task. The default convention of view/view-model will meet your needs a lot of the time, but sometimes you might need a little more power.You might already know of the ability to define views inline (inside of your view-models) using the inlineView decorator, allowing you to eschew view HTML files entirely. The InlineViewStrategy class offers similar functionality for a different purpose.inlineViewInlineViewStrategyUsing the below approach, we can render views dynamically from strings. If your server returns HTML to be rendered for example, then you can take this and use it to create a view.The first argument of InlineViewstrategy accepts your HTML string, which needs to be wrapped in opening and closing template tags <template></template>. There is an optional second argument where you can pass through one or more dependencies in an array.InlineViewstrategy<template></template>import {InlineViewStrategy} from 'aurelia-framework';

export class MyDynamic {
    constructor() {
        this.viewStrategy = null;
        this.someVar = 'a value inside of it.';
    }

    bind(bindingContext, overrideContext) {
        this.viewStrategy = new InlineViewStrategy(`<template>This is my string with ${someVar}</template>`);
    }
}
<template>
    <compose view.bind=""viewStrategy""></compose>
</template>
The above code could be modified to allow for the view to be updated after initial rendering, as view.bind will see changes made to the viewStrategy variable and re-render accordingly.view.bindviewStrategyBecause we are using <compose></compose> without supplying a view-model, the current view-model will be used as the context, allowing us to just specify our own view and keep everything else as intended.<compose></compose>If you don’t want that <compose> element uglying up your beautiful HTML markup you can use the containerless attribute like this:<compose><template>
    <compose containerless view.bind=""viewStrategy""></compose>
</template>
In many cases, the above is for pretty niche scenarios. There are definitely easier ways to achieve most tasks, with additional power there (as shown above) to take control if you need it.",555
Efficiently Looping A Javascript Array Backwards,"Most of the time when you’re looping over an array of elements, you’ll do it sequentially. However, I recently needed to iterate through an array of objects in reverse just using a plain old for loop.This will not be a highly informative or groundbreaking post but, I thought I’d share this in case someone wants to solve the same problem and might be confused with the many different ways you can loop over an array in reverse.I had an idea in mind, I knew Array.reverse would be the ideal candidate but I still Googled to see if smarter developers than me figured something better out.Array.reverseTurns out, there are a lot of scary alternatives to looping an array in reverse (mostly on StackOverflow). Some people are proponents of decrementing and using while loops, others had different ideas. Why not just use a function that’s existed since the dawn of Javascript?var myItems = [
    {name: 'Dwayne'},
    {name: 'Rob'},
    {name: 'Marie'},
    {name: 'Sarah'},
    {name: 'Emma'},
    {name: 'James'}
];

var itemsToIterate = myItems.slice(0).reverse();

for (var i = 0, len = itemsToIterate.length; i < len; i++) {
    var item = itemsToIterate[i];
}
In our example, we take an array of items and then we use slice to make a copy of our array starting at its first index (zero). Then we call reverse on the cloned array.slicereverseI said efficient in the title, but I haven’t benchmarked anything. However, we are using a barebones for loop and you don’t really get much faster than that. Sometimes commonsense and readability beat microoptimisating.The reason we copy the array is so we don’t modify the original array. Using slice allows us to effectively clone the array and gives us a new instance, there are other ways of doing the same thing but I find this way is the cleanest.Without slice, you’ll be modifying the provided value to our function and might produce an unintended result doing so. I tend to keep my functions pure for this kind of thing, nothing that gets input should be modified.Thanks to reverse flipping our array upside down, we iterate like we would normally. Breaking out the reverse functionality into a function might also be a great idea. This would allow us to easily test our functionality from within a unit test.function reverseArray(array) {
    return array.slice(0).reverse();
}
One thing to keep in mind is for my use-case, slice worked — if you’re dealing with arrays containing object references or nested arrays or complex objects, slice does not do a deep copy.slicesliceLodash has some great methods for doing recursive and deep cloning of arrays and collections if you need that kind of power. Post your thoughts and suggestions in the comments below.",678
Configuring Git Pre Commit Hook And TSLint (automatically),"If you’re a TypeScript user and you’re reading this, then you’re using TSLint (most likely). Recently, a situation at work arose where even though TSLint warnings were being thrown in the editor as well as terminal output, some developers (tsk tsk) were still committing these warnings.Naturally, a pre-commit Git hook is the right candidate for this. Being able to run TSLint to ensure that before a developer can even commit let alone push, only valid code conforming to the tslint.json file can be pushed.tslint.jsonThis poses another problem. You can’t automatically add pre-commit hooks into the repository and have everyone automatically pull them down. This is for security reasons, could you imagine if someone committed a hook that deleted a bunch of files/folders?If you’re using a task runner like Gulp or Grunt, then you can create a clever task that copies a file to the .git/hooks directory for you..git/hooksFirstly, let’s create a pre-commit hook. In the root of your application create a new folder called hooks and a new file called pre-commit (with no file extension):hookspre-commit#!/bin/bash

TSLINT=""$(git rev-parse --show-toplevel)/node_modules/.bin/tslint""

for file in $(git diff --cached --name-only | grep -E '\.ts$')
do
        git show "":$file"" | ""$TSLINT"" ""$file""
        if [ $? -ne 0 ]; then
                exit 1
        fi
done
Git hooks are actually bash scripts and can be quite powerful. We are creating a path to TSLint in our local application (some Git clients like Github for Windows require this) and using that to call TSLint on our files.Secondly, let’s create our task. I personally use Gulp, but you can easily adapt the following to any task runner:var gulp = require('gulp');

gulp.task('install-pre-commit-hook', function() {
    gulp.src('hooks/pre-commit')
        .pipe(gulp.dest('.git/hooks'));
});

gulp.task('default', ['install-pre-commit-hook']);
Running gulp or gulp install-pre-commit-hook will copy over our pre-commit hook and put it into the .git/hooks directory. It is possible on Unix based operating systems that you need to adjust the file permissions using chmod which the fs module offers a method for, but possibly not needed.gulpgulp install-pre-commit-hook.git/hookschmodfsNow, throw that task into your Npm build script and anyone else who has the latest changes will get the pre-commit hook every time the task runs. No more warnings from code written by others clogging up your terminal or editor.",618
Static Inject vs @inject In Aurelia,"A few people have actually asked me this question, so I thought a helpful blog post for reference would finally answer the question: What’s the difference between static inject and @inject in Aurelia?@injectThe answer will surprise you. Okay, not really. It’s a pretty clear-cut answer. There is no difference between either.Back in the early days of Aurelia before support for decorators was added (we are talking early 2015 here), you did things a little more verbosely because support for decorators was non-existent.Once upon a time to inject dependencies into a class, you could only do this:import {EventAggregator} from 'aurelia-event-aggregator';

export class MyVm {
    static inject = [EventAggregator];

    constructor(ea) {
        this.ea = ea;
    }
}
Then a decorator in the form of @inject was added and you could inject like this:@injectimport {inject} from 'aurelia-framework';
import {EventAggregator} from 'aurelia-event-aggregator';

@inject(EventAggregator)
export class MyVm {
    constructor(ea) {
        this.ea = ea;
    }
}
But behind the scenes, the @inject decorator is setting the inject property on the class anyway. So injecting dependencies still works the same way it always did, you just have a decorator that does this for you.@injectinjectI probably use @inject 90% of the time, I use TypeScript so that’s @autoinject in most cases for me. Lately, I’ve had an awakening, it’s actually more work to import the @inject decorator and use it than it is to just define dependencies statically.@inject@autoinject@injectI’ve noticed some of the official Aurelia libraries also use the static injection approach. Who knows maybe the lack of decorators is a good thing from a build perspective, less to process perhaps?In future, I will most likely use the static inject method because anything that means I spend less time typing is a good thing. Although, with IDE’s like Visual Studio Code have autocomplete the typing isn’t really an issue.",494
Sneak Peak: Aurelia Markdown Editor,"As many of you know, I have been working on an in-progress book on Aurelia titled Aurelia For Real World Applications for quite a while now.Aurelia For Real World ApplicationsGood news! the book is finally nearing its release and I’ve actually started working on the example applications chapter (amongst the others still in progress). This is probably the most anticipated chapter of the entire book for many readers.One of those applications is a Markdown Editor that you can see running here. Shortly, owners of the book will be able to obtain the source code to this application and numerous other apps.hereThe idea behind these applications isn’t to hold your hand and build something step-by-step, it is to provide functional working applications built with Aurelia for you to learn off of.The idea being you can get the code, run it and do whatever you like with it: even use the provided apps to build something of your own.The Markdown Editor is a great example of working with third-party libraries; Commonmark, Bootstrap, Jspdf and more. While the styling might leave more to be desired, functionally it is pretty cool.There is support for live Markdown preview, the ability to export; Markdown, HTML and PDF files as well as using localStorage for persisting files locally.This is just an early look, the application will be cleaned up before the source code is made available to book owners.I am aware of various little bugs I need to resolve before the code is made available to owners of the book. I just wanted to share what’s happening with the book at the moment.Stay tuned for another application demo soon.",407
Code Splitting Your Aurelia Webpack Applications,"In this article we won’t be detailing how to setup a new Webpack application, but how you can leverage code splitting to reduce the size of your application.If you would like to know how to create a new Webpack application capable of supporting code splitting, this post has you covered.this postThis will be a rather quick post, no advanced concepts to learn or remember here.What is code splitting?It’s a fancy term describing how you can break parts of your application into smaller Javascript bundles. Instead of having one massive Javascript file that contains your application and dependencies, you take parts of your app and split it into smaller files (sharing the load).A good starting point is splitting at routes/screens. This means that your split bundle will only be loaded if it is requested, which means the user is downloading smaller amounts of Javascript instead of everything at once (especially if you only need 10% of your code on first-load).Let’s get startedRemember, we are making the assumption you already have an Aurelia application ready to go, preferably with some routes created.We have a fictional application with four routes. We are going to split our code at these routes on each moduleId which contains the path to our route.moduleIdThe following code goes inside of your configureRouter method and you’ll need to import PLATFORM from the aurelia-pal module or aurelia-framework module.configureRouterPLATFORMaurelia-palaurelia-framework    config.map([
      { route: ["""", ""home""], name: ""home"", moduleId: PLATFORM.moduleName(""home"", ""home""), nav: true, title: ""Homepage"" },
      { route: ""products"", name: ""products"", moduleId: PLATFORM.moduleName(""products"", ""products""), nav: true, title: ""Products"" },
      { route: ""product/:product_id"", name: ""product"", moduleId: PLATFORM.moduleName(""product"", ""product""), nav: true, title: ""Single Product"" },
      { route: ""contact"", name: ""contact"", moduleId: PLATFORM.moduleName(""contact"", ""contact""), nav: true, title: ""Contact Us"" }
    ])
The first argument of moduleName is the name of the view-model we want to load and the second argument is an optional chunk name. The second argument (if supplied) will generate a bundle based on the provided value.moduleNameTaking the above routes, we will have four generated split bundles. These bundles are loaded on demand, with the idea being the initial load is faster because you are loading less. The bundles are lazy loaded when they are needed.Seriously, that’s it. All you need to do is supply a chunk/bundle name as the second argument and Webpack takes care of the rest, even loading the bundles when they’re needed.If you are looking for runnable example code, core team member Joel Dumas (@jods4) has you covered with an example here if the above went over your head.an example hereOnce you grasp the simple concept of code splitting on your routes, you can also do the same for resources and anything else that makes use of PLATFORM.moduleName in your applications.PLATFORM.moduleNameWebpack is the future.",762
